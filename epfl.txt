In this unit, we will go through a concrete example and learn how to fit polynomials using the Numpy polyfit() function. At the end of this unit, you will have to solve a small task using what you’ve learned.

The task
Let’s start by loading the first dataset.

import pandas as pd

data_df = pd.read_csv("c3_poly_data-1.csv")
data_df.head()

As you can see, the Dataframe contains an x and a y variable. In this unit, we will learn how to use the polyfit() function from Numpy’s polynomial package to model the relationship between these two variables.

Let’s take a look at its documentation page. It states that the function computes the “Least squares polynomial fit”.

“Least-squares fit of a polynomial to data. Return the coefficients of a polynomial of degree deg that is the least squares fit to the data values y given at points x. […] The fitted polynomial(s) are in the form

p
(
x
)
=
c
0
+
c
1
∗
x
+
…
+
c
n
∗
x
n

where n is deg. - Numpy documentation

In summary, this function fits a curve to a set of data points using the equation of a polynomial of degree deg. You’ve probably already used similar tools. For instance, the Microsoft Excel software can also perform curve fitting. Here, the word “polynomial” refers to a set of curves that you can fit, and “least squares” refers to how these curves fit the data points. But we will learn more about that in the next units.

Let’s do a quick refresher about polynomials.

Polynomials
When the polyfit() function documentation states that it fits a polynomial to a set of x/y points, it means that it tries to model the relationship between these two variables with a polynomial equation.

The most common polynomial equations are the equation of a line with slope 
a
 and an intercept term 
b
:

y
=
a
x
+
b
and the quadratic function

y
=
a
x
2
+
b
x
+
c
.
The degree of the polynomial refers to the highest power of the variable 
x
 appearing in the polynomial expression. For the line, the degree is 1 since 
x
1
=
x
, while for the quadratic it is 2, as we have a 
x
2
-term.

There are two conventions for writing polynomials. In most textbooks, the polynomial starts with the highest power and ends with the intercept, the constant term. Thus the first term tells you the degree of the polynomial. However, numpy follows the convention to always start with the constant term 
c
0
 and then increase the power of x up to the highest power. This has the advantage that when we write the coefficients as a list, the 
i
t
h
 term corresponds to the coefficient of 
x
i
.

degree	polynomial equation (textbooks)	polynomial equation (numpy)
0	
y
=
a
y
=
c
0
1	
y
=
a
x
+
b
y
=
c
0
+
c
1
x
2	
y
=
a
x
2
+
b
x
+
c
y
=
c
0
+
c
1
x
+
c
2
x
2
3	
y
=
a
x
3
+
b
x
2
+
c
x
+
d
y
=
c
0
+
c
1
x
+
c
2
x
2
+
c
3
x
3
…	…	…
What happens when the degree is zero?

Well done!

y is always equal to some constant value a

The target y doesn’t depend on x

The target y increases with x
When fitting a polynomial using the polyfit() function, we need to specify the degree of the polynomial. This degree determines what kind of relationship we can model, and each individual choice of degree leads to a different model for this data. A polynomial with a higher degree can model more complex relationships than a polynomial with a lower degree. We can think about the degree as a model parameter that captures this complexity.

Let’s experiment with polynomials and test the polyfit() function with the x/y values from our data-1.csv dataset that we loaded above.

First dataset
In the previous courses, we mainly worked with Pandas DataFrame objects. This was particularly convenient since each column in our data can have a different data type.

However, when mapping some input values to an output, machine learning algorithms usually expect values from the data to have a uniform data type. Hence, they mainly work with Numpy arrays. So, before fitting our model, the first step of our ML pipeline is to prepare the x and y Numpy array variables

x = data_df.x.values
y = data_df.y.values

print("Type of x and y:", type(x), type(y))
Type of x and y: <class 'numpy.ndarray'> <class 'numpy.ndarray'>
Let’s now plot this x/y data using a scatter plot. Here is the code to do it with Matplotlib.

%matplotlib inline
import matplotlib.pyplot as plt

# Plot data points
plt.scatter(x, y)
plt.xlabel("x values")
plt.ylabel("y values")
plt.show()

In this course and the next one, we will focus on the machine learning aspect of each task and just use Matplotlib for plotting. However, feel free to use other libraries to plot your data ex. Seaborn

As you can see, the data points follow a linear trend. Hence, it makes sense to fit a line and set the degree of the Numpy polyfit() function to one.

import numpy as np
import numpy.polynomial.polynomial as poly

coefs = poly.polyfit(x, y, deg=1)
print("Coefficients:", coefs)
Coefficients: [-0.3904125   0.77788056]
In the code from above, we pass the x/y arrays to the function and use the deg attribute to set the degree of the polynomial. The function returns the set of coefficients that characterize the polynomial. In our case, the coefs array contains the intercept term 
c
0
and the slope 
c
1
 of the line defined by the equation 
y
=
c
0
+
c
1
x
.

Of course, we can also extract the coefficients as separate variables.

# Extract individual coefficients
c_0, c_1 = coefs

print("c_0 (intercept):", c_0)
print("c_1 (slope)    :", c_1)
c_0 (intercept): -0.39041249887806795
c_1 (slope)    : 0.7778805590197081
What can we say about our model?

Perfect!

The target y increases as x increases

The target y decreases as x decreases

The target y is positive when x is zero

The target y is negative when x is zero
Plotting our model
In our simple scenario, we have a single input x, and a single output y. Thus we can actually visualize our model by plotting some predictions. In this case, the equation of our model is just the equation of a straight line since deg=1. For the purpose of plotting, we first generate a few evenly spaced artificial x_values and then compute their associate y_values using the linear equation.

# Generate a hundred values between 0 and 1
x_values = np.linspace(0, 1, num=100)  # ~ [0, 0.01, 0.02, ... 0.98, 0.99, 1]

# Compute the associate y values
y_values = c_0 + c_1 * x_values

# Plot the line
plt.scatter(x, y)  # Plot the actual data
plt.plot(x_values, y_values, c="C3", label="polyfit(deg=1)")  # Plot the prediction line
plt.legend()
plt.show()

As you can see, our model fits well to the data points.

In the code from above…

That's great!

x and y contain the input data

x_values and y_values are sample x values and their estimated y values

y_values are the y values associated to x
Are there other ways to draw the red line?

Perfect!
Yes, in this case, we only need two points to draw our model since it's a line ex. try running the code with x_values = np.array([0, 1])



No

Yes
Do we get y if we compute c_0 + c_1*x?

Perfect!
We don't get y because our model doesn't perfectly fit the data. If you look at the plot, you can see that the data points in blue are not exactly on the model curve in red. c_0 + c_1*x are the estimated y values for x according to our model



Yes

No
Second dataset
We are now going to work on the second set of points which has a more complex relationship between the x/y variables. Again, let’s use Pandas to read the .csv file and load the variables into two Numpy arrays.

# Load second set of points
data_df = pd.read_csv("c3_poly_data-2.csv")
x2 = data_df.x.values
y2 = data_df.y.values
Again, we should start by visualizing the data points with a scatter plot. This time, it’s your turn to write the code! You should get the following result.


Let’s try to fit a line using the code that we have seen above.

# Fit a line
coefs2 = poly.polyfit(x2, y2, deg=1)
print('Coefficients:', coefs2)

# Generate a hundred x values between 0 and 1
x_values2 = np.linspace(0, 1, num=100)

# Compute the associate y values
c_0, c_1 = coefs2
y_values2 = c_0 + c_1*x_values2
Coefficients: [ 1.17807746 -2.25363975]
You should get the following line with a negative slope of -2.25363975.


Our line captures the main trend but is too rigid to model the relationship between the x and y values well. Let’s increase the degree and fit a polynomial of degree 3.

# Fit a polynomial of degree 3
coefs2 = poly.polyfit(x2, y2, deg=3)
print("Coefficients:", coefs2)
Coefficients: [  0.24324604   8.92987847 -29.43676159  20.52793415]
This time, the polyfit() function returns the four coefficients that correspond to the equation

y
=
c
0
+
c
1
x
+
c
2
x
2
+
c
3
x
3
.
Again, we can extract the coefficients and visualize our model by computing the y_values using the formula of our polynomial.

c_0, c_1, c_2, c_3 = coefs2
y_values2 = c_0 + c_1 * x_values2 + c_2 * (x_values2 ** 2) + c_3 * (x_values2 ** 3)
As you can see, the equation is a bit longer, and it can get complicated to compute the y values manually. For this reason, Numpy provides a polyval() function that takes the x values and the array of coefficients.

# Predict y values with new coefs
y_values2 = poly.polyval(x_values2, coefs2)
You can now plot the curve using these x_values2 and y_values2 arrays.


As you can see, a polynomial of degree 3 is better at capturing the trend in this dataset than a line. Try to increase the degree to 4, 5 or even more! You should see that the curve starts fitting more than just the trend. It gets influenced by the noise in the data. This phenomenon is known as overfitting, but we will learn more about it later in this course.

polyfit() calculates the coefficients of the polynomial that best fits our data x to our target y.
polyval() takes x values and the coefficients to calculate y values using the fitted polynomial. These y values serve as predictions.
polyfit() has a parameter deg that allows us to fix the complexity of our model.
Do we get a better model if we set num to 10 instead of 100 in the linspace() function when creating x_values2?

That's great!
We get the same model since we still fit the curve to the input data x2 and y2 i.e. we pass those variables to polyfit(). By setting num=10, we simply reduce the number of sample values to visualize the model curve with plot() - if you try, you should see that the curve in red looks less smooth. Generating a hundred evenly spaced values with linspace() is a good strategy to get a plot with a "smooth" model curve.



No

Yes
Summary
We now know how to fit a curve to a set of data points using the equation of polynomials and the Numpy polyfit() function. However, there are still many things to learn about this process. For instance,

How does the function find the coefficients?
What does it mean to compute the “least squares solution”?
To answer these questions, we need to learn more about machine learning models and the learning process. In the next units, we will start with the linear regression model.

Small task: third dataset
Before going to the next units, you should load and plot the third dataset and then try to fit a polynomial to this data.


03. Solution - polyfit with the third set of points
Content
Resources 1
Questions 1
Let’s see how to solve the small task from the last unit. Again, feel free to play with the code from this unit as you go through the text.

Solution: third dataset
First, let’s load and plot the data

import pandas as pd
import matplotlib.pyplot as plt

# Load third set of points
data_df = pd.read_csv("c3_poly_data-3.csv")
x3 = data_df.x.values
y3 = data_df.y.values

# Plot points
plt.scatter(x3, y3)
plt.xlabel("x values")
plt.ylabel("y values")
plt.show()

As we can see, the points follow a logarithmic curve that decreases. The larger x is, the slower y values decrease. For instance, we can see that y values decrease from 9.75 to 8.25 (-1.5) when x is between 0 and 50, but then decrease from 8.25 to 8 (-0.25) when x is between 50 and 100.

So, is it possible to model this set of data points with a polynomial? Before answering the question, let’s take a look at the following theorem

It’s possible to approximate any continuous function on a closed interval to an arbitrary precision with a polynomial by increasing its degree -

Weierstrass theorem
In our case, we want to approximate the logarithm function (which is a continuous one i.e. it doesn’t jump and we can draw the line without lifting the pencil) between 1 and 100 (a closed interval).

So, the answer seems to be yes - we should be able to model this data using the polyfit() function from Numpy if we pass a sufficiently high value for its degree deg parameter.

However, if we do the experiment, we can see that the resulting model curve doesn’t look like a logarithmic one. Increasing the degree simply makes the curve bumpier.

Below, we simply collected the code from the previous unit in a single function that takes the x/y values and the degree deg and plot the model with the labels and a legend.

import numpy as np
import numpy.polynomial.polynomial as poly


def fit(x, y, deg, xlabel, ylabel, legend):
    # Fit a polynomial
    coefs = poly.polyfit(x, y, deg=deg)

    # Generate a hundred values between min(x) and max(x)
    x_values = np.linspace(min(x), max(x), num=100)

    # Predict y values
    y_values = poly.polyval(x_values, coefs)

    # Plot curve
    plt.scatter(x, y)
    plt.plot(x_values, y_values, c="C3", label=legend)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.legend()
    plt.show()
And here is the result for deg=15

fit(x3, y3, 15, "x values", "y values", "polyfit(deg=15)")

So, why does increasing the degree not improve the results in this example?

It’s important to understand that the data that we try to model is not perfect. Our data does not represent points from an actual logarithm function. There is some additional noise in the data which cause the points to deviate slightly from the logarithm function. Our high-degree polynomial tries to incorporate this additional noise, too. This phenomenon is known as overfitting, and you will learn more about it in this course.

To avoid overfitting, we should use a lower degree. For instance, we can try with a polynomial of degree 2.

fit(x3, y3, 2, "x values", "y values", "polyfit(deg=2)")

A polynomial of degree 2 captures well the main trend, but it starts increasing around
x
=
80
 which doesn’t look correct. Intuitively, we would expect the points to continue decreasing after 
x
=
100
 to a value around 
y
=
7.75
.

What happens if you increase the degree to 20, 30, 50 or 100?

That's great!
We will see later in this course why polyfit() returns RankWarning messages when discussing ill-conditioning. But in short, this is a numerical issue that happens when slight changes in the data creates large variations in the model output. Usually, we solve this numerical instability by simply adding regularization, but more about this later.

It's also interesting to note that the curve can pass through the 50 data points if we set deg=49. To understand why, take the examples of a line (deg=1) and the quadratic curve from above (deg=2). How many data points can they perfectly model? The line can go through any pair of points while the quadratic curve can model any set of three points. In general, a model with p parameters (ex. the a, b coefficients in the case of a line) can perfectly fit p data points. So, when the number of model parameters p is larger or equal to the number of data points n, overfitting is very likely. This is sometimes referred to as the p >> n problem where >> means "is much greater"



Numpy returns RankWarning messages saying that Polyfit may be poorly conditioned

The curve tries to go through each point which makes it vary even more with very low or high values
Applying the logarithm transformation
To solve the task, we need to use the idea of feature transformation. We know that a polynomial equation of degree 1 can only model linear relationships.

y
=
a
x
+
b
However, if we apply the logarithm function to the 
x
 values before passing them to the polyfit() function, the formula becomes

y
=
a
log
x
+
b
Hence, it can also model a logarithmic relationship between the 
x
 and 
y
 variables.

You can think about this transformation as simply fitting a line (deg=1) to the data points from above after applying the logarithm function to the x-axis. We can see this by calling the fit() function from above with log(x3) instead of x3

fit(np.log(x3), y3, 1, "log(x)", "y values", "polyfit(log(x), y, deg=1)")

As we can see, the transformed data now follows a linear trend, and we can fit a line with polyfit(deg=1)

If we want to plot the model from above with the original scale, we can run

# Fit a polynomial
coefs = np.polyfit({1}, y3, deg=1)

# Generate a hundred values between min(x) and max(x)
x_values = np.linspace({2}.min(), {2}.max(), num=100)

# Predict y values
y_values = np.polyval(coefs, {3})

# Plot curve
plt.scatter(x3, y3)
plt.plot(x_values, y_values, c='C3', label='polyfit(log(x), y, deg=1)')
plt.xlabel('x values')
plt.ylabel('y values')
plt.legend()
plt.show()
The code from above has a few missing statements marked with {1}, {2} and {3}. What should they be to produce the plot from below?

Well done!
Yes, our new model always takes the transformed log-values as inputs - we fit it with {1}: np.log(x3), generate a hundred values between {2}: x3.min(), x3.max() and pass their log-values {3}: np.log(x_values) to polyval()



{1} should be x3

{1} should be np.log(x3)

{2} should be x3

{2} should be np.log(x3)

{3} should be x_values

{3} should be np.log(x_values)

As we can see, the curve now fits the data points better.

Summary
We now know what a polynomial fit is and how to create one for a set of x/y data points using the polyfit() function. Also, we saw the basic idea behind feature transformation with the logarithm transformation example.

In the next units, we will learn about the machine learning jargon and notation, and we will see in greater detail the simple linear regression model which is the one that the polyfit() function uses when you set its degree to one.

Quizzes
What is overfitting?

Well done!
Yes, note that a model that doesn't fit well the data is not necessarily overfitting. For instance, the polynomial of degree 2 in the example from above doesn't really fit well the data, but is not overfitting since it doesn't try to model/fit the noise in the data - the curve is simply a bad fit for this dataset.



It’s when our model starts fitting noise in addition to the trend in our data

It’s when the model doesn’t fit well the data
What is the idea behind feature transformation?

That's great!
Yes, we will see other feature transformations later in the course, but the idea is always to make the model better fit the data.



Transforming the data such that our model can better fit it

Applying the logarithm function to the data
04. ML vocabulary and notation
Content
Resources 1
Questions 1
We will now learn about the jargon and mathematical notation that you will see online when reading about machine learning. At the end of this unit, you should understand the basic vocabulary to describe datasets. For instance

What is the dimensionality and the cardinality of a dataset?
What are feature and target variables? What is an input matrix or an output vector?
What are explanatory and independent variables?
To illustrate these concepts, we are going to work on the synthetic marketing campaign dataset which is inspired by the Advertising dataset from the ISL book - An Introduction to Statistical Learning, from James, G., Witten, D., Hastie, T., Tibshirani, R. - If you want to take a look at the book, you can find a free PDF version online on the book website.

Scenario
In this imaginary scenario, our task is to provide advice to a company to organize a marketing campaign and increase the sales of a particular product. The company may invest in different advertising media: television, web, and radio. To achieve this, we will use data from previous marketing campaigns collected by the company.

In this unit and the next ones, our goal is to fit a linear regression model to the data and predict sales given the budgets of each advertising media.

Important note: In this course, we will focus on the prediction aspect of the linear regression model i.e make accurate predictions. But if you want to go further and also learn about the interpretation aspect of this model, i.e. draw conclusion from data, we recommend you to read chapter 3 of the ISL book. However, note that the interpretation aspect of linear regressions requires a good background in statistics.

Find the tasks that are predictive ones

Congratulations!
Drawing conclusions from data (interpretation aspect) is not easy and requires substantive knowledge about the task. We should always remember that ML models try to fit the data, but don't have access to the expert/domain knowledge ex. drawn from exploratory data analysis (EDA), A/B tests.



Build a ML model to classify user reviews by type

Understand what leads to a positive/negative user review

Detect patients with a certain disease

Analyze the factors that lead to a certain disease
Marketing campaign dataset
Let’s take a look at the dataset.

import pandas as pd

data_df = pd.read_csv("c3_marketing-campaign.csv")
data_df.head()

Each row is a previous marketing campaign and has four values. The first three are the corresponding budgets for each media in thousands of dollars, and the last one is the sales in thousands of units.

Let’s plot the data using a scatter plot for each marketing budget.

%matplotlib inline
import matplotlib.pyplot as plt

# Create figure
fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(12, 3), sharey=True)
fig.suptitle("marketing budgets in 1000$")
ax1.scatter(data_df.tv, data_df.sales)
ax1.set_xlabel("tv")
ax1.set_ylabel("sales in thousands")
ax2.scatter(data_df.web, data_df.sales)
ax2.set_xlabel("web")
ax3.scatter(data_df.radio, data_df.sales)
ax3.set_xlabel("radio")
plt.show()

As we can see, there is a clear correlation between the television budget and the sales. However, before fitting models to this data, let’s learn about the basic jargon and notation to describe datasets.

Machine learning jargon
The data_df DataFrame has four columns and 50 rows.

print("Shape of data_df:", data_df.shape)
Shape of data_df: (50, 4)
Each column corresponds to a variable and each row to an observation or measurement of these variables. We assign different names to them depending on their role in our machine learning task.

For instance, in our example, we want to predict the sales given the different budgets. Hence, the sales variable is called the target or output variable. On the other hand, we compute predictions based on the budget variables. Hence, the tv, web and radio budgets are the features or input variables in our problem.

Hopefully, we measured relevant features that explain well the target variable. For this reason, features are sometimes called the explanatory variables. On the other hand, we could say that the target variable depends on the set of input variables that we measured, so we sometimes call it the dependent variable.

In our marketing campaign scenario

That's great!

TV, web and radio budgets are explanatory variables

Tv, web and radio budgets are called the independent variables

The number of sales is called the dependent or target variable

We have three features and one target variable
Usually, we have a single target variable and many features. However, it’s also possible to have many output variables. For instance, in the next course, we will classify images of cars, bikes, trucks using neural networks. In this case, it makes sense to have a target for each category, and we end up with three target variables: is_a_car, is_a_bike and is_a_truck

Let’s summarize this new vocabulary with the data_df DataFrame


As you can see, the number of features is also referred to as the dimensionality of our dataset, and the number of rows as its cardinality.

What is the cardinality of the marketing campaign dataset?

Congratulations!

50

4

3
The Numpy polyfit(x, y, deg) function

Perfect!
That's right!



fits a polynomial to a single input/output pair of variables

works when there are several features

works when there are several outputs to predict
Feature matrix and target vector
In this course, we will always start by loading the features into a feature matrix X and the target values into an output vector y. We will need these two variables when applying machine learning algorithms.

X = data_df.drop("sales", axis=1).values
y = data_df.sales.values
print("Shape of X: {} dtype: {}".format(X.shape, X.dtype))
Shape of X: (50, 3) dtype: float64
print("Shape of y: {} dtype: {}".format(y.shape, y.dtype))
Shape of y: (50,) dtype: float64
As you can see, we extract the three features into a variable X by dropping the sales column, and we create a target vector y from it. We then convert them into Numpy arrays using the .values attribute.

Why do we convert X, y into Numpy arrays?

That's great!
Yes, we usually need to encode non-numerical values into a numerical format before passing them to ML models. For this reason, ML libraries such as Scikit-learn usually expect to get data as Numpy arrays, but we will learn more about that later in the course.



ML algorithms can usually work out-of-the-box with variables that have different data types ex. numerical and non-numerical

To make sure that we have a uniform numerical data type for all variables
Note that we cannot use the loc() and iloc() functions since X/y are not Pandas objects anymore. However, we can use the standard slicing notation from Numpy to get the data points

X[:5, :]
array([[ 0.916,  1.689,  0.208],
       [ 9.359,  1.706,  1.071],
       [ 5.261,  2.538,  2.438],
       [ 8.682,  2.092,  1.283],
       [11.736,  1.66 ,  1.8  ]])
y[:5]
array([1.204, 4.8  , 3.97 , 5.212, 5.993])
From this output, can you give the number of sales associated to the second data point?

Congratulations!

No, the output doesn’t show this info

4,800

1,071

1,706
Let’s now learn about the basic mathematical notation that we will find in most machine learning resources to denote the input/output data.

Mathematical notation
The mathematical notation is an essential part of the machine learning vocabulary. It allows expressing ideas in a very concise way. In this section, we will go through the important symbols to describe datasets.

First, the 
X
 variable refers to the input or feature matrix. For instance, here are the first five rows of 
X
 for the marketing campaign dataset.

X
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0.916
1.689
0.208
9.359
1.706
1.071
5.256
2.538
2.438
8.682
2.092
1.283
11.736
1.66
1.8
…
…
…
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
Each row in this matrix is a data point. We denote the i-th data point with the vector 
⃗
x
i
. For instance, the second data point is

⃗
x
2
=
⎛
⎜
⎝
9.359
1.706
1.071
⎞
⎟
⎠
Each element of this vector is the value of a feature. We use the 
x
i
j
 notation to refer to the value of the j-th feature of the i-th data point. For instance, the radio budget of the second data point is

x
23
=
1.071
To summarize, we can write the 
X
 matrix using an 
⃗
x
i
 vector for each data point or by enumerating the 
x
i
j
 values.

X
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
⃗
x
⊺
1
⃗
x
⊺
2
⋮
⃗
x
⊺
n
⎞
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
x
11
x
12
…
x
1
p
x
21
x
22
…
x
2
p
⋮
⋮
⋮
⋮
x
n
1
x
n
2
…
x
n
p
⎞
⎟
⎟
⎟
⎟
⎟
⎠
In the first notation, we need to transpose each feature vector 
⃗
x
⊺
i
 to get row vectors. Remember that the rows in 
X
 correspond to data points.

⃗
x
2
=
⎛
⎜
⎝
9.359
1.706
1.071
⎞
⎟
⎠
⃗
x
⊺
2
=
(
9.359
1.706
1.071
)
We usually refer to the cardinality of the dataset with the letter 
n
 and to the dimensionality with 
p
. In our example, there are 
n
=
50
 data points, and 
p
=
3
 parameters (or features). Hence, 
x
n
p
 refers to the last feature of the last data point. Note that some sources also use a capital 
N
 to denote the number of data points, and the letters 
M
, 
P
 or 
D
 for the dimensionality.

What is 
⃗
x
⊺
i
?

Perfect!

The feature values of the i-th data point

The i-th row of the feature matrix

Equal to X[i-1] in Numpy bracket notation

Equal to X[i-1, :] in Numpy bracket notation

Equal to X[:, i-1] in Numpy bracket notation
In the next exercise, you will create a model to predict the daily number of users of a bike sharing service based on temperature values. In that case, the temperature variable is the only feature, and it doesn’t make sense to write each data point as a vector 
⃗
x
i
 with a single value. Hence, we will omit the arrow, and denote the i-th data point with 
x
i
. For instance,
x
n
 corresponds to the last temperature value in the dataset.

We use a similar notation for target values. We group them in an output vector 
⃗
y
 in which each element 
y
i
 refers to the target value of the i-th data point. In our example, the target vector is

⃗
y
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1.204
4.8
3.97
5.212
5.993
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
and the sales of the second data point is 
y
2
=
4.8
.

Let’s say that we are building an ML model to detect a disease. To achieve this, a hospital measured ten factors for a thousand patients and labeled them by severity of the disease.

Well done!

Our feature matrix has 1,000 rows and 10 columns

Our target data is a vector with 1,000 entries (disease severity)

v
e
c
x
1
 is the vector with the 10 measured factors for the first patient in our dataset

y
1
 is the corresponding disease severity for this patient

The dimensionality of the input data is 10

y
i
j
 is the value of the j-th factor for the i-th patient
Summary
In this unit, we saw the basic vocabulary and notation to describe machine learning datasets. In the next unit, we will do a small quiz using examples from ML articles, tutorials and documentation pages that you can find online.

In machine learning the input 
X
 is known as the feature matrix. Its columns may be referred to as
- input variables,
- features,
- explanatory variables, or
- independent variables.

The column vector of 
y
 is known as the
- output,
- target, or
- dependent variable

The shape of 
X
 is given by the cardinality (number of rows) and the dimensionality (number of columns).

06. Simple linear regressions
Content
Resources 1
In a previous unit, we fitted several polynomials of different degrees to a set of x and y data points using the Numpy polyfit(x, y, deg) function. We saw that it fits a line when we set the degree to one and returns the optimal a and b coefficients. In fact, this corresponds to a simple linear regression model. In this unit, we will learn more about it and see what it means for the a and b parameters to be optimal.

Simple linear regressions
First, let’s start by loading the marketing-campaign.csv dataset. The simple linear regression model works with a single feature. However, the dataset has three input variables, one for each media budget

Television
Web
Radio
Hence, we need to choose which one to use with this model to predict the sales target variable.

In a previous unit, we plotted each advertising budget and saw that there is a clear correlation between the television budget and sales. Hence, it’s a good idea to start with this feature. In this code, we load the dataset into a DataFrame and create the x and y Numpy arrays.

import pandas as pd

# Load the dataset
data_df = pd.read_csv("c3_marketing-campaign.csv")

# Extract x/y values
x = data_df.tv.values
y = data_df.sales.values

print("x shape:", x.shape)
print("y shape:", y.shape)
x shape: (50,)
y shape: (50,)
We can now plot the x and y variables with a scatter plot.

%matplotlib inline
import matplotlib.pyplot as plt

# Plot data
plt.scatter(x, y)
plt.xlabel("tv budget in 1000$")
plt.ylabel("sales in thousands")
plt.show()

It seems that an increase in the television budget creates a proportional increase in sales which is precisely what a line expresses geometrically. The idea of simple linear regressions is to quantify this linear relationship by fitting the equation of a line to the data points

^
y
=
a
x
+
b
In this equation, the slope 
a
 defines the direction of the line and the intercept term 
b
 its position. Once we find 
a
 and 
b
 values that fit well the observed 
(
x
i
,
y
i
)
 data points, we can compute predictions 
^
y
 (pronounce y hat) given new input values 
x

Say that we fit our simple linear regression model to the 50 data points from above and estimate that a=0.5 and b=1 - what would the model prediction 
^
y
 be for a television budget of 8 thousand dollars?

Perfect!
Yes, the model predicts y = ax+b = 0.5*8+1 = 5 thousand units



6 thousand units

5 thousand units

4 thousand units

3 thousand units
Polyfit function
Let’s see again how to fit this simple linear regression model with the polyfit() function

import numpy as np

# Fit a linear regression
coefs = np.polyfit(x, y, deg=1)
print("Coefficients:", coefs)
Coefficients: [0.42063597 1.27867727]
# Generate a hundred data points
x_values = np.linspace(x.min(), x.max(), num=100)

# Compute predictions
y_values = np.polyval(coefs, x_values)
In this code, we pass the 
(
x
i
,
y
i
)
 data points to the polyfit() function and set the degree to one to fit the equation of a line. The function computes these optimal 
a
, 
b
 parameters and we store them in a coefs variable. We can then use these values to compute predictions 
^
y
for a hundred different television budgets 
x
 between x.min() and x.max() using the polyval() function, which evaluates the equation from above.

Here is a plot of the x_values and y_values values.

# Plot predictions
plt.scatter(x, y)
plt.plot(x_values, y_values, c="C3", label="linear regression")
plt.xlabel("tv budget in 1000$")
plt.ylabel("sales in thousands")
plt.legend()
plt.show()

To summarize, simple linear regressions model the relationship between a single feature and the target variable using the equation of a line which depends on two parameters: a and b.

What can we say about the simple linear regression model?

Well done!
Later in this course, we will see how we can generalize simple linear regressions to multiple input features. We will then call the model multiple linear regressions or just linear regressions since having multiple features is the standard scenario in ML.



We say that it’s “simple” because it models the relationship between a single input and output pair

Geometrically, it corresponds to fitting a line

We can use the polyfit(x, y, deg=1) function to estimate the a, b coefficients

polyfit() can only fit simple linear regressions
Optimal parameters
Let’s now see what the polyfit() function optimizes when it fits this equation of a line. The Numpy documentation page of the polyfit() function states that it optimizes the squared error

“Returns a vector of coefficients p that minimizes the squared error.” -

Numpy documentation
By fitting a line, we approximate the data points. The error term in this sentence refers to the difference between our approximation 
^
y
 (y hat) and the observed value 
y
 - also called the residual 
r
i

r
i
=
(
y
i
−
^
y
i
)
Hence, the function minimizes the squares over our set of 
n
 data points

RSS
=
n
∑
i
=
1
 
(
y
i
−
^
y
i
)
2
This quantity is known as the Residual Sum of Squares (RSS) and is a standard way to measure how much error a machine learning model makes.

Let’s say that we want to evaluate how well our simple linear regression model from above fits the data. To achieve this, we take the following three 
(
x
,
y
)
 data points: (2, 1) (6, 4) (10, 8) and pass them to our model that makes the following predictions 
^
y
: 2 4, 6. What is the corresponding RSS error score?

Perfect!
Yes, if we sum the squares of the residuals

r1 = 1-2 = -1  ---> r1^2 = 1
r2 = 4-4 = 0   ---> r2^2 = 0
r3 = 8-6 = 2   ---> r3^2 = 4
we obtain RSS = 1 + 0 + 4 = 5



3

4

5

6
Let’s say that we have a second model with an RSS score of 8, which one is better according to this metric?

Perfect!
Yes, RSS is a measure of error i.e. it measures "how much" error the model does - lower scores indicate better models!



The second one - larger RSS scores are better

The first model - lower RSS scores are better
Measuring the amount of error that our models do is really at the hearth of machine learning approaches. In this course, we will see different metrics. But the RSS is one of the most used one when evaluating regression models.

You can take a look at this interactive visualization by Evan Sadler to see how the optimal line changes to minimize these squared residuals when adding new data points.

Compute the RSS score
Let’s now see how to compute this RSS measure for the model and data from above using Python. First, we need to compute the predictions 
^
y
i
 for each input value 
x
i

# Compute predictions for each data point
y_pred = np.polyval(coefs, {1})
What should {1} be in this code?

Well done!
Yes, we want to compute the RSS score for the data x that we have - not for the hundred sample values x_values generated to visualize the line



x_values

x
Let’s now write a function to compute the RSS score between the predictions y_pred and the target values y

def RSS(y, y_pred):
    return np.sum(np.square(y - y_pred))
In this function, what does y - y_pred computes?

Perfect!
In this code, y - y_pred returns the array of residuals. We can then square each value with np.square() and sum the results with np.sum()



the array of residuals of shape (50,)

how far each prediction is from the target value

how far each prediction is from the target value on average

the RSS value
Let’s now use the function to compute the RSS value of our model for the marketing data.

print("RSS value:", RSS(y, y_pred))
RSS value: 15.739481499345722
As we can see, our simple linear regression model has an RSS score around 15.7

By slightly changing the model coefficients coefs - can you get a better RSS score? i.e. a lower score

Congratulations!
This is correct, the coefficients coefs returned by polyfit() are already optimal according to the RSS metric - so it's not possible to get a better RSS score. You can try with ex.

modified_coefs = [0.4, 1.3]
y_pred = np.polyval(modified_coefs, x)
print('RSS value:', RSS(y, y_pred)) # 16.48706488 which is larger


No

Yes
Visualizing the RSS metric
A nice way to visualize these squared residuals is to draw squares between the observed data points and the predicted values on the line - here is an interactive visualization by Victor Powell and Lewis Lehe which does that.

Try to adjust the dials for the slope and intercept a,b parameters and see how they affect the RSS score!


Summary
In this unit, we saw that simple linear regressions model the relationship between a single feature and the target variable using the equation of a line. Also, we learned that the model minimizes the residual sum of squares (RSS) error.

It’s important to understand that this model assumes that there is a linear relationship between the two variables. More generally, each model has its assumptions about the data and performs best when these assumptions are met.

02. Issues with outliers
Content
Resources 1
Questions 4
In the last subject, we fitted polynomials to different datasets using the polyfit() function, and we saw that it’s minimizing the residual sum of squares (RSS) measure. In this unit, we will see that this process is sensitive to extreme points called outliers due to the nature of the RSS measure. At the end of this unit, you should have the necessary tools to detect and remove outliers using Numpy functions.

This time, we will work with a modified version of the marketing campaign dataset in which we added five outliers.

Issues with outliers
Let’s start by loading the dataset.

import pandas as pd

data_df = pd.read_csv("c3_marketing-outliers.csv")
data_df.shape
(55, 4)
The first 50 rows of the DataFrame contain the 50 data points from the marketing-campaign.csv dataset that we saw in the previous units, and we added five rows at the end of the DataFrame. They are outliers in the sense that they are not part of the trend that we can observe in the other data points.

Let’s print these outliers using the Pandas tail() function.

# Last five data points
data_df.tail()

As you can see, rows 50 to 52 have a small television budget but large sales, and rows 53 and 54 have a large television budget but small sales. Since these outliers are at the end of the DataFrame, we can easily separate them from the rest of the points.

%matplotlib inline
import matplotlib.pyplot as plt

# Extract tv and sales
x = data_df.tv.values
y = data_df.sales.values

# Plot the data points
plt.scatter(x[:-5], y[:-5], label="normal")
plt.scatter(x[-5:], y[-5:], c="C3", label="spurious")
plt.legend()
plt.show()

In this code, we plot the last 5 data points (the outliers) using plt.scatter(x[-5:], y[-5:]) and the first 50 “normal” points using plt.scatter(x[:-5], y[:-5]).

Note that these outliers are not necessarily incorrect values. The difference could be due to a phenomenon that does not appear in our data, i.e., unobserved variables.

The problem with outliers is that a few of them can have a significant effect on the final model. To illustrate this issue, let’s compare the coefficients of a linear regression on the entire dataset with those from a linear regression fitted without the five outliers.

import numpy as np

# Fit a linear regression
coefs = np.polyfit(x, y, deg=1)
print("coefs:", coefs)
coefs: [0.20613307 2.76540858]
# Fit a linear regression without the 5 outliers
coefs_wo = np.polyfit(x[:-5], y[:-5], deg=1)
print("coefs without outliers:", coefs_wo)
coefs without outliers: [0.42063597 1.27867727]
The two lines a very different. Their slope and intercept terms are different by a factor of two. Since we have just two variables, we can also plot the two models and compare the prediction lines.

# Compute prediction line
x_values = np.linspace(min(x), max(x), num=100)
y_values = np.polyval(coefs, x_values)
y_values_wo = np.polyval(coefs_wo, x_values)

# Plot them
plt.scatter(x[:-5], y[:-5])
plt.scatter(x[-5:], y[-5:], c="C3")
plt.plot(x_values, y_values, c="C3", label="lr with outliers")
plt.plot(x_values, y_values_wo, c="C2", label="lr without outliers")
plt.xlabel("tv budget in 1000$")
plt.ylabel("sales in thousands")
plt.legend()
plt.show()

In this code, we create a hundred points between min(x) and max(x) with the Numpy linspace() function, and compute the sales estimations using the model coefficients coefs and coefs_wo.

As you can see, the red line doesn’t fit well the trend in the data points in blue. This is because the polyfit() function minimizes the RSS measure which puts too much weight on large errors. In fact, by squaring the residuals, it amplifies the error for points 
y
i
 far away from the predictions 
^
y
i
.

R
S
S
=
n
∑
i
=
1
 
(
y
i
−
^
y
i
)
2
To better understand the issue, let’s compare the RSS score of the five outliers in red to the other points in blue.

# Compute predictions for all data points
y_pred = np.polyval(coefs, x)

# Compute the squares of residuals
squares_residuals = np.square(y - y_pred)

print("RSS normal points:", np.sum(squares_residuals[:-5]))
print("RSS outliers:", np.sum(squares_residuals[-5:]))
RSS normal points: 39.533764623373486
RSS outliers: 115.6176164792112
You can see that most of the error comes from the five outliers, and this is why they have such a large impact on the final model in red.

Remove outliers by hand
In our example, we know that the issue comes from the tv and sales variables, and we can easily separate the outliers from the rest of the data points by defining two regions.

Data points with an x value below 4 and a y value above 6. This corresponds to the upper left corner of the plot.
Data points with an x value above 10 and a y value below 2. This corresponds to the lower right corner.
Here is a plot of the two regions.


The idea is to create an array idx of booleans with True values for data points that are in one of these two regions. To achieve this, we can use the Python bitwise operators & (and) and | (or).

# Select outliers
idx = ((x < 4) & (y > 6)) | ((x > 10) & (y < 2))
Let’s discuss each component of this logical expression. In the first pair of parentheses, we detect values in the x array smaller than 4.

x < 4
array([ True, False, False, False, False, False, False,  True,  True,
       False, False, False, False,  True,  True, False,  True, False,
       False,  True, False, False,  True, False, False,  True, False,
       False, False, False,  True, False, False, False, False, False,
       False,  True, False,  True, False, False, False, False, False,
       False, False, False,  True, False,  True,  True,  True, False,
       False])
The expression returns an array of booleans, and each value corresponds to a data point. For instance, we can see that the first data point has an x component smaller than four.

In the second pair of parentheses, we evaluate the condition on the y values.

y > 6
array([False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
        True, False, False, False, False, False,  True, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False,  True,  True,  True, False,
       False])
This time, we obtain an array of booleans with True values for data points with a y component above 6. We can now combine these two conditions using the bitwise and operator &.

# Points in the upper left corner
(x < 4) & (y > 6)
array([False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False,  True,  True,  True, False,
       False])
This array has a True value for data points with an x component below 4 and a y component above 6. We can see that there are three True values which correspond to the outliers in the upper left corner.

Note that we need to write the parentheses in this expression due to the operator precedence mechanism in Python. In short, operators in an expression are evaluated with a specific order. For instance, the & operator has a higher precedence than the < and > ones. Hence, this operator is evaluated before the two others and we get an error if we remove the parentheses.

try:
    # This is an invalid expression!
    x < 4 & y > 6  # equivalent to (x < (4 & y)) > 6
except TypeError:
    print("Type error!")
Type error!
You can take a look at this page to learn more about the operator precedence in Python. In any case, it doesn’t hurt to add the parentheses.

We can now write the expression for the second region.

# Points in the lower right corner
(x > 10) & (y < 2)
array([False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False,  True,
        True])
This time, the array has a True value for the two outliers in the lower left corner.

Finally, we can combine the two arrays using the bitwise or | operator.

# Points in the upper left or lower right corners
idx = ((x < 4) & (y > 6)) | ((x > 10) & (y < 2))
idx
array([False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False,  True,  True,  True,  True,
        True])
Note: As an alternative to bitwise operators, Numpy offers many differentlogical functions that allow you to write boolean expressions in a more compact and secure way. For example, the expression above could also be written as:

np.logical_or(np.logical_and(x < 4, y > 6),
              np.logical_and(x > 10, y < 2))
array([False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False,  True,  True,  True,  True,
        True])
We can now use this idx array to filter outliers.

# Extract "normal" points
x1, y1 = x[~idx], y[~idx]
print("x1/y1:", x1.shape, y1.shape)

# Extract outliers
x2, y2 = x[idx], y[idx]
print("x2/y2:", x2.shape, y2.shape)
x1/y1: (50,) (50,)
x2/y2: (5,) (5,)
In this code, we use the bitwise complement ~ operator to invert the boolean values and select the “normal” points.

# "normal" points
~idx
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True, False, False, False, False,
       False])
In our example, we can identify and filter the outliers manually. However, this might be more complex in other cases. Also, defining these two regions is somehow arbitrary and we might want to have a more standard definition of what an outlier is. We are now going to see the standard score or z-score measure which quantifies how extreme a value is.

Standard scores
Intuitively, an outlier is a value far from the other ones. The standard score formalizes this idea using two statistics: the mean 
μ
 and the standard deviation 
σ
.

The idea is to check the distance between each value and the center of the distribution with
(
x
−
μ
)
. Note that this quantity depends on the dispersion of the data. For instance, values that have a large spread will be further away from the mean than values with a smaller spread. For this reason, the z-score (or standard score) standardizes this distance by the standard deviation 
σ
 which quantifies the dispersion of the values.

z
=
x
−
μ
σ
Let’s compute the z-scores for the y values in our marketing campaign dataset.

# Compute z-scores
z_scores = (y - y.mean()) / y.std()

print("z-scores:", z_scores.shape)  # (55,)
z-scores: (55,)
In this code, the variable y contains the 55 target values. We compute the mean and the standard deviation using the Numpy mean() and std() functions. Due to broadcasting, Numpy computes the z-score of each element in the y array and returns an array of shape (55,).

We can now visualize these scores using the scatter() function from Pyplot and its c argument.

# Plot z-scores
plt.scatter(x, y, c=z_scores, cmap="coolwarm")
plt.xlabel("tv budget in 1000$")
plt.ylabel("sales in thousands")
plt.colorbar()
plt.show()

We added the bar on the right using the colorbar() function. You can also specify the colormap using the cmap argument. Here is the code using Seaborn.

import seaborn as sns

# Activate Seaborn style
sns.set_style("darkgrid")

# Plot z-scores
plt.scatter(x, y, c=z_scores, cmap="turbo")
plt.colorbar()
plt.show()

As you can see, most of the data points have a score between 
−
1.5
 and 
1.5
. The three outliers in the top left corner have a standard score greater than 
2
.

We can now set the threshold on these scores. We usually take the normal distribution as a point of reference to define this threshold. For a variable that follows a normal distribution, this image shows the cumulative percentage which is the probability of being below a certain threshold.


Source - adapted from Wikipedia

For instance, 99.9% of the data is below three standard deviations above the mean, or alternatively, 0.1% of the data is above this point.

As you can see, no value has probability zero to occur. It’s just less likely to have a point many standard deviations away from the mean. In other words, by setting a threshold to remove the outliers, we will lose some regular data points. For instance, by setting it to two, we will expect to waste 
2
∗
2.3
=
4.6
%
 of the data, .i.e., 2.3% above and 2.3% below two standard deviations.

In our case, we have 55 data points, and we can afford to lose 5% of it which is less than 3 points.

# Select outliers
idx = np.abs(z_scores) > 2
idx
array([False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False,  True,  True,  True, False,
       False])
This time, we only detected the three outliers in the top left corner of the image. Here is the idx mask.

If we remove the outliers using the idx mask and fit a linear regression, we can see that the linear regression is very close to the one obtained by manually removing all the outliers.

# Fit linear regression
coefs_z = np.polyfit(x[~idx], y[~idx], deg=1)

# Prediction line
y_values_zscore = np.polyval(coefs_z, x_values)

# Final comparison
plt.scatter(x[~idx], y[~idx])
plt.scatter(x[idx], y[idx], c="C3")
plt.plot(x_values, y_values_wo, c="C2", label="manual threshold")
plt.plot(x_values, y_values_zscore, c="C3", label="lr without outliers")
plt.xlabel("tv budget in 1000$")
plt.ylabel("sales in thousands")
plt.legend()
plt.show()

Note that we computed the z-scores on the sales target variable. However, we can apply this process to each variable in our dataset, i.e., including the features.

Consider these two transformations. Do they modify the standard scores?

(A) Multiplying the variable by a constant 
c
. For instance, changing the unit of the television budget variable from thousands of dollars to dollars, i.e., 
c
=
1000
(B) Multiplying the variable by a constant 
c
1
 and adding another constant 
c
2
. For instance, you measure a temperature variable in degrees Celsius and you convert it to degrees Fahrenheit, i.e., 
c
1
=
1.8
 and 
c
2
=
32
That's great!
Yes, linear transformations 
f
(
x
)
=
a
∗
x
+
b
 with 
a
>
0
 don't modify the z-scores. In the first case, by multiplying the variable by a constant 
c
>
0
, both the mean and the standard deviation increase by 
c
 which doesn't modify the results. Adding a constant 
c
 shifts the mean but doesn't change the dispersion of the data which also doesn't modify the results.



Yes for both

Yes for (B) but no for (A)

Yes for (A) but no for (B)

No in both cases
Summary
Let’s summarize what we’ve learned in this unit. Here are a few takeaways.

When minimizing the squared residuals (RSS), we need to be careful about outliers because the RSS measure amplifies their effect on our models
If possible, we should try to remove these outliers by hand
However, this is not always possible, and the process is somehow arbitrary. One solution is to set a threshold on the standard scores
In the next units, we will see two alternatives to the RSS measure that work better with outliers: the mean absolute error (MAE) and Huber loss.

03. Mean absolute error
Content
Resources 1
Questions 6
In the last unit, we saw that a few outliers can have a significant effect on our models due to the nature of the RSS measure. In this unit, we will learn about the mean absolute error (MAE) metric which works better than RSS in the presence of outliers.

Mean absolute error
In the previous units, we saw that, when fitting a model, we find the model parameters by optimizing an error metric. For instance, the polyfit() function minimizes the residual sum of squares (RSS) measure.

R
S
S
=
n
∑
i
=
1
 
(
y
i
−
^
y
i
)
2
These error metrics have many different names. For instance, we call them cost functions, loss functions or objective functions. The idea is that they associate a cost to each prediction 
^
y
i
 that we want to minimize.

The RSS metric is part of a class of cost functions that measure the squared errors. This category includes the MSE function that computes the mean of the squared errors and the RMSE function which is the square root of the MSE cost function. It’s important to understand that these measures are equivalent in the sense that we would get the same optimal parameters using the RSS, the MSE or the RMSE cost functions to fit our model. Hence, they have the same issues with outliers.

An alternative to these cost functions is the mean absolute error (MAE) metric which measures the absolute values of the residuals.

M
A
E
=
1
N
n
∑
i
=
1
 
|
y
i
−
^
y
i
|
Here is a plot that compares the MSE and MAE cost functions.


We can see that both cost functions evaluate to zero when the predicted value is equal to the observed one. Also, they are symmetric and assign the same cost to a negative or a positive error of the same amplitude.

One difference is the rate at which they grow. The MSE cost function increases quadratically whereas the MAE one increases linearly. Hence, the MAE metric works better with outliers since it doesn’t overweight them. However, it’s difficult to use this function in practice because it is not smooth around zero. Its slope changes abruptly from -1 to +1 due to the absolute value which makes it harder to find the optimal parameters.

In summary, the RSS-like and the MAE cost functions have both their advantages. One is robust to outliers while the other is smooth. This is known as the statistical versus computational trade-off.

Statistical vs. computational trade-off
The MAE function is robust to outliers which is a desired statistical property. However, it’s not smooth. To illustrate the issue, we can compare the error surfaces of the MAE and MSE cost functions for an arbitrary dataset.

Let’s start with the error surface of the MAE cost function.


To generate this 3-dimensional plot, we took the simple linear regression model and evaluated the MAE function for a dataset with three x/y data points. The x- and y- axes correspond to the slope 
a
 and intercept 
b
 parameters of the simple linear regression model, and the z-axis corresponds to the value of the cost function at each 
(
a
,
b
)
 points after computing the predictions 
^
y
i
=
a
x
+
b
 for this dataset (defined arbitrarily)

(
x
,
y
)
=
{
(
1
,
2
)
,
(
0
,
1
)
,
(
−
1.5
,
0
)
}
We can also plot it using a contour map, with contour lines, which corresponds to a top view of the 3-dimensional plot.


As you can see, the error surface is a set of planes that intersect each other which makes it difficult for optimization algorithms to find the optimal parameters. For instance, we will see later in this course the gradient descent method that “walks down” on this error surface by following the direction of the steepest descent. In this case, it’s difficult for the algorithm to find the optimal solution since it “jumps” from one plane to the others.

This problem doesn’t occur with the MSE cost function.


In that case, the error surface is smooth, and the contour lines are continuous.


We say that smoothness is a computational property because it makes computations easier. Later in this course, we will talk about convexity which is another desired computational property because it ensures that optimization algorithms such as gradient descent don’t get stuck in local optimum.

Summary
Let’s summarize what we’ve learned in this unit. Here are a few takeaways.

When fitting a model, we optimize a cost/loss/objective function
The MAE is robust to outliers which is a nice statistical property but is difficult to optimize because it’s not smooth.
MAE is easy to interpret. For instance, a score of 3 means that the predictions are, in average, above or below the observed value by a distance of 3.
In the next unit, we will learn about the Huber loss which combines the two properties from above. Also, we will see how to fit a model using this cost function with Scikit-learn.04. Huber loss
Content
Resources 1
Questions 6
We will now learn about the Huber loss which is a way to combine the RSS and MAE cost functions. At the end of this unit, you should be able to fit models with this objective function using objects from the Scikit-learn library.

Huber loss
Huber loss is a cost function which is robust to outliers and smooth. In fact, its formula uses both the absolute value and the square function.

L
(
e
)
=
{
1
2
e
2
if 
|
e
|
≤
δ
δ
(
|
e
|
−
1
2
δ
)
otherwise
In our case, the variable 
e
 refers to the residual 
e
=
(
y
−
^
y
)
 and 
L
(
e
)
 is the loss value of a single data point. The formula defines two cases using a threshold 
δ
 (delta).

When the magnitude of the residual is below 
δ
 (small error), the function increases quadratically like the RSS one and then, for larger errors, linearly like the MAE one. The 
δ
value determines when the function becomes linear. Here is a plot of the Huber loss with different threshold values.


As we can see, the 
δ
 value controls the robustness to outliers. For instance, a Huber loss function with 
δ
=
0.5
 is less sensitive to outliers than one with 
δ
=
1.5
. Also, the function is smooth in the sense that its derivative is continuous. The slope of the curve is 
e
 below the threshold and 
±
δ
 above it.

Implementation with SGDRegressor
In practice, it’s a good idea to try to use the Huber loss if there are outliers in the data. Let’s test it on our modified marketing campaign dataset

import pandas as pd
from sklearn.preprocessing import scale

# Load data
data_df = pd.read_csv("c3_marketing-outliers.csv")
x = scale(data_df.tv.values)  # SGDRegressor requires data to be rescaled
y = data_df.sales.values
Here, we load the TV vs. Sales data and rescale the input data with the scale() function. We will come back to this point later, but in short, some algorithms like the one that we will use in this unit require the data to be preprocessed in this way.

The Numpy polyfit() function doesn’t implement Huber loss. Hence, we will now switch to the Scikit-learn library which provides many machine learning models. The workflow is a bit different with this library. Instead of calling functions directly, we create objects that we fit to datasets.

Let’s start with the SGDRegressor object. First, we need to import it from the linear_model module from Scikit-learn.

from sklearn.linear_model import SGDRegressor
The name of the object comes from the optimization algorithm that it uses to find the optimal parameters. The SGDRegressor object implements the stochastic gradient descent (SGD) algorithm which is a very generic algorithm that can minimize a great variety of cost functions. For this reason, the object has many parameters.

By default, it minimizes the square of the residuals plus a penalization term. As for now, let’s remove the penalization term by setting penalty='none'. We can then change the loss function to Huber with loss='huber' and set the threshold value 
δ
 with the epsilon parameter.

# Create a linear regression with Huber loss
lr_huber = SGDRegressor(
    loss="huber", penalty="none", epsilon=1, max_iter=1000, tol=1e-3
)
The penalization term is useful to fight overfitting. In the machine learning vocabulary, it is called a hyperparameter. We will learn more about it later in this course.

SGD is an iterative algorithm that tries to minimize the objective function by taking small steps in the direction of the optimal solution. For this reason, we need to specify when it stops. With the code from above, SGD stops when the loss stops improving (tol parameter) or when it reaches some maximum number of iterations (max_iter one). Regarding the values, we simply use the defaults, recommended ones from Scikit-learn.

We can now fit the model to the data with its fit() function.

import numpy as np

# Fit the model
lr_huber.fit(x[:, np.newaxis], y)
SGDRegressor(epsilon=1, loss='huber', penalty='none')
In machine learning, datasets usually have several features. For this reason, Scikit-learn objects expect to get a 2-dimensional array of features. In our case, the x variable is a vector with shape (55,), but we can convert it to a matrix of shape (55,1) by adding a new axis with the [:, np.newaxis] syntax.

When Scikit-learn fits the model, it stores the optimal parameters in a coef_ and an intercept_ attribute.

# Print the slope of the line
print("Slope:", lr_huber.coef_)
Slope: [1.12800659]
# Print the intercept parameter
print("Intercept term:", lr_huber.intercept_)
Intercept term: [3.78993337]
If you run the fit() instruction from above several times and print the parameters, you should see that they vary slightly between each run. This is due to the stochastic nature of the SGD algorithm. We will learn more about the gradient descent algorithm later in this course, but in short, its stochastic version (SGD) optimizes the loss function by picking a random data point at each iteration. For this reason, we say that the algorithm is stochastic because the results depend on these random choices.

We can now visualize our linear regression by generating a hundred data points between min(x) and max(x) with the Numpy linspace() function. We could use the Numpy polyval() function to compute predictions, but Scikit-learn objects also have a predict() function that uses the values in the coef_ and the intercept_ attributes.

# Compute prediction line
x_values = np.linspace(min(x), max(x), num=100)  # Shape (100,)
y_values_huber = lr_huber.predict(x_values[:, np.newaxis])  # Shape (100,1)
Again, we need to add a dimension to the x_values vector since Scikit-learn works with 2-dimensional arrays of features. Let’s plot the prediction line

%matplotlib inline
import matplotlib.pyplot as plt

# Plot predictions
plt.scatter(x, y)
plt.plot(x_values, y_values_huber, c="C3", label="huber")
plt.xlabel("tv budget - scaled")
plt.ylabel("sales in thousands")
plt.legend()
plt.show()

As we can see, the line fits well the trend in the data points. Now, let’s compare this line to the one that we get when minimizing the squares of the residuals, e.g., as with the polyfit() function. To achieve this, we can simply change the loss parameter to squared_error

# Create a linear regression with RSS loss
lr_squared = SGDRegressor(loss="squared_error", penalty="none", max_iter=1000, tol=1e-3)
Again, we can visualize the line by computing predictions for the hundred points in the x_values array from above

# Fit the model
lr_squared.fit(x[:, np.newaxis], y)

# Compute prediction line
y_values_squared = lr_squared.predict(x_values[:, np.newaxis])
Here is a final comparison

# Linear regression without outliers
coefs = np.polyfit(x[:-5], y[:-5], deg=1)  # Filter outliers (the last five values)
y_values_optimal = np.polyval(coefs, x_values)

# Compare models
plt.scatter(x, y)
plt.plot(x_values, y_values_huber, c="C3", label="huber regression")
plt.plot(x_values, y_values_squared, c="C2", label="squared_error")
plt.plot(x_values, y_values_optimal, c="C0", label="without outliers")
plt.xlabel("tv budget - scaled")
plt.ylabel("sales in thousands")
plt.legend()
plt.show()

In this code, we compare the two models from above to a linear regression without the outliers i.e. the last five values of the x/y arrays.

We can see that the model obtained by minimizing the Huber loss is very close to the one that we get with the polyfit() function after removing the five outliers.

HuberRegressor
Note that the SGDRegressor object is very generic and can fit more cost functions than the Huber and RSS ones. However, Scikit-learn also provides the HuberRegressor object which is dedicated to this loss function.

It works similarly to SGDRegressor. You can fit it to data using the fit() function and compute predictions with the predict() one

from sklearn.linear_model import HuberRegressor

# Create a linear regression with Huber loss
lr_huber = HuberRegressor(epsilon=1.35)

# Fit the model
lr_huber.fit(x[:, np.newaxis], y)
HuberRegressor()
Note that there are a few notable differences with the SGDRegressor(loss='huber') object

The threshold 
δ
 depends on the scale of the residuals. The HuberRegressor implements a mechanism to make it scale invariant. The default value of epsilon=1.35 should work well in most cases.
They use a different optimization algorithm. The SGDRegressor object uses stochastic gradient descent (SGD) while the HuberRegressor object uses the BFGS one. In short, BFGS should be more efficient when there is a small number of samples and converge in fewer iterations to the optimal solution. The default value for the number of iterations max_iter=100 should be fine in most cases.
Summary
Let’s summarize what we’ve learned in this unit. Here are a few takeaways

Ideally, we want to use cost functions that are robust to outliers and easy to optimize
The RSS one is smooth but sensitive to outliers
The MAE one is robust but difficult to optimize
The Huber loss combines both advantages of the RSS and MAE cost functions but adds a new parameter: the threshold 
δ
In practice, if you believe that there are outliers in the data, then it’s a good idea to spend some time removing them using the techniques from the previous lessons. Then, it’s easy to test several models and pick the best one. We will learn more about model evaluation later in this subject.

In the next exercise, you will work on a dataset with outliers, and you will fit a linear regression with the Huber loss.

Challenge - Fit polynomial
As an optional challenge, let’s use our Huber regression object to fit a polynomial of degree three.

# Fit a polynomial of degree 3
X_poly = np.c_[x, x ** 2, x ** 3]  # Feature engineering
lr_huber.fit(X_poly, y)

# Same with polyfit
coefs = np.polyfit(x, y, deg=3)

# Compute prediction line
x_values = np.linspace(min(x), max(x), num=100)
y_values_polyfit = np.polyval(coefs, x_values)
y_values_huber = lr_huber.predict(np.c_[x_values, x_values ** 2, x_values ** 3])

# Plot it
plt.scatter(x, y)
plt.plot(x_values, y_values_polyfit, c="C2", label="polyfit(deg=3)")
plt.plot(x_values, y_values_huber, c="C3", label="huber")
plt.xlabel("tv budget - scaled")
plt.ylabel("sales in thousands")
plt.legend()
plt.show()

You should see a “robust” poly. curve of degree 3.
Challenge: compare the result with polyfit(deg=3)!

05. Exercise - Body brain weights outliers
Content
Resources 1
Questions 1
Task description
In this exercise, you are going to work with the body and brain weights dataset which contains body and brain weights measurements of 65 species.

62 of land mammals such as pigs, cows, cats, horses, monkeys and humans
3 dinosaurs (the outliers): Triceratops, Diplodocus and Brachiosaurus
Here is a plot of the 65 data points.


The goal is to fit and compare three different models.

A linear regression on the entire dataset
A linear regression without the three outliers
A linear regression with Huber loss
At the end of the exercise, you will compute estimations of the Encephalization quotient (EQ) of these 65 species which is a theoretical approximate of their intelligence level.

Log-log plot
The plot from above uses a logarithmic scale for both axes. In this exercise, we will fit our models to this transformed version of the dataset. Hence, instead of working with an 
x
 and a
y
 variable, we use the 
log
(
x
)
 and 
log
(
y
)
 ones. For this reason, the equation of a simple linear regression becomes

log
(
y
)
=
a
log
(
x
)
+
b
If you apply the exponential function to both sides of the equation, you can see that it models a non-linear relationship between the 
x
 and 
y
 variables.

e
log
(
y
)
=
e
a
log
(
x
)
+
b
which simplifies to

y
=
e
a
log
(
x
)
e
b
=
(
e
log
(
x
)
)
a
e
b
=
x
a
e
b
Hence, the linear regression model fits the 
y
=
c
x
a
 equation where 
c
=
e
b
. We will use this equation in the last part of the exercise to compute the encephalization quotient. You can take a look at page 126 of this publication if you want to learn more about the measure.

07. Introduction to model evaluation
Content
Resources 1
Questions 1
In the previous units, we learned more about the process of fitting a model. We saw that functions such as the polyfit() one or objects like SGDRegressor or HuberRegression use an optimization algorithm to find the model parameters that minimize a cost function.

We will now learn about baseline strategies and see how to implement them using Numpy and Scikit-learn. We will also see how to use error metrics to evaluate and compare the baseline and models. At the end of this unit, you should know how to compare your models to a meaningful baseline.

This time, we will work on a modified version of the bike sharing dataset from the UCI repository. We split the data set of 302 samples into a training set and a test set containing 151 samples each.

The training and test sets are two important terms that you will see them frequently until the end of the program:

The training set is to fit the models.
The test set is to generate predictions from the trained model.
Let’s start by loading the training data.

import pandas as pd

# Load the data into a Pandas DataFrame
train_df = pd.read_csv("c3_train_df.csv")
train_df.head()

The objective is to train models (on the 151 training samples) that use the temperature, temp, to predict the number of users, users. Then we should use error metrics to select the model that gives the best prediction on the 151 test samples.

But before any modeling exercise we should stop and ask ourselves, do we ever need to build a model to predict something? Ideally we could use our common sense, or experience or any reliable source of knowledge and give a meaningful prediction without building any model. For instance, one could argue that the average (or median) number of users in the training data can approximate the number of users in the test data. This sounds very naive but it is worth to try it and see the error.

The average and median are two baseline strategies. They don’t need any modeling, and they don’t depend on the patterns that appear in the data other than users.

We often use baseline strategies on our day-to-day life to give our best guess or prediction for different things. For instance, our simple guess for the weather temperature of tomorrow could be simply today’s temperature, because temperature doesn’t usually change drastically from one day to the next. Or we can safely predict that a random person living in Basel speaks Swiss German, because most people there speak this language. Finally, we will not be wrong if we say that the weight of a random person is close the average or median weight of the population. Note that in none of these cases we build a model. These are all baseline predictions.

Now, let’s see how to implement these baselines with Numpy and Scikit-learn.

Compute the baseline
In our example, we compute the mean baseline. We should compute the average number of users on the training set and compare it with the number of users on the test set.

import numpy as np

# Extract the target
y_train = train_df.users.values

# Compute baseline
pred_baseline = np.mean(y_train)  
pred_baseline
695.2582781456954
The prediction of the baseline is 695 users. But why calculating the mean and not the median? Be patience, you will see when to use mean or median by the end of this unit.

Let’s see how good our baseline is. Let’s load the test data.

# Load test data
test_df = pd.read_csv("c3_three-models.csv")
test_df.head()

The test data are in the first two columns. We will shortly explain what the other three columns are. To check how accurate the value 695 is, we should compare it with the values in the users column.

Note that we can use any cost function to calculate the error. For instance, we can use the mean absolute error (MAE).

# Mean absolute error
def MAE(y, y_pred):
    return np.mean(np.abs(y - y_pred))
We can now calculate the error.

# Extract the target
y_test = test_df.users.values

# compare the baseline with the target from the test data
mae_baseline = MAE(y_test, pred_baseline)
mae_baseline 
316.3029253102934
The advantage of MAE is that it has a clear interpretation. For instance, in our example, predictions from the baseline is wrong by, in average, 316 users. That’s a lot!

The average or median can serve as a baseline, and here are things you should know about a baseline:

Baseline is a strategy that doesn’t rely on any modeling.
Baseline can be calculated by just using the target.
The mean and median are two ways to calculate a baseline, but there are also other strategies available and you will learn about them by the end of the program.
In some domains (e.g. time series analysis), a simple baseline can give difficult-to-beat predictions.
Note that pred_baseline is a single value (i.e. 695) and not an array like y_test. However, we can use the MAE() function from above because it computes the residuals with (y-y_pred) which produces an array even if y_pred is a single value due to broadcasting.

If you want to produce a vector of predictions, you can create an array with the same shape as y_test and fill it with the mean value using the Numpy full_like() function.

# Vector with predictions from the baseline
pred_baseline = np.full_like(y_test, fill_value=np.mean(y_train), dtype=float)
pred_baseline
array([695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815, 695.25827815,
       695.25827815, 695.25827815, 695.25827815])
print(len(pred_baseline), len(y_test))
151 151
Both are arrays with the same length.

How about Scikit-learn? how to use it to work out the baselines? Scikit-learn implements baselines in a DummyRegressor object which has a strategy parameter. We can set it to 'mean' or 'median'.

from sklearn.dummy import DummyRegressor

# Create the DummyRegressor object
dummy = DummyRegressor(strategy="mean")
Just like SGDRegressor and HuberRegressor, the DummyRegressor object implements the Scikit-learn estimator interface which means that it provides the fit() and predict() functions.

Again, we provide the features in a 2-dimensional array.

# Extract the features
x_train = train_df.temp.values

# Fit the estimator
dummy.fit(x_train[:, np.newaxis], y_train)
DummyRegressor()
The fit() function simply calculates the mean value of y_train, and we can then compute predictions with the predict() function.

# Extract the features
x_test = test_df.temp.values

# Vector with predictions from the baseline
pred_baseline = dummy.predict(x_test[:, np.newaxis])

# compare the baseline with the target from the test data
mae_baseline = MAE(y_test, pred_baseline)
mae_baseline  
316.3029253102934
We got the same result as in the case of Numpy calculation. Note that x_train[:, np.newaxis] and x_test[:, np.newaxis] didn’t influence the fit and predict methods.

Now that we know about baselines and how they are calculated, let’s compare predictions from models with the baseline and see if they lead to smaller errors than 316 users given by the baseline.

Compare models
As we said before, we split the original data set of 302 samples into a training set and a test set containing 151 samples each. We fit three models on the training set and use the fitted models to predict users on the test set. We don’t show these steps but only put models’ predictions in the last three columns of the test data:

test_df.head()

More precisely:

The first model is a simple linear regression. We use the polyfit() function with degree equal to 1. Then we use the fitted model to predict the target for the second half of the data, i.e. test data. The predictions are named as pred_lr in the third column.
In the second model, we change the degree to 3, and follow the same steps. The predictions are named as pred_poly3 in the fourth column.
In the third model, we still fit a polynomial of degree 3, but with the HuberRegressor and the PolynomialFeatures objects from Scikit-learn. Yes, this sounds new and you will learn about it later in this course. The predictions are named as pred_huber3 in the last column.
Let’s visualize the three models by plotting their predictions.

%matplotlib inline
import matplotlib.pyplot as plt

# Extract variables
x_test = test_df.temp.values

# Plot the models
plt.scatter(x_test, y_test, s=10)
plt.plot(x_test, test_df.pred_lr, c="C0", label="linear regression")
plt.plot(x_test, test_df.pred_poly3, c="C2", label="polyfit(deg=3)")
plt.plot(x_test, test_df.pred_huber3, c="C3", label="polyfit(deg=3) with Huber loss")

plt.title('predictions from three models vs test data')
plt.xlabel('temperature')
plt.ylabel('number of users')
plt.legend(loc='upper left')
plt.show()

It’s difficult to compare the three models visually. One solution is to use a cost function such as the RSS one. In practice, we prefer to use the root mean squared error (RMSE) since it’s independent of the number of data points (i.e., it’s an average) and has roughly the same unit as the target variable.

Let’s compute the RMSE scores using Numpy code from the previous exercises.

# Root mean squared error
def RMSE(y, y_pred):
    mse = np.mean(np.square(y - y_pred))  # MSE
    return np.sqrt(mse)  # RMSE


rmse_baseline = RMSE(y_test, pred_baseline)
rmse_lr     = RMSE(y_test, test_df.pred_lr)
rmse_poly3  = RMSE(y_test, test_df.pred_poly3)
rmse_huber3 = RMSE(y_test, test_df.pred_huber3)

# Print values
print("baseline:", rmse_baseline)
print("linear regression:", rmse_lr)
print("polyfit(deg=3):", rmse_poly3)
print("polyfit(deg=3) with huber loss:", rmse_huber3)
baseline: 371.6823731957837
linear regression: 232.53050786099598
polyfit(deg=3): 210.55145192395318
polyfit(deg=3) with huber loss: 215.66518077491205
According to this measure, the poly3 and the huber3 models have a better performance than the simple linear regression one, and the poly3 one performs slightly better than huber3.

Let’s change the cost function to the mean absolute error (MAE):

mae_baseline = MAE(y_test, pred_baseline)
mae_lr     = MAE(y_test, test_df.pred_lr)
mae_poly3  = MAE(y_test, test_df.pred_poly3)
mae_huber3 = MAE(y_test, test_df.pred_huber3)

# Print values
print("baseline:", mae_baseline)
print("linear regression:", mae_lr)
print("polyfit(deg=3):", mae_poly3)
print("polyfit(deg=3) with huber loss:", mae_huber3)
baseline: 316.3029253102934
linear regression: 186.58278145695365
polyfit(deg=3): 161.71523178807948
polyfit(deg=3) with huber loss: 164.75496688741723
The predictions from the poly3 model are wrong by, in average, 162 users. This is the lowest error among the three models, and all models perform better than the baseline. Note that MAE and RMSE are different cost functions. Hence, they might not agree on what is the best model.

Let’s create a final comparison of the different models using a bar chart.

# Bar chart
plt.bar([1, 2, 3, 4], [rmse_baseline, rmse_lr, rmse_poly3, rmse_huber3])
plt.xticks([1, 2, 3, 4], ["baseline (mean)", "linreg", "poly3", "huber3"])
plt.title('comparing predictions errors (RMSE)')
plt.xlabel('models')
plt.ylabel('RMSE')
plt.show()

This chart gives us a good overview of our different models.

In practice, data sets usually have several features, and it’s difficult to visually “see” how well our models actually fit the data. For this reason, setting a baseline and comparing many different models can help in two ways:

To verify that our code works, i.e., a model that performs worse than the baseline is a sign that something might be wrong.
To choose an appropriate model, i.e., the model with the best accuracy isn’t necessarily the most appropriate one. We might consider other aspects such as speed and interpretability of the model.
Before finishing this unit, let’s do a small experiment to see when it makes sense to use mean and median as baselines.

A note on the choice of median versus mean
Which statistic should we use? In fact, both are good summaries of a set of values. They simply minimize different measures. The mean value optimizes the squares of the distances to the other values, whereas the median minimizes the absolute distances.

Let’s do a little experiment to verify that. We define a set of values and search for its optimal summary. For instance, let’s find the best summary of the set [1, 2, 3, 5, 6, 25] according to the MSE function. We can take one hundred values between 0 and 25 and treat them as candidate summaries. Here is the plot of the MSE cost function for these candidates.


As we can see, the mean value at 
x
=
7
 is the best summary of this list according to the MSE cost function.

Let’s do the same experiment with the MAE one.


This time, the optimal summary is not unique. All median values (
x
 between 3 and 5) are optimal summaries according to the MAE cost function. The MAE does not change in this range because moving to the summary candidate closer to 3 reduces the absolute errors for 1, 2, 3 while increasing the absolute errors for 5, 6, 25 by the same amount.

To summarize, it makes sense to use the mean as a baseline when we measure the squares of the residuals, e.g., RSS, MSE and RMSE cost functions. On the other hand, we should use the median as the baseline when we measure absolute distances like with the MAE cost function.

You can take a look at this article by John Myles White if you want to learn more about the relation between the mode, median, mean statistics and machine learning cost functions.

Summary
Let’s summarize what we’ve learned in this unit:

In a typical machine learning workflow, we fit models to data points from a training set and evaluate them on a test set
The baseline gives us a good first reference for our model performance.
The mean and median values are both good baselines. The mean works well for squares of the residuals while the median works well for their absolute values.
Scikit-learn provides a DummyRegressor object to create baseline models.
In the next exercise, you have the opportunity to test all the models that you’ve learned so far on the entire bike sharing data set (731 data points) and compare them with a baseline model.08. The train-validate-test paradigm
Content
Questions 2
In the previous units we have seen how to build linear models and how to fit polynomials. We have further seen that outliers can affect the quality of our model and how we can address this by removing data points before training our model or by using a cost function that can handle outliers better (the Huber loss).

Generalization
Our goal is to find the best model possible. But how could we measure whether one model is better or worse than another. Since the plain linear regression algorithm optimizes for the RSS (Residual Sum of Squares), we could calculate this score for each model and then compare them.

We observed that using high degree polynomials leads to smaller errors, which leads to an improved RSS score. However, a simple visual inspection showed us that for high degree polynomials any noise in the data could strongly affect the outcome. We termed this “overfitting”. As a result, despite their improved score these models did not seem to represent the general pattern of the data very well.

This last point highlights a very important objective of machine learning:

We want our models to generalize beyond their training data. Hence we want to empirically test our model on unseen data, i.e. data that was not used for training already.

In this context, we often speak of in-sample evaluations when using training data vs out-of-sample evaluations when using unseen test data. The performance of the model on the test data set is known as the generalization error and gives us an idea of how well our model performs in general. Hence good out-of-sample scores are more valuable than improved in-sample scores.

The test set
In practice, we either collect a separate data set for the out-of-sample evaluation, or we split our original data set into two parts: a training set for training our models and a test set for evaluating the final models. The test set is also known as the hold-out set.


Since the purpose of the test set is to provide an independent evaluation of the performance of our model, we must make sure that we don’t look into the test set or analyze it in any way. Otherwise we risk gaining some form of insight from the test data, which could influence how we design and train our model. Such insight is known as information leakage. In order to avoid this it is good practice not to access test data until all our models are trained.

Correctly preparing the test set
In order to prepare the test data correctly and not compromise its independence for the evaluation of our model, here are things to do and to avoid at all costs.

DOs
Treat training data and test data in the same way. For example, when we want to fix missing values we must feed the exact same value to the fillna() function on both the training set and the test set. So if we are using the median strategy, we must calculate the median on the training data only, and then use the particular value inside fillna() on both training and test sets. This approach ensures that our missing data is imputed in a consistent way, and we don’t suddenly change values between data sets.
Ensure that training data and test data use the same features in the same order. Machine learning algorithms work with numerical arrays. The only care about the numerical data in the columns and not the column names or any meaning of the data to us. Thus we must ensure that the individual columns in training and test data represent exactly the same information. A common problem arises with one-hot-encoding as pandas’ get_dummies() creates the binary columns in the order in which the values appear in each data set. So this order may be different between training and test set or we may even have a different set options. We will discuss and address this issue in more detail in a later unit.
DON’Ts
Don’t calculate any statistical information on the test set. For example, suppose we want to fix missing values using the median of a feature. In this case we must calculate this median on the training set only and feed it to the fillna() function applied on the test set. The approach also ensures that our missing data is imputed in a consistent way, and we are not suddenly changing the value we use between
Don’t fit transformers on the test set. Transformers are objects in scikit-learn that learn particular values from our data set and use them to transform the data in a predefined way. A common example is the Standardscaler which calculates the z-scores of our feature values — more details later. When we fit the scaler on the training set, it learns the mean and the std of the training data. They are then used to transform the data into the z-scores. But we don’t fit the scaler again on the test set, as this would calculate the mean and the std for the test data – and we just saw that this is a no-go! Instead apply the transform() method of our fitted scaler to the test set, thus using the mean and the std of the training data and ensuring that the scaling has been applied consistently.
In order to ensure consistency we may also consider creating a preprocessing function that gets applied to both the training and the test data frames. But we must be careful. Either we calculate all statistical values upfront and then hardcode them into our function; or our function gets two data frames — one whose features will be processed by it and one from which it gets statistical information on how to process the data.

We will see many examples of the above points later on in this course. In short:

Any data transformations performed on the training or test data can only use information obtained from the training data. We must ensure that no information from the test data can influence these transformations.

A simple way of thinking of the entire process is as follows. In order to ensure the correct procedures and have an independent evaluation of our model. At the start of the project we split off some data as our test set and give it to our colleague Alice for safe keeping. Alternatively Alice could collect her own test set. Once we have finished training our model, we pass the model to Alice, together with detailed instructions on how she should prepare the test data. Our instructions should take care of all the preprocessing steps and the feature engineering that we have applied to our training data. Alice will have to prepare the test samples one at a time. That way Alice won’t have access to any information based on the entire test set, like its mean or median. So our instructions can only use statistical values calculated on the training data that we must pass along with our instructions. After preparing the test data, Alice feeds each sample to the model for prediction and then compares them with the true target value in order to evaluate the performance of the model.

The validation set
When building our models we have to make a number of decisions that influence the performance of our model. We want to be able to identify which option provides the best overall model performance. In an earlier unit we built several polynomial regression models with different polynomial degrees. Polynomials with higher degrees lead to more complex models that adapted better to the training data and gave better training scores. But as we have discussed these models did not generalize very well. A simple step could be to evaluate and compare all models on the test data and choose the polynomial degree of the model with the best out-of-sample performance. However, this approach would use information from the test data to determine the optimal degree for our model. This is a no-go as we would lose our ability to measure the generalization error reliably.

Instead, at the start of our model building we put aside part of our training set to help us decide on the best degree. This set is excluded from training and is known as the validation set. It acts as an internal hold-out set to evaluate how well different models are doing before selecting the best one.


The model performance on the validation set also provides an estimate of the generalization error. And since we have not used the test set up to this point, the model performance on the test set can still provide an independent estimation of how well our model will generalize.

Tuning hyperparameters
Unlike the coefficients (weights) the value of the polynomial degree is not part of the training process. It is not learned by the model using data. Instead we fixed the polynomial degree before fitting our model. This is an example of a hyperparameter.

Hyperparameters are model parameters that are set manually as they cannot be calculated from data. They are fixed ahead of training and influence how the model is constructed or how it is trained.

The optimal values of the hyperparameters depend on the problem and the data set. We can find suitable values by training multiple models with different hyperparameter combinations and then evaluating them on the validation set. This process is known as hyperparameter tuning.

More generally, the purpose of the validation set is to help us make decisions, without breaking the sanctity of the test set. Later on, we will see how to use validation sets to determine the best hyperparameters or select the best features. We will introduce different approaches to setup validation sets and create a robust decision making process for tuning our models.

Summary
In this unit we have discussed why we split our data into a training set, a validation set and a test set.

We create these three sets before we start the modeling process.
The test set is ‘locked away’ and only used for the final evaluation.
Any insight for transforming the data comes from the training data, never the test data.
We train our various models on the training set.
We select model hyperparameters by comparing different models on the validation set.
The score on the validation set provides an estimate of the expected generalization error.
The score on the test set gives us a more reliable performance metric.
02. Multiple linear regressions
Content
Resources 1
In the previous subject, we learned about simple linear regressions which use the equation of a line to model the relationship between a single feature and the target variable. We will now see how to generalize this idea to multiple features.

At the end of this unit, you should be able to fit linear regressions to datasets with multiple features using the lstsq() function from the Scipy library.

Note that in this subject we will be working only with training data. Therefore, all the subsequent predictions and model evaluations will be in-sample, i.e. on the training data. The only exception is the bike sharing exercise and solution at the end of this subject where we use both train and test data.

Multiple linear regressions
First, let’s start by reviewing some of the mathematical notations from the previous units. We will use the marketing campaign dataset for illustration. You can find the dataset file in a zipped folder under the resources tab.

import pandas as pd

# Load data
data_df = pd.read_csv("c3_marketing-campaign.csv")
print("data_df shape:", data_df.shape)
data_df shape: (50, 4)
data_df.head()

This code should return the first five rows of the data_df DataFrame.

Each row corresponds to a data point. Using the notation, we denote the i-th one with the vector 
⃗
x
i
.

⃗
x
i
=
⎛
⎜
⎜
⎜
⎜
⎝
x
i
1
x
i
2
⋮
x
i
p
⎞
⎟
⎟
⎟
⎟
⎠
This vector is a column vector that contains the values of the 
p
 features. In our example, the shape of the data_df DataFrame is (50,4). Hence, we have 
n
=
50
 data points with
p
=
3
 features each.

To write the feature matrix 
X
, we convert these data points to row vectors using transposes
⃗
x
⊺
i
.

X
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
⃗
x
⊺
1
⃗
x
⊺
2
⋮
⃗
x
⊺
n
⎞
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
x
11
x
12
…
x
1
p
x
21
x
22
…
x
2
p
⋮
⋮
⋮
⋮
x
n
1
x
n
2
…
x
n
p
⎞
⎟
⎟
⎟
⎟
⎟
⎠
In our example, the 
X
 matrix corresponds to the data_df DataFrame without the sales column. Hence, its shape is (50,3).

So far, we fitted models using a single feature, e.g., the tv budget. Hence, it wasn’t necessary to write the arrow above each data point 
⃗
x
i
 in the equation of the simple linear regression.

^
y
i
=
a
x
i
+
b
However, we now have 
p
 values 
x
i
1
,
x
i
2
,
…
,
x
i
p
 for each data point 
⃗
x
i
 and our goal is to model a linear relationship between these 
p
 variables and the target one 
y
. To achieve this, instead of using the equation of a line, we can generalize to multiple dimensions by using the equation of a hyperplane.

^
y
i
=
w
0
+
w
1
x
i
1
+
w
2
x
i
2
+
…
+
w
p
x
i
p
In this equation, we multiply each feature with a coefficient and add a 
w
0
 parameter that corresponds to the intercept term. We can also write the equation using the inner product between the data point 
⃗
x
i
 and a vector with the coefficients 
⃗
w
.

⃗
w
=
⎛
⎜
⎜
⎜
⎜
⎝
w
1
w
2
⋮
w
p
⎞
⎟
⎟
⎟
⎟
⎠
With the inner product 
⃗
x
⊺
i
⃗
w
, the equation becomes

^
y
i
=
w
0
+
⃗
x
⊺
i
⃗
w
Alternatively, we can write the equation for all data points.

⃗
y
p
r
e
d
=
X
⃗
w
+
w
0
Note that some sources use the greek letter beta 
β
 to denote the coefficients. Using this notation, 
⃗
β
 is the vector of coefficients and 
β
0
 is the intercept term.

Now that we know the equation behind linear regressions with multiple features, let’s see how to fit one to the marketing campaign dataset using the lstsq() function from Scipy.

Implementation with Scipy
The first step is to create the input matrix 
X
 and the output vector 
y
.

# Extract input matrix X
X = data_df.drop("sales", axis=1).values
print("X:", X.shape)
X: (50, 3)
# Extract target vector y
y = data_df.sales.values
print("y:", y.shape)
y: (50,)
We now want to find the vector of coefficients 
⃗
w
 that minimizes an objective function. In this unit, we will use the lstsq() function from Scipy which computes the least squares solution of the equation 
A
x
=
b
. In our case, 
A
, 
b
 and 
x
 correspond respectively to 
X
, 
y
and 
⃗
w
, and the least squares solution is the vector 
⃗
w
 that minimizes the squared distances between the two sides of the equation

X
⃗
w
=
y
In other words, the lstsq() function will return the parameter values that minimize the squares of the difference between the predictions of our linear regression model (without the intercept term) 
⃗
y
p
r
e
d
=
X
⃗
w
 and the target values 
y
.

Let’s try it with our X and y arrays.

from scipy.linalg import lstsq

# Fit a multiple linear regression
w, rss, _, _ = lstsq(X, y)
print("w:", w)
print("RSS:", rss)
w: [0.3958359  0.47521518 0.31040001]
RSS: 1.6884039033000031
The function returns four values. As for now, we will only look at the first two. The first one w is the vector of coefficients (one for each feature) and the second one rss is the residual sum of squares.

For reference, the RSS score of our simple linear regression model was around 15.7. Hence, we gained a lot in accuracy by bringing the two other marketing budgets in the equation.

Adding the intercept term
The code from above computes the optimal solution without the intercept term 
w
0
. However, it’s possible to make the lstsq() function compute it using a little trick. The idea is to add a column of ones in the matrix 
X
. This column corresponds to the 
w
0
 element in
⃗
w
.

X
⃗
w
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
x
11
x
12
…
x
1
p
1
x
21
x
22
…
x
2
p
1
⋮
⋮
⋮
⋮
1
x
n
1
x
n
2
…
x
n
p
⎞
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
w
0
w
1
w
2
⋮
w
p
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
The ones multiply the intercept term 
w
0
, and we get the equation of linear regressions from above. To generate this additional column, we can use the ones() function from Numpy. Then, we concatenate it to the X matrix with the Numpy c_ object.

import numpy as np

# Add a column of ones
X1 = np.c_[np.ones(X.shape[0]),  # Vector of ones of shape (n,)
           X]                    # X matrix of shape (n,p)

X1[:5, :]
array([[ 1.   ,  0.916,  1.689,  0.208],
       [ 1.   ,  9.359,  1.706,  1.071],
       [ 1.   ,  5.261,  2.538,  2.438],
       [ 1.   ,  8.682,  2.092,  1.283],
       [ 1.   , 11.736,  1.66 ,  1.8  ]])
We can now pass the X1 matrix to the lstsq() function.

w, rss, _, _ = lstsq(X1, y)

print("w:", w)
print("RSS:", rss)
w: [0.02487092 0.39465146 0.47037002 0.30669954]
RSS: 1.6854508680824716
The array w has now four elements. The first one w[0] corresponds to the intercept term and the other three w[1:] to the coefficients. Note that the intercept is close to zero. Hence the RSS score didn’t change significantly.

We can now use this vector w to compute predictions.

# Compute predictions
y_pred = np.matmul(X1, w)
print("y_pred:", y_pred.shape)
y_pred: (50,)
This code computes the predictions for the data points in X, and we can verify that we get the same RSS score as lstsq().

# Verify RSS score
def RSS(y, y_pred):
    return np.sum(np.square(np.subtract(y, y_pred)))


rss = RSS(y, y_pred)
print("RSS:", rss)
RSS: 1.6854508680824707
Summary
In this unit, we saw how to generalize linear regressions to multiple features, and how to implement them using the lstsq() function from Scipy. In practice, we usually work with higher level tools such as the LinearRegression object from Scikit-learn. However, it’s good to know how they work internally.

In the next unit, we will learn about the coefficient of determination 
R
2
 which is a common evaluation metric for linear regressions.
 03. R^2 coefficient
Content
Resources 1
Questions 2
In this unit, we will learn about the coefficient of determination 
R
2
 which is a common evaluation metric for linear regressions. It’s also the default metric returned by many Scikit-learn objects such as SGDRegressor or HuberRegressor.

R
2
 coefficient
Unlike cost functions, the coefficient 
R
2
 doesn’t directly express the error made by our model. In fact, we can interpret it as a comparison between our model and the constant mean baseline.

Let’s take an example. Say that we are working on the marketing campaign dataset.

import pandas as pd

# Load data
data_df = pd.read_csv("c3_marketing-campaign.csv")
X = data_df.drop("sales", axis=1).values
y = data_df.sales.values
Our goal is to fit and evaluate a linear regression model. To achieve this, we define the RSS() function and start by evaluating the constant mean baseline.

import numpy as np

# Define RSS measure
def RSS(y, y_pred):
    return np.sum(np.square(np.subtract(y, y_pred)))


# RSS of the baseline
rss_baseline = RSS(y, y.mean())
print("RSS baseline:", rss_baseline)
RSS baseline: 100.86060792
Now that we have a baseline, we can fit other models and compare them against this baseline score. For instance, let’s fit a linear regression using the code from the previous unit.

from scipy.linalg import lstsq

# Fit a multiple linear regression
X1 = np.c_[np.ones(X.shape[0]), X]
w, model_rss, _, _ = lstsq(X1, y)
print("RSS:", model_rss)  # ~1.685
RSS: 1.6854508680824716
As we can see, there is a significant difference in the RSS scores between our baseline and the linear regression. To quantify this difference, we can compute the 
R
2
 coefficient which is one minus the ratio between the two scores.

R
2
=
1
−
R
S
S
m
o
d
e
l
R
S
S
b
a
s
e
l
i
n
e
It’s interesting to note that the ratio is always greater than or equal to zero since RSS scores cannot be negative. Hence, the 
R
2
 coefficient is smaller than or equal to one. We can distinguish four cases.

If 
R
2
=
1
, then the ratio is zero which means that the RSS of the model is zero and that it makes no error
If 
R
2
 is close to 
1
, then the model performs way better than the baseline
If 
R
2
 is close to 
0
, then the model and the baseline have a similar performance
If 
R
2
<
0
, then it performs worse than the baseline
Let’s compute the 
R
2
 coefficient of our linear regression model.

# R^2 coefficient
R2 = 1 - (model_rss / rss_baseline)
print("R^2 coefficient:", R2)  # Prints: ~0.983
R^2 coefficient: 0.9832893048848236
We get a score above 98%. Let’s compare it to the 
R
2
 coefficient of the simple linear regression model from the previous subject. Using the television budgets, we obtained an RSS score around 15.74.

# R^2 of simple linear regression model
R2 = 1 - (15.74 / rss_baseline)
print("R^2 coefficient:", R2)  # Prints: ~0.844
R^2 coefficient: 0.8439430385697798
This time, the 
R
2
 coefficient is around 84%.

Proportion of variance explained
Viewing the 
R
2
 coefficient as a comparison between our model and the constant mean baseline can help to understand the idea behind it. However, here is the more formal definition from its Wikipedia article.

The coefficient of determination 
R
2
 is the proportion of the variance in the target variable 
y
 that is predictable from the input variable(s) 
x
In other words, it’s a measure of how well we can explain, with our model, how the target variable 
y
 varies using the input variable(s) 
x
. The idea is that the target values vary around their mean value and we can quantify this variation by summing the squares of the differences between each target value 
y
i
 and the mean target value 
¯
y
.

SS
tot
=
n
∑
i
=
1
 
(
y
i
−
¯
y
)
2
Note that this corresponds to the RSS score of the mean baseline. Now, after fitting our model, the 
y
i
 values vary around their predictions 
^
y
i
, and we can also quantify this variation by summing the squares of the residuals.

SS
res
=
n
∑
i
=
1
 
(
y
i
−
^
y
i
)
2
Note that this corresponds to the RSS score of our model.

The ratio between these two values correspond to the proportion of variance left in the data or not explained by the model. If we subtract this ratio to one, we obtain the proportion of variance explained by our model which is the 
R
2
 coefficient.

R
2
=
1
−
SS
res
SS
tot
Summary
Here are a few takeaways about the 
R
2
 coefficient.

Intuitively, it measures how well a model performs compared to the constant mean baseline
It is always smaller than or equal to one. Ideally, it should be close to one
The coefficient is defined as the proportion of variance explained by the model
So far, we focused on how the linear regression model works, but not on the methods that find the set of optimal parameters 
⃗
w
. In the next unit, we will learn about the ordinary least squares (OLS) method which is the one used by the polyfit() and the lstsq() functions.
04. Ordinary least squares
Content
Resources 1
The goal of this unit is to get familiar with the topic of cost function optimization. We will start with a review of derivatives and gradients, and see how we can use them to optimize cost functions.

In the second part of this unit, we will learn about the ordinary least squares (OLS) method which is the one used by the lstsq() function to compute the least squares solution. We will implement this method using basic Numpy code and get the set of optimal parameters of a linear regression model for the marketing campaign dataset.

Cost functions and derivatives
Let’s start with a review of derivatives and see how we can use them to find the minimum of a function with a single variable. We will use the function 
f
(
x
)
=
(
x
−
2
)
2
. Let’s plot it and see how its derivative (or slope) vary around the minimum value at 
x
=
2
.


The three plots show the function 
f
(
x
)
 with a blue curve and its derivative at three different points 
x
=
1
,
2
,
3
 with a tangent line in red. Using calculus, we can compute the derivative 
f
′
(
x
)
 at any point of the function.

f
′
(
x
)
=
2
x
−
4
It’s interesting to note that the derivative is negative below the optimal value and positive above it.

f
′
(
x
)
<
0
 when 
x
<
2
 which corresponds to the left plot. For instance 
f
′
(
1
)
=
−
2
f
′
(
x
)
>
0
 when 
x
>
2
 which corresponds to the right plot. For instance 
f
′
(
3
)
=
+
2
Intuitively, since the derivative switches from a negative value to a positive value between 1 and 3, we know that there is a point in between with a slope of exactly zero, i.e., where the function 
f
 is flat. This point corresponds to the minimum value of 
f
(
x
)
.

This is the idea of the OLS method. It finds the parameters of a linear regression model by analyzing where the slope of the 
RSS
 function is zero.

Gradient of the cost function
The plot from above shows a function 
f
(
x
)
 with a single parameter 
x
. However, the
RSS
(
⃗
w
)
 function depends on a set of parameters 
⃗
w
. For instance, a simple linear regression model has two parameters, and we can plot its error surface using a 3-dimensional plot. Here is the plot that we saw in a previous unit.


In this plot, the x- and y-axis correspond to the 
a
 and 
b
 parameters of the model and the z-axis to the cost function. Our goal is to find the point 
(
a
,
b
)
 that has a derivative of zero in the direction of all parameters. In particular, it’s not sufficient to have a derivate of zero in the direction of just one parameter.

For instance, imagine that we take a slice of this surface along one of its axes. For instance, we can get a curve very similar to the one from above by slicing at 
a
=
−
5
. Since we fix the
a
 parameter, this slice corresponds to a function 
f
 with a single parameter and we can find its minimal value 
b
∗
 by solving 
f
′
(
b
)
=
0
. By construction, this point 
(
−
5
,
b
∗
)
 has a derivative of zero in the direction of the 
b
 parameter but is not the minimum of the cost function from above.

To summarize, in practice, we work with models that have many parameters, and the set of optimal parameters have a derivative of zero in the direction of all parameters. This corresponds to solving the equation on the gradient of the cost function 
∇
f
 (using the nabla symbol 
∇
) which is a vector with the derivatives of 
f
 in the direction of all parameters.

∇
f
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
∂
f
∂
w
0
∂
f
∂
w
1
⋮
∂
f
∂
w
p
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
We call the i-th element 
∂
f
∂
w
i
 of this vector the partial derivative of the function in the direction of 
w
i
. Our goal is to find the vector of parameters 
⃗
w
 with 
∇
f
=
0
. In the example from above, the gradient has a 
∂
f
∂
a
 and a 
∂
f
∂
b
 entry and our goal is to find a set of 
a
, 
b
 values that makes these partial derivatives equal to zero.

You can take a look at the youtube video Gradients and Partial Derivatives by Eugene Khutoryansky which shows a 3d example of partial derivatives and gradients.

Ordinary least squares
Solving the equation on the gradient 
∇
f
=
0
 only works for a specific class of functions 
f
that are “flat” only at their optimal value. The good news is that the RSS cost function of a linear regression model is one of them and we can find the optimal parameters analytically by solving the equation. You can take a look at this post if you want to learn more about the proof.

This analytical solution is called the ordinary least squares (OLS) solution. Here is the formula that we get when solving the equation on the gradient.

⃗
w
=
(
X
⊺
X
)
−
1
X
⊺
⃗
y
Here, the 
X
 and 
⃗
w
 variables correspond to the input matrix and the vector of parameters with the additional column of ones and the intercept term 
w
0
. You can take a look at this article by Dustin Stansbury that shows how to derive the formula. Note that he uses the 
⃗
β
notation to denote the vector of parameters instead of 
⃗
w
.

OLS on the marketing campaign dataset
We will now implement the OLS method using the formula from above. Our goal is to find the set of optimal parameters of a linear regression model for the marketing campaign dataset.

Let’s start by loading the dataset into an X and a y Numpy array.

import pandas as pd

# Load data
data_df = pd.read_csv("c3_marketing-campaign.csv")
X = data_df.drop("sales", axis=1).values
y = data_df.sales.values
To implement the OLS formula, we first need to create the 
X
 matrix with the additional column of ones.

import numpy as np

# Create X1 matrix
X1 = np.c_[np.ones(X.shape[0]),  # Column of ones, shape: (n,)
           X]                    # Input matrix, shape: (n,p)
We can then use the matmul() function from Numpy to compute matrix multiplications and get the transpose 
X
⊺
 with the T attribute of Numpy arrays. For the matrix inversion
(
X
⊺
X
)
−
1
, we can use the Numpy inv() function from its linalg module.

# Compute OLS solution
XX = np.matmul(X1.T, X1)
Xy = np.matmul(X1.T, y)
w = np.matmul(np.linalg.inv(XX), Xy)

print("w:", w)
w: [0.02487092 0.39465146 0.47037002 0.30669954]
We get a vector of parameters, each one corresponding to a column of the 
X
 matrix. The first one is the intercept term 
w
0
, and the three others are the coefficients for each marketing budget.

Let’s do a quick verification using the lstsq() function from Numpy.

from scipy.linalg import lstsq

# Verify with Scipy lstsq
w, _, _, _ = lstsq(X1, y)

print("w:", w)
w: [0.02487092 0.39465146 0.47037002 0.30669954]
We can see that we get the same parameter values.

Summary
In this unit, we learned about the ordinary least squares (OLS) solution which is the standard way to find the set of parameters for linear regressions. In particular, we saw that it’s an analytical solution that minimizes the RSS measure by solving an equation on its gradient.

In the next unit, we will see how to implement linear regressions with the Scikit-learn library.
05. In Scikit-learn
Content
Resources 1
Questions 1
In this unit, we will see how to implement linear regressions with the Scikit-learn library.

LinearRegression
First, let’s start by creating the X and y Numpy arrays.

import pandas as pd

# Load data
data_df = pd.read_csv("c3_marketing-campaign.csv")
X = data_df.drop("sales", axis=1).values
y = data_df.sales.values
The Scikit-learn library provides a LinearRegression object that uses the OLS method to fit a linear regression model. Just like the HuberRegressor and the SGDRegressor ones, this object implements the fit() function. In the Scikit-learn jargon, these objects are estimators and the function is part of the estimator API.

from sklearn.linear_model import LinearRegression

# Create a linear regression object
lr = LinearRegression()

# Fit the model
lr.fit(X, y)

# Print coefficients
print("Coefficients:", lr.coef_)
print("Intercept:", lr.intercept_)
Coefficients: [0.39465146 0.47037002 0.30669954]
Intercept: 0.02487091788819429
In this code, we start by creating a LinearRegression() object. Then, we call fit() with the X, y variables. This function also relies on the lstsq() function from Scipy to compute the set of optimal parameters. However, note that we don’t have to add the additional column of ones to the input matrix X. The LinearRegression() object will automatically compute the intercept term 
w
0
. We can access the parameters with the coef_ and the intercept_ attributes.

For reference, here are the optimal coefficients that we computed in the last unit with the OLS formula.

w: [ 0.02487092  0.39465146  0.47037002  0.30669954]
Predict and score functions
Scikit-learn estimators also implement a predict() and a score() function.

The predict(X) function uses the coef_ and the intercept_ attributes to compute predictions for the data points in the X matrix.

# Compute predictions
y_pred = lr.predict(X)
y_pred[:3]
array([1.24462012, 4.84934038, 4.04266482])
We can also manually compute these predictions with the matmul() function from Numpy.

import numpy as np

y_pred = np.matmul(X, lr.coef_) + lr.intercept_
y_pred[:3]
array([1.24462012, 4.84934038, 4.04266482])
The score(X, y) function computes the predictions for the data points in X and evaluates the 
R
2
 coefficient using the target values in y.

# Compute the R2 coefficient
R2 = lr.score(X, y)
print("R2:", R2)
R2: 0.9832893048848236
As we can see, the coefficient is around 0.98 which is similar to what we found in the unit about 
R
2
.

For reference, in the last unit we found a value of:

R2: 0.983289304885
SGDRegressor
The OLS method is the most common way to find the parameters of a linear regression model that minimize the squares of the residuals. However, in the unit about Huber loss, we saw the SGDRegressor object which implements a variant of the gradient descent (GD) algorithm.

It’s important to understand that this algorithm doesn’t compute an analytical solution. It’s an iterative algorithm that tries to get closer to the optimal solution after each iteration. However, unlike the OLS method, gradient descent is very generic and can optimize many different cost functions, e.g., Huber loss.

To minimize the squares of the residuals, we can set its loss parameter to squared_error. By default, it adds a penalization term to the cost function. We will learn more about it later in this course. As for now, we can set it 'none'.

from sklearn.linear_model import SGDRegressor

# Create the SGDRegressor object
lr_sgd = SGDRegressor(
    loss="squared_error",  # Cost function
    penalty="none",  # Add a penalty term?
    max_iter=1000,  # Number of iterations
    random_state=0,  # The implementation shuffles the data
    tol=1e-3,  # Tolerance for improvement (stop SGD once loss is below)
)

# Fit the linear regression model
lr_sgd.fit(X, y)

# Print coefficients
print("Coefficients:", lr_sgd.coef_)
print("Intercept:", lr_sgd.intercept_)
Coefficients: [0.39968853 0.44409771 0.25894341]
Intercept: [0.12807209]
The implementation of the SGDRegressor object shuffles the data before running the optimization algorithm. To get the results from above, you should set its random_state parameter to zero.

The object also implements the estimator API. For instance, we can compute the 
R
2
 with the score() function.

# Compute R2 coefficient
R2_sgd = lr_sgd.score(X, y)
print("R2_sgd:", R2_sgd)
R2_sgd: 0.9821546772612869
Huber loss
We usually fit linear regressions using the least squares approach, i.e., minimizing the squares of the residuals. However, it’s also possible to use other objective functions such as Huber loss.

To achieve this, we can create an HuberRegressor object which also implements the estimator interface.

from sklearn.linear_model import HuberRegressor

# Create the estimator
huber = HuberRegressor(epsilon=1.35)

# Fit it to X,y
huber.fit(X, y)

print("Coefficients:", huber.coef_)
print("Intercept:", huber.intercept_)
print("R^2 coefficient:", huber.score(X, y))
Coefficients: [0.39172544 0.4788203  0.29315421]
Intercept: 0.04586298819193867
R^2 coefficient: 0.9830701571142849
In this code, we create a HuberRegressor object with a threshold of 1.35 and fit it to the X, y data. We can then access the parameters with the coef_ and the intercept_ attributes. They are a bit different than the ones from above because they optimize another cost function. Finally, we compute the 
R
2
 coefficient with the score() function.

Summary
Let’s summarize what we’ve learned in this unit. Here are a few takeaways.

The Scikit-learn library implements several estimators to fit linear regressions, e.g., the LinearRegression, SGDRegressor and HuberRegressor objects
They implement the estimator API, e.g., the fit(), predict(), score() functions.
Scikit-learn estimators work with 2-dimensional arrays of features X, i.e., unlike the polyfit() function which expects a 1-dimensional input vector x.
In the next unit, we will do a summary of the different functions and objects to fit linear regressions, and we will see when to use each method.

06. How to choose the appropriate method?
Content
Questions 1
We’ve seen many different functions and objects to fit linear regressions. Let’s do a summary and see when to use each method.

Numpy polyfit
The polyfit(x, y, deg) function fits a polynomial of degree deg to a set of x/y points. The function computes the least squares solution which corresponds to a simple linear regression model when deg=1. Later in this course, we will see that this function is a special case of linear regressions with a polynomial basis, and we will see how to implement it with the LinearRegression and the PolynomialFeatures objects from Scikit-learn.

Use the polyfit() function when fitting a polynomial between a single input variable x and an output variable y.

OLS: Numpy implementation
The ordinary least squares (OLS) formula is an analytical solution of linear regressions with the RSS cost function. It’s possible to implement the formula with the Numpy matmul() and inv() functions.

⃗
w
=
(
X
⊺
X
)
−
1
X
⊺
⃗
y
However, in practice, we don’t implement this formula by ourselves but use existing optimized implementations.

OLS: Scipy lstsq
There are many ways to implement the OLS solution. You can take a look at this post to learn more about them. The lstsq() function from Scipy uses optimized algorithms to compute it.

Scikit-learn LinearRegression
Scikit-learn provides a LinearRegression estimator to compute the OLS solution which uses the lstsq() function internally. It’s very convenient to work with this object since it implements the estimator API. For this reason, we recommend you to use this object to fit linear regressions.

Scikit-learn HuberRegressor
Scikit-learn also provides a HuberRegressor estimator for linear regressions with Huber loss. The object optimizes this cost function using a iterative method called BFGS.

It’s a good idea to try fitting a linear regression with Huber loss if you believe that there are outliers in the data.

Scikit-learn SGDRegressor
It’s also possible to fit linear regressions with the SGDRegressor estimator from Scikit-learn which implements the stochastic gradient descent (SGD) algorithm. In short, it’s a variant of the gradient descent (GD) algorithm that can optimize many different functions, e.g., Huber loss, squared loss. We will learn more about this algorithm in the next subject.

One of its main advantages is its complexity - it is faster for very large datasets (e.g., millions of data points or tens of thousands of features) and requires less memory. You can take a look at this tutorial from Scikit-learn if you want to learn more about it.

Use it for linear regressions when it’s not possible to use the LinearRegression or the HuberRegressor objects.
07. Ill-conditioning
Content
Resources 1
Questions 9
In this unit, we will learn about ill-conditioning which can cause numerical issues when computing the ordinary least squares solution (OLS). We will start by describing collinearity which is a phenomenon closely related to ill-conditioning. Then, we will see how nearly collinear features can cause numerical issues. Finally, we will experiment with regularization which is a way to handle ill-conditioning.

This time, we will work with the simple version of the bike sharing dataset with just a single input variable: temperatures.

Collinearity
Collinearity (and multicollinearity) happens when there is an exact linear relationship between one or more features in the input matrix 
X
. This means that one feature is a combination of the others plus some constant. This may sound abstract, so let’s take an example.

Say that you are working on the bike sharing dataset. You can find the dataset file in zipped folder under the resources tab.

import pandas as pd

# Load data
data_df = pd.read_csv("c3_bike-sharing.csv")

# Create Numpy arrays
temp = data_df.temp.values
users = data_df.users.values

# First five rows
data_df.head()

As we can see, temperatures are not in degrees Celsius or Fahrenheit. In fact, the dataset web page states that the original temperatures in degrees Celsius temp_C were rescaled between zero and one using the formula temp=(temp_C+8)/47.

We will see later that rescaling features is a good idea in some cases. At the moment, however, you might want to work with temperatures in degrees Celsius temp_C rather than the standardized version temp. To compute this new temp_C variable, we can simply revert the formula from above.

# Create collinear feature
temp_C = 47 * temp - 8
By construction, the temp and the temp_C variables are collinear. The issue with collinear features is that they make the 
X
 matrix with the additional column of ones, and its moment matrix 
X
⊺
X
, rank deficient which means that not all columns are linearly independent. A matrix that is rank-deficient is not invertible. Hence, the OLS solution does not exist.

⃗
w
=
(
X
⊺
X
)
−
1
X
⊺
⃗
y
Let’s illustrate this with the temperature variables.

import numpy as np

# Create input matrix X
X = np.c_[temp, temp_C]

# Add a column of ones
X1 = np.c_[np.ones(X.shape[0]), X]

# Compute rank
rank = np.linalg.matrix_rank(X1)
print("Rank", rank)
Rank 2
In this code, we create the input matrix X with the collinear variables temp and temp_C. We then add a column of ones and end up with an input matrix X1 with three columns. This matrix is rank-deficient because its number of independent features is 2. This number is called the rank of the matrix, and we say that X1 is rank-deficient because its rank is not maximal, i.e., it’s not 3.

Collinearity in practice
In theory, the OLS solution doesn’t exist when 
X
 contains collinear features. However, most machine learning tools can handle these situations and return a vector of parameters 
⃗
w
 as if there were no collinear features.

from scipy.linalg import lstsq

# Compute OLS using lstsq
w, rss, rank, _ = lstsq(X1, users)

print("w:", w)
print("rank:", rank)
print("RSS:", rss)
w: [155.34445517  27.10638524  31.24446504]
rank: 2
RSS: []
In this code, we call the lstsq() function with the rank-deficient matrix X1 which returns a set of parameters w, the rank of the matrix, and the RSS score which is an array of length zero when the matrix is rank-deficient. The function also returns the singular values of the matrix (the fourth return value), but we can discard this result by assigning it to the “throwaway” variable _.

Let’s compare the performance of this model to a simple linear regression with the 
R
2
coefficient. We can use the r2_score(y,y_pred) function from the Scikit-learn metrics module.

from sklearn.metrics import r2_score

# R^2 coefficient of simple linear regression
coefs = np.polyfit(temp, users, deg=1)
y_pred_normal = np.polyval(coefs, temp)
r2_normal = r2_score(users, y_pred_normal)
print("R^2 normal:", r2_normal)
R^2 normal: 0.5954233080185317
# R^2 coefficient with collinear features
y_pred_collinear = np.matmul(X1, w)
r2_collinear = r2_score(users, y_pred_collinear)
print("R^2 collinear:", r2_collinear)
R^2 collinear: 0.5954233080185317
We can see that collinearity didn’t affect performance in this case.

Nearly collinear features
Sometimes, features are highly correlated but there isn’t a perfect linear relationship between them. These are nearly collinear features.

For instance, say that we measure temperatures with two different thermometers. One gives temperatures in degrees Celsius and the other in degrees Fahrenheit. To simulate this scenario, we can simply convert the temp_C values to degrees Fahrenheit.

# Convert to degrees Celsius to Fahrenheit
temp_F = 1.8 * temp_C + 32
It’s unlikely that the two thermometers give the same temperature values. In that sense, temp_F and temp_C are nearly collinear features. To simulate this difference, we can add a small noise to the temp_F variable.

def compute_ols_with_noise(temp_C, users):

    # Convert to degrees Fahrenheit
    temp_F = 1.8 * temp_C + 32

    # Add small variations
    noise = np.random.normal(loc=0, scale=0.01, size=temp_F.shape)
    temp_F += noise

    # Create input matrix X
    X = np.c_[temp_C, temp_F]

    # Compute OLS using lstsq
    X1 = np.c_[np.ones(X.shape[0]), X]  # Create X1 matrix
    w, rss, rank, _ = lstsq(X1, users)  # OLS

    return w, rss, rank, X1


w, rss, rank, X1 = compute_ols_with_noise(temp_C, users)

print("rank:", rank)  # Returns: 3
print("RMSE:", np.sqrt(rss / len(users)))  # Depends on the noise value
print("w:", w)  # Depends on the noise value
rank: 3
RMSE: 230.9823214507588
w: [106585.84112462   6017.5679473   -3325.60382844]
In this code, we generate a random noise using a Gaussian distribution centered at zero and with a standard deviation of 0.01.

This time, X1 is full rank and the lstsq() function returns the residual sum of squares. If you run the code several times, you can see that the coefficients vary a lot.

txt_fmt = "{:<5}{:<6}{:<20}{:}"
print(txt_fmt.format("run", "rank", "RMSE", "coefficients"))
for i in range(5):
    w, rss, rank, X1 = compute_ols_with_noise(temp_C, users)  # Compute OLS using lstsq
    print(txt_fmt.format(i, rank, np.sqrt(rss / len(users)), w))
run  rank  RMSE                coefficients
0    3     231.97901641015937  [-81676.15697432  -4571.65608979   2557.49056003]
1    3     233.32283371464874  [15240.81349185   880.17957622  -471.29978682]
2    3     233.18324423920885  [31923.08919117  1818.61430418  -992.61142281]
3    3     233.09452280151808  [37538.63574666  2134.06339415 -1168.00245259]
4    3     233.229281153421    [25663.36630346  1466.41275646  -796.98589284]
This is due to ill-conditioning.

Ill-conditioning
In the example from above, X1 has full-rank and the OLS solution exists. However, it’s numerically unstable. A small change in the data produces very different coefficients. We can quantify this phenomenon with the condition number. Inverting a matrix with a large condition number is numerically unstable.

We can compute the condition number of X1 with the cond() function from the Numpy linalg module.

# Condition number
cn = np.linalg.cond(X1)
print("Condition number:", cn)  # Depends on the noise value
Condition number: 211480.60364141126
The value of cn depends on the noise in the X1 matrix from above. However, you should get a value above 200 thousand. By increasing the scale of the noise, you decrease the correlation between the two variables and you should see that it reduces the condition number.

Again, ill-conditioning doesn’t necessarily affect the predictive accuracy of the model. In most cases, it will simply result in large variations in the model coefficients.

# Same with the nearly collinear matrix
y_pred_nearcol = np.matmul(X1, w)
r2_nearcol = r2_score(users, y_pred_nearcol)

# R^2 coefficient with nearly collinear features
print("R^2 nearly collinear:", r2_nearcol)
R^2 nearly collinear: 0.5959003332086129
Regularization
One way to solve ill-conditioning is to create a constraint on the coefficients. The idea is to modify the objective function and add a regularization term that penalizes large coefficients.

Scikit-learn implements regularization with the Ridge estimator which is similar to the LinearRegression one.

from sklearn.linear_model import Ridge


def compute_with_regularization(temp_C, users):

    # Add small variations
    noise = np.random.normal(loc=0, scale=0.01, size=temp_C.shape)
    temp_F = (1.8 * temp_C + 32) + noise

    # Create input matrix X
    X = np.c_[temp_C, temp_F]

    # Fit a Ridge regression
    ridge = Ridge(alpha=100)
    ridge.fit(X, users)

    return ridge, X

ridge, X = compute_with_regularization(temp_C, users)

print("Coefficients:", ridge.coef_)
print("Intercept:", ridge.intercept_)
print("R^2:", ridge.score(X, users))
Coefficients: [ 7.35842228 13.572953  ]
Intercept: -273.8593062366117
R^2: 0.5954447415005615
In this code, we create an input matrix X of nearly collinear features, and we fit the Ridge model to it. Note that we pass the X matrix to the fit() function (i.e., without the column of ones) since Scikit-learn objects automatically compute the intercept term. Also, we use an alpha parameter which controls the regularization strength. Finally, we print the model coefficients and the 
R
2
 score.

If you run the code several times, you should see that the coefficient are more stable than before.

txt_fmt = "{:<5}{:<27}{:<21}{:}"
print(txt_fmt.format("run", "coefficients", "intercept", "R^2"))
for i in range(5):
    ridge, X = compute_with_regularization(temp_C, users)
    print(txt_fmt.format(i, str(list(ridge.coef_.round(8))),
                         ridge.intercept_, ridge.score(X, users)))
run  coefficients               intercept            R^2
0    [7.49076098, 13.49937365]  -271.489441959431    0.5954239064077432
1    [7.46639372, 13.51310292]  -271.9384503588652   0.5954277529209779
2    [7.6713482, 13.39906618]   -268.28141166124317  0.5953958502549056
3    [7.53255351, 13.47589654]  -270.7316908626924   0.5954173692406003
4    [7.36875307, 13.5681769]   -273.7426541231872   0.5954432470664646
It’s interesting to note that the second run has an 
R
2
 score of 0.595460937325 which is slightly better than the one of the simple linear regression model from above 0.595423308019. This is due to overfitting - the model fits the noise in the temp_F variable.

But we will learn more about regularization, overfitting and the Ridge estimator later in this course.

Summary
Let’s summarize what we’ve learned in this unit. Here are a few takeaways.

Collinearity happens when there is an exact linear relationship between one or more features. In this case, the
X
 matrix, with the additional column of ones, and its moment matrix 
X
⊺
X
 are rank-deficient and the OLS solution doesn’t exist.
Nearly collinear features make computations numerically unstable which result in large variations in the model coefficients. This is called ill-conditioning. It’s numerically unstable to compute the inverse of matrices with a large condition number.
Regularization stabilizes model coefficients.
Ill-conditioning is a complex topic. You can take a look at this thread if you want to learn more about it. However, note that some answers refer to models that we will see later in this course.

In the next exercise, you will fit a linear regression model to a dataset with multiple features (including collinear and nearly collinear ones). This is an excellent opportunity to experiment with ill-conditioning.
02. Learning using the gradient
Content
In the last subject, we learned about the ordinary least squares (OLS) solution which is the most common way to find the set of parameters of a linear regression model that minimizes the squares of the residuals. We will now see another method called gradient descent (GD) which is a generic approach that can find the optimal parameters of a variety of models and cost functions.

Just like OLS, gradient descent uses the gradient of the cost function to minimize it. In this unit, we will see the basic idea behind the algorithm. We will start with a recap about derivatives and then see why it’s very natural to use them to optimize functions.

Note that in this subject we will be working only with training data. Therefore, all the subsequent predictions and model evaluations, if there is any, will be in-sample, i.e. on the training data. The only exception is the bike sharing exercise and solution at the end of this subject where we use both train and test data.

Follow the gradient
By definition, the derivative of a function corresponds to its rate of change. In the simplest case, the function is a line.


As you can see, its rate of change or slope is the height of the triangle divided by its width.

For more complex curves, the idea is the same. The rate of change at some point 
x
 can be approximated by drawing a triangle between 
x
 and some 
x
+
h
 value.


Using this idea, we can compute the derivative 
f
′
 of a function 
f
 at some point 
x
 with

f
′
(
x
)
=
f
(
x
+
h
)
−
f
(
x
)
h
Where 
f
(
x
+
h
)
−
f
(
x
)
 is the height of the triangle and 
h
 its width. Of course, this only works if our triangle is small enough. In other words, the 
h
 value should tend toward zero.

Why is this formula interesting from an optimization perspective? Well, it indicates the value of the function 
f
(
x
)
 after a small step 
h
. It’s easier to see it if we reverse the formula

f
(
x
+
h
)
=
f
(
x
)
+
h
∗
f
′
(
x
)
In other words, the value of the function 
f
(
x
)
 after a small step 
h
 changes by 
h
∗
f
′
(
x
)
.

If the derivative 
f
′
(
x
)
 is positive, should we take a positive step 
h
>
0
 or a negative step
h
<
0
 in order to decrease the value of 
f
?

Well done!

We should take a negative step

We should take a positive step
If the derivative 
f
′
(
x
)
 is negative, should we take a positive step 
h
>
0
 or a negative step
h
<
0
 in order to decrease the value of 
f
?

That's great!

We should take a negative step

We should take a positive step
Hence, to minimize the value of the function 
f
, we should take a step in the opposite direction of its derivative. Equivalently, we can say that the derivative shows the direction of increase. This observation also applies to functions with multiple parameters. In that case, the gradient 
∇
f
 gives us the direction of the steepest increase. This may sound abstract, so let’s take an example.

This image shows the gradient of the function in blue with two parameters. The red arrows correspond to the values of the gradient at different 
(
x
,
y
)
 locations. They have a dimension of two because the function has two parameters and hence two partial derivatives.


Source: Wikipedia - link to the image

The idea is that this plot could be the error surface of a simple linear regression model where the 
x
 and 
y
 variables correspond to the 
a
, 
b
 parameters and the cost function (an exotic one by the look of it) to 
f
(
x
,
y
)
.

Local minimum
In the gradient descent method, we take small steps in the opposite direction of the gradient. Hence, it might return suboptimal solutions when the function is not convex. A function is not convex if it’s possible to find a line, joining two points on the function, which intersects with it. Again, this may sound abstract, so let’s take an example.

Here are three functions. The left one is convex, and the two others are not. We indicate the minimum value with a green dot.


In the convex case, regardless of where we start on the curve, we will always end up on the green point if we follow the direction of descent. On the other hand, there are no guarantees that we find the optimal solution in the two nonconvex cases. If we start in the wrong place, it’s possible that we get stuck in a local minimum (the red dots).

Summary
Let’s summarize what we’ve learned in this unit. Here are a few takeaways.

Gradient descent follows the opposite direction of the gradient.
It may return suboptimal solutions when the function is not convex.
In the next unit, we will see how to implement the algorithm for the simple linear regression model with the mean squared error (MSE) objective function.
03. Gradient descent algorithm
Content
Questions 1
In the last unit, we saw the basic idea behind gradient descent. We will now derive and implement the algorithm to find the set of optimal 
a
, 
b
 values of a simple linear regression model with the mean squared error (MSE) objective function. However, note that gradient descent is not limited to this model and loss function.

Step size
The gradient descent algorithm works by taking small steps in the opposite direction of the gradient. Hence, before running the algorithm, we should define a step size that is usually denoted by the Greek letter eta 
η
. This parameter should be a positive number and is usually smaller than one. For instance, if we want to minimize a function 
f
 with a single parameter
x
, we perform the following update at each step.

x
←
x
−
η
f
′
(
x
)
In practice, 
f
 is the loss function and depends on the vector of parameters 
⃗
w
. Hence, we update them using the gradient 
∇
f
.

⃗
w
←
⃗
w
−
η
∇
f
(
⃗
w
)
From the previous unit, we know that 
f
 decreases after each step if 
η
 is close enough to zero. The step size, also called learning rate, is an important parameter of the gradient descent algorithm.

if 
η
 is too small, we won’t make enough progress at each step
if 
η
 is too large, we might overshoot the optimal value
Let’s compute the 
∇
f
(
⃗
w
)
 for the simple linear regression model with MSE.

Compute the gradient
The first thing to do is to write the equation of the model. In our case, a simple linear regression one. Here is the equation to compute the prediction 
^
y
i
 for the i-th data point given its input value 
x
i
 and the model parameters 
a
 and 
b
.

^
y
i
=
a
x
i
+
b
We can now define the cost function. In the last unit, we saw that gradient descent can return suboptimal solutions if the function is not convex. For linear regressions, we can use the MSE cost function which is convex. You can take a look at this page if you want to learn about the mathematical proof.

Let’s write the MSE loss function 
L
(
a
,
b
)
 for the simple linear regression model.

L
(
a
,
b
)
=
1
N
N
∑
i
=
1
 
(
y
i
−
^
y
i
)
2
Since the data points 
x
i
 and 
y
i
 are known when fitting the model, this function only depends on the 
a
 and 
b
 parameters. We can expand the formula using the equation for 
^
y
i
.

L
(
a
,
b
)
=
1
N
N
∑
i
=
1
 
(
y
i
−
a
x
i
−
b
)
2
We can now compute the two partial derivatives. To achieve this, we use the following results from calculus.

The derivative of a constant 
c
 is zero. For instance, the derivative of 2 is zero.
The derivative of a variable 
x
 multiplied by a constant 
c
 is 
c
. For instance, the derivative of 
2
x
 is 
2
.
The derivative of a summation of elements is the sum of the derivatives of each element. For instance, the derivative of 
1
+
2
x
+
3
 is 
2
.
The derivative of the square of a variable 
x
2
 is 
2
x
. More generally, the derivative of the square of a function
f
(
x
)
2
 is 
2
f
(
x
)
f
′
(
x
)
 where 
f
′
(
x
)
 is the derivative of the function with respect to 
x
. For instance, to calculate the derivative of 
(
1
+
2
x
+
3
)
2
, we can say that 
f
(
x
)
=
(
1
+
2
x
+
3
)
. Hence, the derivative is
2
(
1
+
2
x
+
3
)
2
=
4
(
1
+
2
x
+
3
)
.
Now, let’s compute the partial derivative of 
L
(
a
,
b
)
 with respect to 
a
.

The derivative of 
(
y
i
−
a
x
i
−
b
)
 is 
−
x
i
 using points 1 to 3. Remember that we are computing the partial derivative with respect to 
a
. Hence, the 
y
i
, 
x
i
 and 
b
 elements are constant and 
a
 is the only variable.
The derivative of 
(
y
i
−
a
x
i
−
b
)
2
 is 
2
(
y
i
−
a
x
i
−
b
)
(
−
x
i
)
 using point 4
Finally, we can use point 3 for the summation and we get that:

∂
L
∂
a
=
1
N
N
∑
i
=
1
 
2
(
y
i
−
a
x
i
−
b
)
(
−
x
i
)
We can do the same for parameter 
b
. The only difference is in the derivative of
(
y
i
−
a
x
i
−
b
)
 which is 
−
1
 since 
b
 is the variable this time.

∂
L
∂
b
=
1
N
N
∑
i
=
1
 
2
(
y
i
−
a
x
i
−
b
)
(
−
1
)
It’s interesting to note that both partial derivatives depend on the error made. We can rewrite these formulas using the residuals 
e
i
=
(
y
i
−
^
y
i
)
=
(
y
i
−
a
x
i
−
b
)
.

∂
L
∂
a
=
−
2
N
N
∑
i
=
1
 
e
i
x
i
∂
L
∂
b
=
−
2
N
N
∑
i
=
1
 
e
i
As we can see, the gradient is proportional to the residuals, and the algorithm makes bigger steps when it’s further away from the optimal solution.

Algorithm
We can now use these partial derivatives to write the algorithm. Note that the learning rate lr, the initial a, b values and the number of iterations n_steps depend on the dataset. For this reason, we don’t assign them to specific values in the code below.

import numpy as np

# Initialization
lr = ... # learning rate
a, b = ... # initial a,b values
n_steps = ... # number of iterations

# n_steps iterations
for step in range(n_steps):
    # Predictions with the current a,b values
    y_pred = a*x + b

    # Compute the error vector
    error = y - y_pred

    # Partial derivative with respect to a
    a_grad = -2*np.mean(x*error)

    # Partial derivative with respect to b
    b_grad = -2*np.mean(error)

    # Update a and b
    a -= lr*a_grad
    b -= lr*b_grad
At each iteration, we compute predictions y_pred using the current a, b values. Then, we create an error variable which is an array of shape (n,), and use it to compute the two partial derivatives. Finally, we update the parameters using the learning rate lr.

Summary
In this unit, we saw how to derive and implement the gradient descent algorithm for the simple linear regression model with MSE. Here are a few takeaways.

Gradient descent uses a learning rate and the gradient of the loss function to compute the parameters update.
It’s an iterative algorithm that takes small steps in the opposite direction of the gradient.
In the next unit, we will test our implementation on the bike sharing dataset.
04. Numpy implementation
Content
Resources 1
Questions 2
In the last unit, we derived the gradient descent algorithm and implemented it with Numpy. We will now test our implementation on the bike sharing dataset. You can download it from the resource section.

At the end of this unit, you should have a better understanding of the algorithm and know how to track its progress.

Monitor the loss value
Let’s start by loading the dataset.

import pandas as pd

# Load the data
data_df = pd.read_csv("c3_bike-sharing.csv")
data_df.head()

Our goal is to fit a simple linear regression model to this data using gradient descent. If our implementation is correct, we should obtain the following result.

%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

# Linear regression using polyfit
x = data_df.temp.values
y = data_df.users.values
coefs = np.polyfit(x, y, deg=1)

# Plot prediction curve
x_values = np.linspace(x.min(), x.max(), num=100)
y_values = np.polyval(coefs, x_values)
plt.scatter(x, y, s=15)
plt.plot(x_values, y_values, c="C3", label="polyfit(deg=1)")
plt.xlabel("temperatures")
plt.ylabel("users")
plt.legend()
plt.show()

This image shows the 302 data points from the dataset and a simple linear regression model obtained with the Numpy polyfit(x, y, deg=1) function.

Let’s implement the algorithm using the code from the previous unit. This time, we will track the value of the root mean squared error (RMSE) function at each iteration. If our implementation is correct, this value should decrease after each step.

# Root mean squared error (RMSE)
def rmse(y, y_pred):
    return np.sqrt(np.mean(np.square(y - y_pred)))
Let’s start by setting the initial values. We can set the learning rate to 0.7 and run the algorithm for 30 steps. Also, we initialize the a, b parameters to zero.

# Initialization
lr = 0.7  # learning rate
a, b = 0, 0
n_steps = 30
We can now run the gradient descent algorithm. Here is the code from the previous unit. We added a line in the for loop to compute and save the RMSE value in a log_rmse variable.

# Save RMSE after each iteration
log_rmse = []

for step in range(n_steps):
    # Predictions with the current a,b values
    y_pred = a * x + b

    # Compute the error vector
    error = y - y_pred

    # Partial derivative with respect to a
    a_grad = -2 * np.mean(x * error)

    # Partial derivative with respect to b
    b_grad = -2 * np.mean(error)

    # Update a and b
    a -= lr * a_grad
    b -= lr * b_grad

    # RMSE error
    log_rmse.append(rmse(y, y_pred))
The code should perform 30 iterations and update the a, b at each step. We can verify that the RMSE value decreases by plotting the log_rmse variable.

# Plot RMSE values
plt.plot(log_rmse, label="learning rate: {}".format(lr))
plt.title("RMSE after {} steps is {:.6f}".format(n_steps, log_rmse[-1]))
plt.xlabel("time step")
plt.ylabel("RMSE")
plt.legend()
plt.show()

We can see that the error decreases after each step. The final value is around 244. We can now verify that the a, b values are optimal by computing the ordinary least squares (OLS) solution. To achieve this, we can use the Numpy polyfit() function.

a_polyfit, b_polyfit = np.polyfit(x, y, 1)

# Optimal parameters vs. ones obtained with GD
print("Polyfit (a, b)=          ({:.2f}, {:.2f})".format(a_polyfit, b_polyfit))
print("Gradient descent (a, b)= ({:.2f}, {:.2f})".format(a, b))
Polyfit (a, b)=          (1495.60, -94.61)
Gradient descent (a, b)= (1133.56, 98.91)
We can see that the values are far from the optimal ones. The slope is 1133.56 instead of 1495.6, and the intercept term is 98.91 instead of -94.61.

Let’s try to increase the number of steps to 400 instead of 30.

# Initialization
lr = 0.7
a, b = 0, 0
n_steps = 400  # increase to 400
We can now rerun the algorithm with these new values.


After 400 steps, the RMSE error is around 233, and the parameters are equal to the ones from the polyfit() function with a precision of 2 digits.

a_polyfit, b_polyfit = np.polyfit(x, y, 1)

# Optimal parameters vs. ones obtained with GD
print("Polyfit (a, b)=          ({:.2f}, {:.2f})".format(a_polyfit, b_polyfit))
print("Gradient descent (a, b)= ({:.2f}, {:.2f})".format(a, b))
Polyfit (a, b)=          (1495.60, -94.61)
Gradient descent (a, b)= (1495.60, -94.61)
Convergence
The algorithm doesn’t always converge to the optimal solution. For instance, if you increase the learning rate to 0.8, you should see that the error increases after each step and becomes very large.

# Initialization
lr = 0.8
a, b = 0, 0
n_steps = 400

In this case, we say that the algorithm diverges. But we will learn more about that in the next unit.

In the plot from above, Matplotlib uses the e-notation to write the values in the y-axis. For instance, the final RMSE value is 7.1e+10 which corresponds to 
7.1
∗
10
10
. In general, we use this notation to write very small or very large numbers.

Stochastic and mini-batch variants
In the code from above, we computed the gradient using all the data points.

# Predictions with the current a,b values
y_pred = a * x + b

# Compute the error vector
error = y - y_pred

# Partial derivative with respect to a
a_grad = -2 * np.mean(x * error)

# Partial derivative with respect to b
b_grad = -2 * np.mean(error)

# Print shape
print("y_pred:", y_pred.shape)
print("error:", error.shape)
print("a_grad:", a_grad.shape)
print("b grad:", b_grad.shape)
y_pred: (302,)
error: (302,)
a_grad: ()
b grad: ()
As you can see, the y_pred, error vectors are arrays with n=302 elements, and we compute the two partial derivates using these 302 data points. In practice, we often calculate the gradient on a subset of the data points. We can distinguish three variants of the gradient descent algorithm.

Batch gradient descent - computes the gradient using all data points.
Mini-batch gradient descent - uses a mini-batch of data to compute gradients, e.g. 64, 128, 256 data points.
Stochastic gradient descent - uses a single data point to compute the gradient.
Mini-batch and stochastic gradient descent are often used in practice because they scale well to large datasets, e.g., with millions of samples. For instance, the SGDRegressor object from Scikit-learn implements the stochastic variant. Note that we usually select the points at random in these two cases.

Summary
Let’s summarize what we’ve learned in this unit. Here are a few takeaways about gradient descent.

Its mini-batch and stochastic variants scale to large datasets with millions of data points.
We can track its progress by monitoring the loss value.
Gradient descent is a generic and easy to implement optimization method. Also, its mini-batch and stochastic variants are very fast and can be applied to large datasets without using much memory. However, note that we usually prefer to use analytical solutions when they exist (e.g., OLS for linear regressions). For instance, we will only use the SGDRegressor object when it’s not possible to use the LinearRegression one.

In the next unit, we will do a small experiment with gradient descent and see that its performance depends on the scale of the features.

05. Issue with the gradient
Content
Resources 1
Questions 3
In the last unit, we used gradient descent to fit a simple linear regression model to the bike sharing dataset. We will now do a simple experiment and see that the algorithm is sensitive to the scale of the data.

Rescaling data
Let’s start by loading the dataset.

import pandas as pd

# Load the data
data_df = pd.read_csv("c3_bike-sharing.csv")
data_df.head()

Again, the dataset has two variables. The temperatures temp and the number of users users.

We saw in a previous unit that these temperatures are not in degrees Celsius or Fahrenheit. In fact, the temp values were computed using the formula temp=(temp_C+8)/47 where temp_C is the original temperatures measured in degree Celsius.

Let’s do a simple experiment. Instead of applying gradient descent with these rescaled temperatures, let’s use the original values in degree Celsius. To compute them, we can simply reverse to formula from above.

# Create x/y arrays
x = 47 * data_df.temp.values - 8  # Degrees Celsius
y = data_df.users.values
Intuitively, rescaling a variable shouldn’t affect the algorithm since we provide the same information. Here is a comparison of the two datasets.

%matplotlib inline
import matplotlib.pyplot as plt

# Compare the two version
fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(7, 3))
ax1.scatter(data_df.temp, y, s=15)
ax1.set_xlabel("temperatures (rescaled)")
ax1.set_ylabel("users")
ax2.scatter(x, y, s=15)
ax2.set_xlabel("temperatures in °C")
plt.tight_layout()
plt.show()

Now, let’s run the gradient descent algorithm with these x, y values. Here is the code from the previous unit.

import numpy as np

# Initialization
lr = 0.7
a, b = 0, 0
n_steps = 400

# Root mean square error (RMSE)
def rmse(y, y_pred):
    return np.sqrt(np.mean(np.square(y - y_pred)))


# Gradient descent
log_rmse = []
for step in range(n_steps):
    # Compute partial derivatives
    y_pred = a * x + b
    error = y - y_pred
    a_grad = -2 * np.mean(x * error)
    b_grad = -2 * np.mean(error)

    # Update parameters
    a -= lr * a_grad
    b -= lr * b_grad

    # Log RMSE score
    log_rmse.append(rmse(y, y_pred))
If you run the code, you should get a RuntimeWarning which says that an overflow happened. This simply means that one instruction led to a number too large to be stored in the computer memory. It’s likely that gradient descent diverged. Let’s verify that by plotting the log_rmse variable.

# Plot RMSE values
plt.plot(log_rmse, label="learning rate: {}".format(lr))
plt.title("RMSE after {} steps is {}".format(n_steps, log_rmse[-1]))
plt.xlabel("time step")
plt.ylabel("RMSE")
plt.legend()
plt.show()

We can see that the RMSE error increases exponentially and that the algorithm diverged. Also, if you print the final value log_rmse[-1], you should get a nan value which represents the result of a numerical operation that didn’t succeed.

Gradient descent step by step
Let’s try to understand why the algorithm diverged with the temperatures in degrees Celsius. The idea is to visualize how the a, b values change after each iteration of the algorithm.

To achieve this, we will use the rescaled temperatures between zero and one.

# Create x/y arrays
x = data_df.temp.values
y = data_df.users.values
Let’s run the algorithm and save the a, b values after each step in a log_a and a log_b variable.

# Initialization
lr = 0.7
a, b = 0, 0
n_steps = 400

# Gradient descent
log_a = [a]
log_b = [b]

for step in range(n_steps):
    # Compute partial derivatives
    y_pred = a * x + b
    error = y - y_pred
    a_grad = -2 * np.mean(x * error)
    b_grad = -2 * np.mean(error)

    # Update parameters
    a -= lr * a_grad
    b -= lr * b_grad

    # Log a, b values
    log_a.append(a)
    log_b.append(b)
We can now plot the two variables.

# Visualize steps
plt.plot(log_a, log_b)
plt.grid()
plt.xlabel("parameter a")
plt.ylabel("parameter b")
plt.show()

As you can see, gradient descent takes a zigzag path to the optimal solution. To understand why it takes this path, we can plot the error surface. The idea is to evaluate the loss value for each 
(
a
,
b
)
 point in the plot from above. Plotting the error surface is not straightforward. For this reason, we provide a visualize_steps() function in the annex below. The function expects four parameters.

log_a, log_b - two lists that contain the a, b values at each iteration of the algorithm
x, y - the input and target values
Let’s test the function.

# Plot the error surface
fig = plt.figure()
visualize_steps(fig, fig.gca(), log_a, log_b, x, y)

As you can see, gradient descent is jumping from one side of the “cost function valley” to the other and converges slowly to the optimal solution in dark blue. Also, it seems that the algorithm is taking unnecessarily large steps in the direction of the 
b
 parameter.

We know from the last units that gradient descent is following the opposite direction of the gradient. In our example, the algorithm is taking this suboptimal path because this direction is far from being the direction of the optimal value. In other words, there seems to be a problem with the gradient.

Issue with the gradient
We can see in the plot from above that the error surface is elongated and has elliptical level curves. For this reason, the opposite direction of the gradient doesn’t point toward the optimal solution. Here is an illustration of the issue.


As you can see, the direction of the steepest descent (the red arrows) given by the gradient doesn’t point toward the optimal value when the level curves are not circular. In this case, gradient descent is more likely to diverge.

This is exactly what happened with the temperatures in degrees Celsius. The error surface is a “steep valley” with gradient values nearly perpendicular to the direction of the optimal solution. Hence, the algorithm simply “jumps” above the valley and gets further away from it after each step.

Summary
Let’s summarize what we’ve learned in this unit. Here are a few takeaways.

The direction of the steepest descent isn’t necessarily the direction of the optimal solution.
For this reason, gradient descent is sensitive to the scale of the features.
In the next unit, we will see how to solve the issue with standardization.

Annex - plotting the error surface
Here is the code of the visualize_steps() function. Note that it uses the meshgrid() and the contourf() functions from Numpy and Matplotlib. You might want to take a look at this stackoverflow answer to understand how they work.

# Plot the error surface
def visualize_steps(fig, axis, log_a, log_b, x, y):
    # Define a grid of a,b parameters
    min_ab = min(min(log_a), min(log_b))
    max_ab = max(max(log_a), max(log_b))

    d = max_ab - min_ab
    min_ab -= d * 0.1
    max_ab += d * 0.1

    a = np.linspace(min_ab, max_ab, num=40)
    b = np.linspace(min_ab, max_ab, num=40)
    a_grid, b_grid = np.meshgrid(a, b)

    # Compute the RMSE score for each a,b pair on that grid
    rmse_grid = np.zeros_like(a_grid)

    for i in range(40):
        for j in range(40):
            a, b = a_grid[i, j], b_grid[i, j]
            rmse_grid[i, j] = rmse(a * x + b, y)

    # RMSE surface
    axis.set_aspect("equal", adjustable="box")
    mpl_contourset = axis.contourf(a_grid, b_grid, rmse_grid, 20, cmap=plt.cm.coolwarm)
    fig.colorbar(mpl_contourset, ax=axis, label="RMSE")

    # Plot the GD steps
    axis.plot(log_a, log_b, c="#00abe9")
    axis.scatter(log_a, log_b, c="#00abe9")

    # Set titles and labels
    axis.set_xlabel("parameter a")
    axis.set_ylabel("parameter b")

    axis.set_xlim(min_ab, max_ab)
    axis.set_ylim(min_ab, max_ab)
    06. Standardization
Content
Resources 1
Questions 3
In the last unit, we saw that gradient descent is sensitive to the scale of the features. We thus need to ensure that all the features have the same scale. In this unit we will learn how to solve this issue with standardization. In a later unit we will introduce some alternative approaches.

Standardization
In the unit about outliers, we saw that we can remove outliers by computing the standard scores or z-scores.

z
=
x
−
μ
σ
This process is called standardization. We standardize the values 
x
 of a feature by subtracting its mean 
μ
 (mu) and by dividing the result by its standard deviation 
σ
 (sigma).

It’s important to note that

z
=
0
 when 
x
=
μ
, i.e., standardize a value equal to the mean
z
=
±
1
 when 
x
=
μ
±
σ
, i.e., standardize a value one standard deviation away from the mean
In other words, each standardized feature has a mean of zero and a standard deviation of one. This might be helpful to remember the formula.

Say that you implemented a scale(X) function to standardize a matrix of features X. Now, you want to test it on one of your datasets. How can you verify that your implementation is correct?

That's great!
Yes. Remember that the matrix of features X is a 2-dimensional array where each column corresponds to a feature and each row to a data point. Hence, it's the mean and the standard deviation of the columns (axis-0) that are respectively zero and one.



By checking that numpy.mean(scale(X), axis=0) and numpy.std(scale(X), axis=0) return an array of 0s and an array of 1s respectively

By checking that numpy.mean(scale(X), axis=0) and numpy.std(scale(X), axis=1) return an array of 0s and an array of 1s respectively

By checking that numpy.mean(scale(X), axis=1) and numpy.std(scale(X), axis=0) return an array of 0s and an array of 1s respectively

By checking that numpy.mean(scale(X), axis=1) and numpy.std(scale(X), axis=1) return an array of 0s and an array of 1s respectively
A common misconception is that standardization makes the original distribution look more like a normal Gaussian distribution. Standardization only changes the scale of the values, while preserving the overall shape of the distribution. Thus standardization does not redistribute the values. The rescaled values will have the same problems of long tails and skewness and the same bumps and subpopulations.

So while the rescaled values have a zero mean and a standard deviation of 1, they are typically not normally distributed (unless they where so already beforehand.)

Standardize temperatures
Here we will continue to work with the code from the previous unit. Scikit-learn implements a scale() function in its preprocessing module to apply standardization. Let’s try it with the temperature variable x from the previous unit.

from sklearn.preprocessing import scale

# Standardize x
x_standardized = scale(x)

print("Mean:", x_standardized.mean())
print("Standard deviation:", x_standardized.std())
Mean: -7.058371547285764e-17
Standard deviation: 1.0
We can see that the mean is very close to zero, i.e., it’s a seven preceded by 16 zeros. It’s not exactly zero because computers store floating point numbers using a limited amount of memory. For this reason, small rounding errors can occur. You can take a look at this website which explains the problem in more details.

Let’s rerun the gradient descent algorithm with this x_standardized variable. Here is the code from the previous unit.

# Initialization
lr = 0.7
a, b = 0, 0
n_steps = 400

# Gradient descent
log_a = [a]
log_b = [b]

for step in range(n_steps):
    # Compute partial derivatives
    y_pred = a * x_standardized + b
    error = y - y_pred
    a_grad = -2 * np.mean(x_standardized * error)
    b_grad = -2 * np.mean(error)

    # Update parameters
    a -= lr * a_grad
    b -= lr * b_grad

    # Log a, b values
    log_a.append(a)
    log_b.append(b)
We can now plot the error surface with the visualize_steps() function from the previous unit.

# Plot the error surface
fig = plt.figure()
visualize_steps(fig, fig.gca(), log_a, log_b, x_standardized, y)

As we can see, the level curves are now perfectly circular and that the algorithm goes in the direction of the optimal solution. However, it seems that the algorithm overshoots the optimal solution. We can counteract this behavior by decreasing the learning rate.

# Initialization
lr = 0.1
a, b = 0, 0
n_steps = 400
If you rerun the algorithm and plot the error surface, you should get the following result.


Summary
In this unit, we learned about standardization. Here are a few takeaways.

Standardization ensures that all features have the same scale.
We should always apply standardization before running gradient descent.
Standardized features have a mean of zero and a standard deviation of one.
Gradient descent is faster to converge when features are standardized.
In the next exercise, you will implement gradient descent to fit a multiple linear regression model. Hence, you will need to standardize each feature in the dataset before applying the algorithm.
09. Note about feature scaling
Content
Questions 2
In the last units, we learned that we should standardize the features before applying gradient descent, because the algorithms converges faster when all features have the same scale. In machine learning, many algorithms and models require the input features to have approximately the same scale. In this unit we will discuss and compare a few scaling approaches.

The simplest and most common scaling approaches apply linear transformations to the feature values 
x
i
. Hence we speak of linear scaling. The general formula looks like this:

x
′
i
=
x
i
−
b
a
where
a
>
0.
where the values of 
a
 and 
b
 are typically descriptive statistics related to the distribution of our feature values.

You might be more used to linear transformations being written as 
y
=
m
x
+
c
. Here we have simply set 
m
=
1
a
 and 
c
=
−
b
a
. The advantage of the above formula is that we can easily interpret it as follows:

Scaling shifts the data to the left by 
b
 and then squeezes (
a
>
1
) or stretches it (
0
<
a
<
1
).

Observe that 
x
i
=
b
 is mapped to 
x
′
i
=
0
 and that two values 
x
i
,
x
j
 that were originally 
a
units apart will now be 
1
 unit apart.

Careful, it is a common misconception that scaling corrects the distribution, or can be used to address related problems like skewness. It only changes the scale. An important property of linear transformations is that they preserve the relative distance between different feature values. Hence we often use linear scaling because we actually want to preserve the overall shape of the feature distributions.

Below, we will compare two methods for feature scaling: min-max scaling and standardization.

Min-max scaling
In the last exercises, we worked with the bike sharing data. If you look at the dataset web page, you can see in the “attribute information” section that the temperatures, in degrees Celsius, were rescaled between zero and one using the min-max scaling formula.

x
′
i
=
x
i
−
min
(
x
)
max
(
x
)
−
min
(
x
)
Here we have 
b
=
min
(
x
)
 and 
a
=
max
(
x
)
−
min
(
x
)
 is the width of the range
[
min
(
x
)
,
max
(
x
)
]
. As a result 
min
(
x
)
 is shifted to 
0
 and 
max
(
x
)
 is mapped 
1
 (simply set
x
 to 
min
(
x
)
 or 
max
(
x
)
 in the formula). In short, all features values (including outliers) are mapped onto the same interval: 
[
0
,
1
]
. Hence the name min-max-scaler.

A undesired side effect of mapping all values into the predefined interval 
[
0
,
1
]
 is the fact that when features have a few extreme values like outliers or features are strongly skewed then the majority of the data (the bulk) may be mapped to a very small interval.

This transformation is also known as normalization. However, we won’t use this term in this course, because it can also refer to the process of normalizing a vector to change its length to one.

Standardization
Min-max scaling is often used in machine learning (see also the end of this unit), but it’s not the only way to rescale features. In the previous exercise, we applied standardization before gradient descent to change the mean and the standard deviation of the temperatures using the z-score formula:

x
′
i
=
x
i
−
μ
σ
.
Here we have 
b
=
μ
 and 
a
=
σ
. Simply put standardization maps feature values to their associated z-scores. As a result the new mean and standard deviation of all features are zero and one. However, unlike for the min-max-scaler, the standardized features are not bounded in a predetermined range. Some input features might be skewed or have outliers, so that the new 
min
 and 
max
, and hence the range, will differ from feature to feature.

Standardization is typically required for principal component analysis (PCA), the k-nearest neighbors (kNN) model or clustering algorithms.

A common misconception is that the min-max-scaler (“normalization”) or standardization (z-scores) make the original distribution look more like a normal Gaussian distribution. however as highlighted above, scaling preserves the overall shape of the feature distributions and does not redistribute the values. The rescaled values will have the same problems of long tails and skewness and the same bumps and subpopulations.

When to apply feature scaling
In practice, we often need to rescale our data because some algorithms don’t work well when the features have a different range. For instance, we saw that we need to apply standardization before fitting a linear regression with gradient descent. In the next course, we will see other algorithms that require standardization. For instance, the k-NN and the support vector machine (SVM) algorithms are based on distance measures between data points. In that case, we will see that the algorithm is biased towards features with larger scales, and this bias reduces its performance.

Scaling is also a recommended preprocessing step when working with deep neural networks. They use a process quite similar to the gradient descent and hence unscaled features can lead to slow convergence of these models or result in an unstable learning process. Deep neural networks often use a variation of the min-max-scaler by mapping the values to the range 
[
−
1
,
1
]
 (Simply replace 
b
 by the midpoint of 
m
i
n
(
x
)
 and 
m
a
x
(
x
)
 and divide 
a
 by 2).

Here is a comment from the documentation of the pretrained MobileNet neural network that can be used for image recognition tasks:
Note: each Keras Application expects a specific kind of input preprocessing.
For MobileNet, call keras.applications.mobilenet.preprocess_input on your inputs before passing them to the model. mobilenet.preprocess_input will scale input pixels between -1 and 1.

However, there are some counterexamples. For instance, unless we want to interpret the co efficients for feature importance, we don’t need to rescale features when fitting a linear regression with the ordinary least squares (OLS) method. In the next course, we will also learn about decision trees and random forests which don’t require preprocessing.

Min-max scaling versus standardization
We will mostly use standardization in this course, but it’s good to know both techniques since each has its advantages. Let’s take an example.

Say that you want to rescale a set of values. If you use min-max scaling, then you are sure that your data lies between zero and one. On the other hand, if you use standardization, your data is not bounded by a minimum and a maximum value. In this image, we rescaled 50 data points using min-max scaling and standardization. We show each version of the data using a swarmplot from the Seaborn library.


As you can see, min-max scaling guarantees that all points lie between zero and one which is a useful property in some situations.

With standardization, we know that the data points are centered around zero (mean of zero) and that they have a fixed dispersion (standard deviation of one). In our example, the points vary between -2.5 and +2, but we could have points further away.

As highlighted above we are using statistics to fix the values of 
a
 and 
b
. In the case of min-max scaling and standardization these statistics are sensitive to outliers, which can affect our results. For instance, we can add an outlier with a value of -1.5 to the 50 data points.


As you can see, the outlier is five standard deviations away from the mean, and the green points vary between -5 and +1.5. However, standardization worked better than min-max scaling which squeezed the previous data points between 0.5 and 1. However, if the outlier were more extreme than the effect on the mean and the standard deviation would be more severe and standardization would squeeze the data together as well. For that reason in practice we often consider first removing outliers before applying scalers.

Other approaches
Since our problem with the outlier arose from using descriptive statistics that were sensitive to outliers we might instead apply statistically robust statistics like the median and the interquartile range IQR (the difference between the 25th and 75th percentiles). We set
b
=
m
e
d
i
a
n
(
x
)
 and 
a
=
I
Q
R
:

x
′
i
=
x
i
−
m
e
d
i
a
n
I
Q
R
.
So if there is reason to keep the outlier in the dataset, we may prefer to use the robust scaler.

When working with multiple features, we tailor a separate scaler to each feature and may use different types of scalers for different features. Or we may need to apply some non-linear transformations to some features before applying the scalers. In the next section we will introduce log-transformations.

The success of each approach depends on several aspects: the importance of outliers in the given domain, the machine learning algorithms we intend to use later and most importantly the data itself. Different approaches are more or less effective in different case hence it is important to always visualize your data first before selecting your approach.

Summary
Let’s summarize what we’ve learned in this unit.

Min-max scaling rescales features between zero and one. It may rescale them to very small intervals when there are outliers.
Standardization changes the mean and standard deviation using the z-score formula. Standardized values are not bounded.
The best approach depends on the data and hence we should always visualize the data first.
So far, we applied feature scaling before fitting our models. In the next unit, we will see how to use these models to compute predictions for new unscaled data points.

QUESTIONS



02. Feature engineering
Content
Questions 3
We will now learn about feature engineering which is the process of building new features from existing ones. There are many different ways to extend the set of features, and we will start by discussing polynomial features which is at the basis of polynomial regressions. We will then present other feature engineering techniques and illustrate logarithmic transformations with the Boston housing dataset.

Polyfit revisited
In the previous subjects, we saw that the polyfit(x, y, p) function models the relationship between a single input variable 
x
 and a target variable 
y
 by fitting the equation of a polynomial of degree 
p
.

^
y
=
w
0
+
w
1
x
+
w
2
x
2
+
w
3
x
3
+
…
+
w
p
x
p
This is also called a polynomial regression. Intuitively, the formula seems different than the linear regression one because it involves powers of 
x
. However, there is a little trick. The 
x
variable is known when fitting the model. Hence, we can create new features from it. In particular, we can generate a feature for each power in the equation from above.

z
=
[
x
,
x
2
,
x
3
,
…
,
x
p
]
We can now use this new list of features 
z
 and rewrite the equation.

^
y
=
w
0
+
w
1
z
1
+
w
2
z
2
+
w
3
z
3
+
…
+
w
p
z
p
As we can see, each feature 
z
i
 is associated with a coefficient 
w
i
, and the formula corresponds to a linear regression one. In other words, by fitting a linear regression to the polynomial features in 
z
, we obtain the coefficients of the polynomial equation from above.

Augmenting the input
Creating new features from existing ones is called feature engineering. The idea is to build an extended set of features to improve the performance of our models. There are many ways to create new features.

Combine numerical variables - windspeed * humidity
Combine binary variables - weekend and sunny
Create indicator variables - windspeed > mean(windspeed)
Applying transformations - log(x), sqrt(x), polynomial features
Other functions - max(x1, x2), distance(x1, x2)
Building meaningful features is not an easy task and requires domain knowledge. However, it’s often critical to the success of a machine learning project. Here is a quote from the famous paper “A Few Useful Things to Know about Machine Learning”.

“At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used. If you have many independent features that each correlate well with the class, learning is easy. On the other hand, if the class is a very complex function of the features, you may not be able to learn it. Often, the raw data is not in a form that is amenable to learning, but you can construct features from it that are. This is typically where most of the effort in a machine learning project goes. It is often also one of the most interesting parts, where intuition, creativity and “black art” are as important as the technical stuff.” - Pedro Domingos

Source: Pedro Domingos, point 8 of “A Few Useful Things to Know about Machine Learning” - google scholar page

Boston housing dataset
In practice, we often apply logarithmic transformations to one or more features in the dataset. Let’s take the Boston housing dataset example.

In this machine learning task, the goal is to build a model to predict the housing values in suburbs of Boston using several features. Here are some variables from the dataset.

crime - per capita crime rate by town
indus - proportion of industries per town.
ptratio - pupil-teacher ratio by town
..
To solve the task, we can try fitting a linear regression to the data. We know that this model works better when there is a linear relationship between the features and the target variable. However, in this example, we can observe a clear inverse relationship between the crime rate and the housing values.


Most of the data points have a small crime rate, and very few have a large one. However, we can use a logarithmic scale to redistribute the values. The two plots from above compare two simple linear regressions in red. One with the raw values, and the other with the transformed crime rates.

In general, we usually apply the logarithmic function to features that have a skewed distribution. We can detect these variables using histograms.


Looking at the two plots, we can see that the value distribution of the feature “crime rate” is much closer to a normal distribution, after a logarithmic transformation is applied.

For this reason, we might want to try fitting a linear regression to the following set of variables.

z
=
[
log
(
crime
)
,
indus
,
ptratio
,
…
]
Summary
In this unit, we saw the basic idea behind feature engineering. Here are a few takeaways.

Feature engineering is the process of building new features from existing ones.
The polyfit() function fits a linear regression to a set of polynomial features.
Applying the logarithmic transformation to variables with a skewed distribution can help.
In the next unit, we will implement polynomial regressions using Scikit-learn.

As you can see, electric cars consume more energy during cold weather. The optimal consumption is around 70°F (21°C) and then increases with warmer temperatures.

We can use a polynomial to model the relationship between the two variables. Let’s see how to do that with Scikit-learn.

Create the polynomial features
Scikit-learn implements a PolynomialFeatures object to build the polynomial features. Note that it’s considered as a preprocessing step because it’s done before fitting the model.

from sklearn.preprocessing import PolynomialFeatures
In the Scikit-learn jargon, this object is a preprocessor which means that it implements the fit() and transform() functions.

Here is the code to create the polynomial features that correspond to a polynomial of degree 2.

import numpy as np

# Create the object
pf_obj = PolynomialFeatures(degree=2)

# Create the polynomial features
X2 = pf_obj.fit_transform(
    # Pass two dimensional array
    x[:, np.newaxis]  # (57,) -> (57,1)
)

print("Shape:", X2.shape)
Shape: (57, 3)
We start by creating the PolynomialFeatures object and set its degree with the degree parameter. Then, we obtain the features by passing the input data to its fit_transform() function which is a shortcut for the fit() and transform() ones.

# Fit the preprocessor
pf_obj.fit(x[:, np.newaxis])

# Create the polynomial features
X2 = pf_obj.transform((x[:, np.newaxis]))
In this code, fit() computes the number of output features and transform() creates them. Again, Scikit-learn works with 2-dimensional Numpy arrays, and we need to add a dimension to x using the x[:, np.newaxis] syntax.

Scikit-learn labels each new feature in X2, and we can obtain these labels with the get_feature_names_out() function.

# Get feature names
feature_names = pf_obj.get_feature_names_out()

print("Features:", feature_names)
Features: ['1' 'x0' 'x0^2']
Here, x0 denotes temperatures. As we can see, Scikit-learn adds a 1 to the polynomial features which corresponds to the intercept term. However, we will use the LinearRegression object which already fits one. Hence, this new feature is not necessary and we can remove it by setting the include_bias parameter to False.

# Create the object
pf_obj = PolynomialFeatures(degree=2, include_bias=False)

# Create the polynomial features
X2 = pf_obj.fit_transform(x[:, np.newaxis])

# Get feature names
feature_names = pf_obj.get_feature_names_out()

print("Features:", feature_names)
Features: ['x0' 'x0^2']
It’s also possible to create the new set of features manually with the Numpy c_ object.

# Create the polynomial features manually
X2 = np.c_[x, x ** 2]
Fit a linear regression
Let’s fit a linear regression to the new set of features.

from sklearn.linear_model import LinearRegression

# Create linear regression
linreg = LinearRegression()

# Fit it
linreg.fit(X2, y)
LinearRegression()
We can now plot the model by generating a hundred sample data points.

# Generate a hundred values between min(x) and max(x)
x_values = np.linspace(min(x), max(x), num=100)
X_values2 = pf_obj.transform(x_values[:, np.newaxis])
Note that we need to create the polynomial features for these x_values with the transform() function. Again, you can also use the Numpy c_ object.

X_values2 = np.c_[x_values, x_values ** 2]
We can now compute and plot the predictions for these sample points.

# Compute predictions
y_values = linreg.predict(X_values2)

# Plot predictions
data_df.plot.scatter("Temperature [°F]", "Energy Consumption [Wh/mil]")
plt.plot(x_values, y_values, color="C3")
plt.show()

The model captures the main trend but doesn’t fit well the data. You can try to increase the degree of the set of polynomial features. We leave it to you as an exercise. Here is the result for degrees 3, 5 and 10.


It seems that the curve gets more complex when we increase the degree which can lead to overfitting. Large degrees might result in models that fit the noise in the data rather than the signal. In our example, the degree five seems to generalize well from the data.

Summary
In this unit, we learned how to build a set of polynomial features with Scikit-learn and Numpy, and saw that large degrees can lead to overfitting. In the next unit, we will see how to detect models that overfit using the train/test sets methodology.
04. Detect models that overfit
Content
Questions 1
In the last unit, we saw that it’s easy to build models that overfit. A simple way to identify such models is to split the data into train/test sets. Let’s see how this works.

Detect models that overfit
Let’s fit two models to the following set of 18 x/y data points. One is a simple linear regression and the other is a polynomial regression of degree 9.


We can see on the plots that the simple line provides a good representation of the overall trend in the data. On the other hand, the complex curve (polynomial with degree 9) goes beyond representing the true trend and likely fits the noise in the data. In fact, it overfits the data. As a result, looking only at error values after fitting the models can be misleading. For instance, here are the MSE scores of these two models.

MSE polynomial regression: 1.14
MSE linear regression: 2.39
According to these scores, the polynomial fits better the data points than the line. However, the polynomial regression is overfitting which means that it fits well the actual data points (train set) but will fail to generalize to new data points (unseen data or test set). We show this in the following by splitting the data into two sets.

Train and test sets
One solution to detect the overfit is to split the data into train and test sets. We learned earlier in this course that we fit our models on train set and we evaluate our models on test set which represents unseen data points.

In practice, we usually shuffle the data points before splitting them because we want to avoid having train/test sets that don’t represent well the data. For our example, we take 10 points for the train set the other 8 for the test set.


Let’s fit the two models again using the train set.


It’s interesting to note that the polynomial passes through each data point from the train set. Hence, its MSE score on this set is zero.

Train set
MSE polynomial regression: 0.00
MSE linear regression: 2.93
However, if we evaluate the MSE scores for the test set, we can see that the polynomial has a larger error than the line. This error is an estimation of the generalization error, i.e., the error made on new, unseen data points.

Test set
MSE polynomial regression: 6.01
MSE linear regression: 2.10
By looking at the differences between the train and the test scores, it’s clear that the polynomial is overfitting. Later in this course we introduce regularization that helps to avoid overfitting.

Summary
Let’s summarize.

We can detect models that overfit by looking at the differences between the error made on the train and the test sets.
The error on the test set is an estimation of the generalization error.
In the next unit, we will work with the electric car consumption data set. We will split the data into train/test sets with Numpy and Scikit-learn, and use them to compare two polynomials of different degrees.

05. Splitting data
Content
Resources 1
In this unit, we will see how to split data into train and test sets using Numpy and Scikit-learn. We will work with the electric car consumption dataset.

Splitting data with Numpy
Let’s start by loading the dataset.

import pandas as pd

# Load the data
data_df = pd.read_csv("c3_electric-cars.csv")

data_df.head()

Our goal is to split these x/y points into a train and a test set. To achieve this, we will generate two lists of indexes. One for data points in the train set and the other for data points in the test set.

First, let’s start by creating a list with all the indexes that we can then split into two.

import numpy as np

# Generate a list of indexes
n = len(data_df)
indexes = np.arange(n)

print("indexes:", indexes)
indexes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56]
In this code, we count the number of data points n and generate a list of indexes between 0 and n-1 using the Numpy arange() function.

We can now split this list. For this example, we will do a 70-30 split.

# Split into train/test indexes
split_idx = int(n * 0.7)
train_idx = indexes[:split_idx]
test_idx = indexes[split_idx:]

print("train indexes:", train_idx)
print("test indexes:", test_idx)
train indexes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38]
test indexes: [39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56]
The idea is to compute the index split_idx at which we need to split the data, and then use it to separate the list of indexes into two. Note that the data is not necessarily separable into 70-30 parts. For this reason, we use the int() Python built-in function to convert n*0.7 into an integer.

Finally, we can separate the x/y data points into train/test sets using the train_idx and test_idx lists.

# Extract x, y data
x = data_df.iloc[:, 0].values  # Temperatures
y = data_df.iloc[:, 1].values  # Consumption

# Split data
x_tr, y_tr = x[train_idx], y[train_idx]
x_te, y_te = x[test_idx], y[test_idx]

print("train:", x_tr.shape, y_tr.shape)
print("test:", x_te.shape, y_te.shape)
train: (39,) (39,)
test: (18,) (18,)
As we can see, there are 39 data points in the train set and 18 in the test one. Let’s plot these two sets.


%matplotlib inline
import matplotlib.pyplot as plt

# Plot data points
plt.scatter(x_tr, y_tr, label="train set")
plt.scatter(x_te, y_te, label="test set")
plt.xlabel("temperature")
plt.ylabel("Consumption")
plt.legend()
plt.show()

This is a bad split because neither of the sets represents well the entire data. The issue is that the data points are ordered by temperatures in the original .csv file. Hence, when we split the data, the 39 points with the lowest temperatures go into the train set and the remaining 18 into the test set.

Shuffling the indexes
One solution is to shuffle the data before splitting it. In this unit, we will shuffle the list of indexes rather than the data points directly. This has the advantage of leaving the original data unmodified.

To achieve this, we can use the shuffle() function from the Numpy random module.

# shuffle the indexes
np.random.shuffle(indexes)

print("indexes:", indexes)  # result depends on the seed
indexes: [35 34 41 27 11  2 33 56 22 55  4 10 29 50 32 40 37  7 14 31 28 46 51 18
 54 26 15  5 30 16 48 20 49  8 13 25 17 42 45 38  1 12 43 24  6 23 36 21
 19  9 39 52  3  0 53 47 44]
Numpy shuffles the data differently each time we run this code. In fact, it uses a random number generator to shuffle the data, and we can fix its results by setting its seed number with the seed() function.

# Create the indexes
indexes = np.arange(n)

# Shuffle the indexes
np.random.seed(0)
np.random.shuffle(indexes)

print("indexes:", indexes)
indexes: [35 34 41 27 11  2 33 56 22 55  4 10 29 50 32 40 37  7 14 31 28 46 51 18
 54 26 15  5 30 16 48 20 49  8 13 25 17 42 45 38  1 12 43 24  6 23 36 21
 19  9 39 52  3  0 53 47 44]
It’s important to understand that each seed number generates a different but fixed result. Hence, by setting the seed, we get predictable results. You can take a look at this StackOverflow answer to learn more about random number generators.

We can now split the indexes into train/test indexes and then plot them.

# Split into train/test indexes
split_idx = int(n * 0.7)
train_idx = indexes[:split_idx]
test_idx = indexes[split_idx:]

# Split data
x_tr, y_tr = x[train_idx], y[train_idx]
x_te, y_te = x[test_idx], y[test_idx]

# Plot data points
plt.scatter(x_tr, y_tr, label="train set")
plt.scatter(x_te, y_te, label="test set")
plt.xlabel("temperature")
plt.ylabel("Consumption")
plt.legend()
plt.show()

The two sets are now good samples of the original data.

With Scikit-learn
Splitting the data is a common operation in machine learning. For this reason, Scikit-learn implements a train_test_split() function in its model_selection module.

from sklearn.model_selection import train_test_split

# Split data
x_tr, x_te, y_tr, y_te = train_test_split(
    x, y, train_size=0.7, test_size=0.3, random_state=0
)
The function takes a set of arrays and split them into train/test sets. We can specify the size of each set with the train_size and the test_size parameters, and set the seed with random_state. In this code, the two sizes are percentages of data. But we can also specify the number of points.

from sklearn.model_selection import train_test_split

# Split data
x_tr, x_te, y_tr, y_te = train_test_split(
    x, y, train_size=39, test_size=18, random_state=0
)
Note that the function also works with two-dimensional arrays and DataFrames. For instance, you will use it in the next exercise to split a dataset with multiple features.

Model selection
In a previous unit, we fitted a polynomial of degree 5 and another of degree 10 to this dataset. Let’s see which one has a better generalization error according to our test set.

We create the two models using the Numpy polyfit() function and evaluate them with the mean_squared_error() function from Scikit-learn.

from sklearn.metrics import mean_squared_error as mse

# Polynomial regressions of degree 5 and 10
coef5 = np.polyfit(x_tr, y_tr, deg=5)
coef10 = np.polyfit(x_tr, y_tr, deg=10)

# Evaluate performance
y_pred5_tr = np.polyval(coef5, x_tr)
y_pred5_te = np.polyval(coef5, x_te)

print(
    "Degree 5 MSE: {:.0f} (train) vs {:.0f} (test)".format(
        mse(y_tr, y_pred5_tr), mse(y_te, y_pred5_te)
    )
)
# Degree 5 MSE: 719 (train) vs 651 (test)

y_pred10_tr = np.polyval(coef10, x_tr)
y_pred10_te = np.polyval(coef10, x_te)

print(
    "Degree 10 MSE: {:.0f} (train) vs {:.0f} (test)".format(
        mse(y_tr, y_pred10_tr), mse(y_te, y_pred10_te)
    )
)
Degree 5 MSE: 719 (train) vs 651 (test)
Degree 10 MSE: 707 (train) vs 1218 (test)
We can see that the polynomial of degree 10 has a better training error than the polynomial of degree 5. However, its test score nearly doubles which means that it’s likely overfitting. On the other hand, the polynomial of degree 5 seems to generalize well from the data with a test score close to its train score.

Note that in practice we don’t use the test data to decide about the degree of the polynomial. Instead, we can hold out some of the training data as the validation data and use it to decide which degree of the polynomial leads to the best prediction. We can consider the degree of the polynomial as a hyperparameter and tune it using the validation data. We will discuss hyperparameter tuning in the next subject.

A note on sample split for time series data
Splitting time series data should be done with extra care. In time series modeling and prediction, the objective is often to use data from the past to predict a future value. A particularity of time series data is that strength of the relation between the predictors and the target can vary significantly over time for some phenomena. For instance, nowadays, a company’s stock price is much more sensitive to its carbon footprint than in the past. Therefore, a model cannot capture such sensitivity from a train set that contains only the past data. To mitigate this issue we can think of several splitting schemes.

Suppose that we have daily data from 2010 to 2020. A simple scenario is to split the data into a train set that starts from 2010 and ends in 2015, and a test set from 2016 to 2020 (see the image below). This method requires doing the training and testing only once and is simple to implement. However, if there are major changes in the data during the test period, the test data will be significantly different from the train data. Therefore, a model that is trained on the train data will not return a good prediction on the test data. This weakness can be addressed by using rolling windows or expanding windows as described in the following two scenarios.


Scenario 1: in this scenario the train and test split is performed multiple times using rolling windows. That is, by shifting the windows forward the most recent data points come into play whereas the oldest data points are excluded from the windows. For instance, the first train set can start from 2010 to 2015 followed by the test set of 2016 (see the image below). The second split leads to a train set from 2011 to 2016 followed by a test set of 2017. In the last split, the train set is from 2014 to 2019 and is followed by a test set of 2020. By training and testing our model on each split, we will have 5 test scores and the final score can be calculated by taking the average or median of these scores.


Scenario 2: this case is similar to the scenario with rolling windows, except that the start of the training set is fixed and the windows can only expand for each split (see the image below). An advantage of this method is that the earlier data points will remain in the training set, and the model will be also trained on them. However, a disadvantage of this method is that it can be computationally intensive for big data sets.


Note that our choice of 1-year increment here is arbitrary and can be different depending on the granularity of the data and machine learning project that we intend to do. Moreover, the choice of these two scenarios over the single split scheme can depend on the domain of analysis. For instance, the single split scheme for financial markets which are subject to irregularities and randomness may result in less accurate test scores.

Summary
In this unit, we saw two different ways to split the data into train/test sets.

Manually, by separating the data points using a list of indexes.
With the train_test_split() function from Scikit-learn.
In the next exercise, you will work on an extended version of the bike sharing dataset with additional polynomial features, and you will use this train/test sets methodology to evaluate your models. This extended dataset also contains categorical features. In the next unit, we will see how to preprocess them for linear regressions with one-hot encoding.
06. One-hot encoding
Content
Resources 1
Questions 9
So far, we worked with variables that are quantitative, e.g., temperatures, wind speed, humidity. But many datasets also have categorical features, e.g., the day of the week, season, type of weather. In this unit, we will see how to preprocess these categorical variables using one-hot encoding.

This time, we will work with a modified version of the bike sharing dataset with one quantitative and one categorical variable.

Categorical variables
Let’s start by loading the dataset.

import pandas as pd

# Load the data
data_df = pd.read_csv("c3_bike-small.csv")

# First five rows
data_df.head()

The dataset has three variables. The first one corresponds to temperatures and the second one to the day of the week. Our goal is to predict the number of casual users using these two features.

The weekday variable is categorical and associates each day of the week with a numerical value between 0 and 6. We can get the different values using the value_counts() function from Pandas. The function should return a Pandas Series with the different values and their number of entries. (Alternatively we could use the unique() function for this column and get an array.)

data_df.weekday.value_counts()
6    105
0    105
1    105
2    104
3    104
4    104
5    104
Name: weekday, dtype: int64
In this example, Monday corresponds to zero and Sunday to six. But this numerical encoding is somewhat arbitrary. In fact, with this encoding, the weekday variable doesn’t indicate the day of the week but how far we are from the beginning of the week. However, in its current format a linear regression model would try to interpret the numerical values so that Wednesday (2) is twice as much as Tuesday (1) and half of Friday (4). We know this makes no actual sense, but the model does not.

Let’s do a quick experiment. Let’s fit a linear regression with and without this variable and compare the two 
R
2
 coefficients. Note that in this unit we work only with the training set and for simplicity we use the names x, y instead of x_tr, y_tr.

# Create X/y data
X = data_df[["temp"]].values
y = data_df.casual.values

# Fit a linear regression
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X, y)
lr.score(X, y)
0.29525001840042764
In this code, we fit a linear regression only on temperature values and then evaluate the 
R
2
coefficient using the score() function from the LinearRegression estimator.

Now let’s add the weekday variable in its current format and see whether this improves the performance of our model.

# Create X/y data
X = data_df[["temp", "weekday"]].values
y = data_df.casual.values

# Fit a linear regression
lr = LinearRegression()
lr.fit(X, y)
lr.score(X, y)
0.2988588081409286
We can see that the two 
R
2
 coefficients are very close. So the weekday variable does not add any extra predictive value. Thus it isn’t really useful in its current format. However, there is a more appropriate way to encode this variable. Let’s find out.

One-hot encoding
The idea of one-hot encoding, or OHE for short, is to create a boolean variable for each possible value of a categorical feature. In our example, we will create a boolean variable for each day of the week which indicates whether the measurement was made on that day or not. It’s called one-hot encoding because only one variable from the seven can be 1, for True`, at a time. These indicators are also called dummy variables.

Pandas implements a get_dummies() function to create these dummy variables. When we pass a DataFrame to get_dummies() the function automatically checks for categorical columns by checking for columns whose dtype is object, string, or category. These columns will then be one-hot encoded.

Since our weekday column is encoded as integers we need use the columns parameter to inform get_dummies() that we want this specific column to be one-hot encoded. If we didn’t specify this, get_dummies() would return the original data frame as it found no categorical features.

Let’s one-hot encode the weekday and display the first five samples.

pd.get_dummies(data_df, columns=["weekday"]).head()

As you can see, the weekday variable is replaced with seven dummy features. Their names are given by the categorical feature name and the feature value that gets tested. Observe that our numerical values are sorted in their mathematical order, even if the first entry was a 6. If we had used the actual names of the days instead, the new columns would be sorted in alphabetical order, i.e. weekday_Friday, weekday_Monday, weekday_Saturday, weekday_Sunday, … .

Now let’s see whether this encoding makes any difference to our linear regression model.

# One-hot encoding
encoded_df = pd.get_dummies(data_df, columns=["weekday"])

# Create X/y data
X = encoded_df.drop(["casual"], axis=1).values
y = encoded_df.casual.values

# Fit a linear regression
lr = LinearRegression()
lr.fit(X, y)
lr.score(X, y)
0.5969174988134782
As we can see, the 
R
2
 coefficient is now close to 0.6 which is much better than the two previous models.

Observations:

When we use a linear regression model each one-hot encoded column is assigned its own weight.
When the value of a feature is 0 we get no contribution to the overall total..
When the value of a feature is 1 we add the coefficient exactly once to the overall total.
Each weekday gets its own column and hence its own independent weight.
Since only one weekday is ever active, we only get one coefficient contributing each time.
Let’s take a quick look at the coefficients and the intercept term:

print("Coefficients:" ,lr.coef_)
print("Intercept", lr.intercept_)
Coefficients: [2134.19905796  516.43832665 -168.51677465 -309.64458291 -315.36024223
 -274.98886363  -94.88509307  646.95722984]
Intercept -210.42468433257784
According to the model the individual weekdays indeed have different effects on the final predictions. Note: We actually have a redundant column, since knowing six of the weekday_* columns fixes the value of the remaining one. We can address this with the drop_first attribute.

# One-hot encoding
encoded_df_drop = pd.get_dummies(data_df, columns=["weekday"], drop_first=True)
encoded_df_drop.head()

The first OHE column weekday_0 was removed.

Let’s verify that we didn’t loose in performance.

# Create X/y data
X = encoded_df_drop.drop(["casual"], axis=1).values
y = encoded_df_drop.casual.values

# Fit a linear regression
lr_drop = LinearRegression()
lr_drop.fit(X, y)
lr_drop.score(X, y)
0.5969174988134782
All good! Now let’s compare the coefficients and the intercept terms:

print("Coefficients all:" ,lr.coef_)
print("Coefficients drop:" ,lr_drop.coef_)
print("Intercept all", lr.intercept_)
print("Intercept drop", lr_drop.intercept_)
Coefficients all: [2134.19905796  516.43832665 -168.51677465 -309.64458291 -315.36024223
 -274.98886363  -94.88509307  646.95722984]
Coefficients drop: [2134.19905796 -684.9551013  -826.08290957 -831.79856888 -791.42719028
 -611.32341972  130.51890319]
Intercept all -210.42468433257784
Intercept drop 306.01364231846765
Apart form the first coefficient for temp, it looks like the remaining coefficients changed completely between the models. But let’s take a closer look at the differences of the six OHE columns common to both models and of the intercept terms.

lr_drop.intercept_ - lr.intercept_
516.4383266510455
lr_drop.coef_[-6:]-lr.coef_[-6:]
array([-516.43832665, -516.43832665, -516.43832665, -516.43832665,
       -516.43832665, -516.43832665])
So in the second model the intercept term has absorbed the coefficient of the missing column weekday_0. As a result the coefficients of the other six OHE columns had to adjust by exactly the same value in order to keep the overall predicted values the same.

Preparing new samples for predictions
When we get new data, e.g. for making predictions, we will need to prepare the data so that it matches the structure of the data on which we trained our model. So we must have the same columns to represent the same information. Moreover, the columns must be given in the same order, because when we use .values we create a numpy array that is no longer aware of the column names. Let’s see what problems we might encounter when using one-hot encoding again and how we need to address these problems.

Suppose we have four new samples for making predictions with our model.

pred_df = pd.DataFrame({'temp': [0.241, 0.257, 0.239, 0.262], 
                        'casual': [138, 162, 174, 295],
                        'weekday': [3, 2, 5, 9]})
pred_df

You might ask what happened here? We have built this example set to demonstrate two very common problems when using one-hot encoding across multiple data sets.

We have only taken a few samples so we will not encounter all possible options of weekday;
We have added a previously unseen categorical value: 9. In turns out the value 9 is used to indicate that the day is a national holiday, and our training data did not contain a national holiday.
So how do we deal with the above issues?

Let’s start by applying one-hot encoding on the weekday column again.

pred_df_encoded = pd.get_dummies(pred_df, columns=["weekday"])
pred_df_encoded.head()

We can see that the columns of pred_df_encoded do not match those of encoded_df. We only got 3 of the original 7 columns and a new column weekday_9 was added.

Our final training data did not contain a column weekday_9. Thus our model has never encountered this information during training and it never learned about the customer behaviour on such national holidays. Hence our final encoded prediction frame should not contain a new column weekday_9 either.

We need to find a way that

creates the missing 4 columns and fills them appropriately with data;
removes the unknown column weekday_9 as it does not feature in our trained model.
The reindex() method does that job for us. Alternatively we could use loc[].

pred_df_encoded.reindex(encoded_df.columns, axis=1)

We can see that weekday_9 has been successfully removed. The missing columns have been added correctly, but currently contain NaN. Now remember that in one-hot encoding each column indicates whether the sample was taken on that particular weekday. Since we know that they were taken on another weekday, the values should be 0 and we can use .fillna(0).

pred_df_encoded=pred_df_encoded.reindex(encoded_df.columns, axis=1).fillna(0)
pred_df_encoded

Observe for the final row, which had initially a weekday value of 9, the OHE columns are all 0 now.

Now we can finally get our predictions:

X_pred=pred_df_encoded.drop(["casual"], axis=1).values
y_true=pred_df_encoded["casual"].values

y_pred=lr.predict(X_pred)

print(y_true)
print(y_pred)
[138 162 174 295]
[-11.4429536   28.41989065 204.76379745 348.73546885]
What if we wanted to use our model lr_drop where we used drop_first=True?

A common mistake is to also use drop_first=True when handling a second data frame, here pred_df.

But this leads to problems. The easiest way to understand why, is to simply run the code with drop_first=True and see what happens.

pred_df_encoded_drop = pd.get_dummies(pred_df, columns=["weekday"], drop_first=True)
display(pred_df_encoded_drop)

pred_df_encoded_drop=pred_df_encoded_drop.reindex(encoded_df_drop.columns, axis=1).fillna(0)
display(pred_df_encoded_drop)

We can see that we have initially lost the column weekday_2. While the reindexing recovered this missing column it now no longer has the value 1 for samples taken on Wednesday. Instead the second row looks like the sample was taken on a Monday (weekday_0). So this approach unintentionally falsified some of our data.

The take-home lesson:
We must use drop_first=False for pred_df, otherwise we risk losing information. Nevertheless, we can still use encoded_df_drop.columns for the reindexing and work with the reduced number of OHE columns, as weekday_2 will not be altered by this procedure.

Note that in both cases the previously unseen value of 9 was encoded as a row of 0s. Hence the weekday will not make any contribution to the prediction of the model.

Sklearn’s OneHotEncoder
The sklearn library provides a transformer that can do the one-hot encoding for us. However, by default it one-hot encodes all features and returns large sparse matrices. In order to tailor the OHE with ease we need to introduce some other concepts like column transformers first. We will do this a little later when we discuss pipelines.

Summary
In this unit, we learned about one-hot encoding and saw how to encode categorical variables with the Pandas library. Note that one-hot encoding is considered as a preprocessing step and not as being part of the feature engineering process.

One-hot encoding automatically creates a column for each unique categorical value.
We have the option to remove one redundant column by setting drop_first=True.
For new data used for predictions we must use One-hot encoding as well, but must not use drop_first=True.
We may be missing some columns from the prediction frame or we may be creating completely new ones. We can ensure that the final encoding matches that the encoding the training data by using .reindex() with the columns of the training data frame.
In the next exercise, you will experiment with one-hot encoding and feature engineering using an extended version of the bike sharing dataset with categorical features.
02. Overfitting and Occam's razor
Content
In the last subject, we discussed model complexity and the ability to generalize from data. We saw two cases.

Underfitting - the model is too simple and fails to fit the data/signal
Overfitting - the model is too complex and fits the noise in addition to the signal
In this subject, we will see how to control overfitting using regularization. But first, let’s talk about Occam’s razor which is the basic idea behind it, but also an interesting principle in general.

Occam’s razor
Occam’s razor is a principle which states that if multiple solutions are available, the simplest one is better than the others. The idea is that it’s easy to build overly complicated solutions with ad-hoc rules that don’t generalize well.

In the context of machine learning, the principle says that we should prefer simpler models unless we are sure that the complex ones are necessary.

We often say that generalization is the central goal of machine learning. Occam’s razor is one of the important principles to achieve this. You can take a look at section 3 and 4 of the paper “A few useful things to know about machine learning” by Pedro Domingos to learn more about the intuition behind generalization. Here is the link to the google scholar page.

Increasing the amount of data
The amount of data also plays a role in the under-/overfitting balance. Let’s do a quick experiment. In this image, we show two polynomial regressions of degree 9 fitted to 10 and 80 data points from the same source of data.


In the first case, the model is strongly overfitting. In fact, the polynomial passes through each data point. The problem is lessened in the second case.

Summary
In this unit, we learned about Occam’s razor which is an important principle in machine learning. In the next unit, we will learn about regularization which is an efficient way to reduce overfitting.
03. Regularization
Content
Questions 4
In practice, we use regularization to fight overfitting and to improve the generalizability of a model. By regularization we opt for models that are less complex, because more complex model do not generalize well on the unseen data even though they may provide a good fit on the training data. In this unit, we will see the basic idea behind it. We will then implement regularization with Scikit-learn in the next unit.

L2 regularization
When fitting a model, we are searching for a set of optimal parameters 
⃗
w
 that minimize the loss function 
L
(
⃗
w
)
.

min
⃗
w
 
L
(
⃗
w
)
As you can see, there are no constraints on the parameters 
⃗
w
. In particular, they can get very large as long as they minimize the loss function. However, large coefficients is one of the symptoms of overfitting. With large coefficients, a small variation in the input data has a big effect on the predictions.

Regularization can take different forms depending on the model. One method is to regularize the estimated coefficients by posing a constraint on the value of the parameters. In practice, we include a penalization term in the cost function that measures how large the parameters are. For instance, 
L
2
 regularization measures the squares of the parameters 
w
i
.

min
⃗
w
 
(
L
(
⃗
w
)
+
p
∑
i
=
1
 
w
2
i
)
Note that the sum doesn’t include the intercept term 
w
0
. Its role is to shift the predictions and hence, it doesn’t affect model complexity.

Here are two vectors of parameters for a linear regression with three features.

→
w
1
=
⎛
⎜
⎝
1
0
2
⎞
⎟
⎠
→
w
2
=
⎛
⎜
⎝
2
−
4
0
⎞
⎟
⎠
Which one is better according to the formula from above if the loss values are 
L
(
→
w
1
)
=
15
and 
L
(
→
w
2
)
=
5
. Note that the intercept terms are respectively 
4.5
 and 
3.75

Perfect!
Yes. Despite the large loss value, the first vector is better than the second one when adding the 
L
2
 term. The value of 
L
 plus the sum of 
w
2
i
 is

For the first vector: 
15
+
(
1
2
+
0
2
+
2
2
)
=
20
For the second vector: 
5
+
(
2
2
+
(
−
4
)
2
+
0
2
)
=
25


w
1

w
2
In practice, the formula involves a regularization strength parameter alpha 
α
 (or lambda 
λ
in some sources) that multiplies the 
L
2
 term.

min
⃗
w
 
(
L
(
⃗
w
)
+
α
p
∑
i
=
1
 
w
2
i
)
When 
α
 tends toward zero, the constraint on the parameters vanishes, and the problem is the same as before. When 
α
 tends toward infinity, the 
L
(
⃗
w
)
 term becomes irrelevant compared to the 
L
2
 one. In this case, all parameters are zero except the intercept term 
w
0
.

You can take a look at this article from the Scikit-learn documentation which shows the effect of the regularization strength 
α
 on the model coefficients.

Geometrical interpretation
The 
L
2
 regularization has a nice geometrical interpretation. To see it, we need to rewrite the formula from above. First, we can simplify it using the 
L
2
 norm 
∥
⃗
w
∥
, also known as the Euclidean norm, which is a way to measure the length of the vector 
⃗
w
.

min
⃗
w
 
(
L
(
⃗
w
)
+
α
∥
⃗
w
∥
2
)
As you can see, we replaced the sum by the square of the norm. In other words, the 
L
2
regularization penalizes long vectors 
⃗
w
. This optimization problem can be seen as a constrained optimization one using Lagrange multipliers.

min
⃗
w
 
L
(
⃗
w
)
s.t.
∥
⃗
w
∥
≤
c
We are searching for the vector 
⃗
w
 that minimizes the loss value 
L
(
⃗
w
)
 such that (
s.t.
) the length of the vector is smaller than some constant 
c
.

In practice, we never use this formulation to fit our models. However, it’s useful because it provides a nice geometrical interpretation - For a model with two parameters 
w
1
 and 
w
2
, we are searching for a point inside a circle of radius 
c
. In three dimensions, we are searching for a set of values inside a sphere of radius 
c
. Here is an illustration of the two-dimensional case.


Adapted from Bishop, C. Pattern Recognition and Machine Learning Figure 3.4

In this image, the blue point outside the circle represents the parameters 
w
1
 and 
w
2
 that minimize the unconstrained loss value 
L
(
⃗
w
)
. This minimal value is not inside the circle of radius 
c
. Hence it’s not a valid solution according to the constraint. The solution that minimizes the cost function inside the gray circle is denoted 
w
∗
 (
w
 star).

Other regularizers
We will use 
L
2
 regularization in this course, but there are other regularizers. For instance, the lasso regularization 
L
1
 is a variant of 
L
2
 which penalizes the absolute value of the coefficients instead of their squares.

min
⃗
w
 
L
(
⃗
w
)
s.t.
p
∑
i
=
1
 
|
w
i
|
≤
c
Just like 
L
2
 regularization, lasso has a nice geometrical interpretation.


Adapted from Bishop, C. Pattern Recognition and Machine Learning Figure 3.4

It’s interesting to see that the rhombus that corresponds to the constraint and the level curves of the loss function intersect on the 
w
2
 axis. If you move around the level curves of the loss function 
L
(
⃗
w
)
, you should see that the optimal solution is often on one of the two axes.

In other words, with 
L
1
 regularization, the optimal solution only has a few non-zero parameters, and we say that the solution is sparse which is a desired property in some cases. You can take a look at this thread if you want to learn more about this topic.

Summary
Let’s summarize what we’ve learned in this unit. Here are a few takeaways.

The idea behind regularization is to add a constraint on the amplitude of the coefficients.
This constraint corresponds to an additional term in the cost function called the penalization term.
We use an alpha 
α
 parameter to control the regularization strength.
In the next unit, we will implement 
L
2
 regularization for linear regressions.

04. Ridge regression
Content
Resources 1
Questions 2
In the last unit, we learned about 
L
2
 regularization and saw that it adds a constraint on the length of the vector of parameters 
⃗
w
. So far, we didn’t specify any particular model or cost function, but if we use multi-linear regressions and minimize the squares of the residuals, we obtain the ridge regression model.

min
⃗
w
 
(
n
∑
i
=
1
 
(
y
i
−
^
y
i
)
2
+
α
p
∑
j
=
1
 
w
2
j
)
In this unit, we will implement ridge regressions with Scikit-learn and test our model on a set of 50 noisy observations of a sine curve.

Sine curve data set
Let’s start by loading the training data set.

import pandas as pd

# Load the training data
training_data = pd.read_csv("c3_data-points.csv")

# Print shape
print("Shape:", training_data.shape)
Shape: (50, 2)
# First five rows
training_data.head()

We generated the 50 x/y data points using a sine curve. The idea is to create 50 points on the curve and add a small noise to each point. Let’s create the x, y variables and plot the data. Note that in this unit we work only with the training set and for simplicity we use the names x, y instead of x_tr, y_tr.

%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

# Extract x, y data
x = training_data.x.values
y = training_data.y.values

# Plot data points
plt.scatter(x, y)

# Plot sine curve for reference
x_values = np.linspace(min(x), max(x), num=100)
y_sine = np.sin(x_values)
plt.plot(x_values, y_sine, c="C3", label="sine curve")
plt.legend()
plt.show()

In this unit, we will try to recover this sine curve from the 50 noisy observations of it.

Polynomial regression
Let’s fit a polynomial regression of degree 10 to this set of points. We can generate the polynomial features using the PolynomialFeatures object from Scikit-learn.

from sklearn.preprocessing import PolynomialFeatures

# Create the polynomial features
poly_obj = PolynomialFeatures(degree=10, include_bias=False)
X_poly = poly_obj.fit_transform(x[:, np.newaxis])

print("Shape:", X_poly.shape)
print("Features:", poly_obj.get_feature_names_out())
Shape: (50, 10)
Features: ['x0' 'x0^2' 'x0^3' 'x0^4' 'x0^5' 'x0^6' 'x0^7' 'x0^8' 'x0^9' 'x0^10']
Later in this unit, we will apply regularization to our models. Since regularization is sensitive to the scale of our features, we will start by standardizing our data.

This is a common operation in machine learning, so Scikit-learn implements a StandardScaler() object to do it. Let’s see how to use to it.

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_rescaled = scaler.fit_transform(X_poly)
In this code, we first have to create the object and then call its fit_transform() function to compute the mean/standard deviation of each feature (fit) and apply standardization (transform).

Important note: The scaler object has also a transform() function which does not compute the mean/standard deviation of a feature but only applies the transformation. However, since here we are working with the training data we should use fit_transform() function. Later in this course we will see that in order to only apply the standardization on the validation and testing data we should use the transform() function. This is because we shouldn’t learn the mean/standard deviation from the validation and testing data.

Let’s verify that the data is standardized. The mean should be close to zero.

X_rescaled.mean(axis=0)
array([ 1.77635684e-16,  2.66453526e-16, -1.68753900e-16, -8.88178420e-18,
       -7.99360578e-17,  8.88178420e-18, -1.77635684e-17, -1.77635684e-17,
        4.44089210e-17,  1.06581410e-16])
And the standard deviation close to one.

X_rescaled.std(axis=0)
array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
Let’s now fit a linear regression to the standardize data and plot its prediction curve.

from sklearn.linear_model import LinearRegression

# Linear regression
lr = LinearRegression()
lr.fit(X_rescaled, y)

# Pass sample x values through the preprocessing steps
X_values_rescaled = scaler.transform(poly_obj.transform(x_values[:, np.newaxis]))
y_values_lr = lr.predict(X_values_rescaled)
Note that we need to apply all the preprocessing steps to the hundred x_values used to plot the prediction curve: we first create the polynomial basis with poly_obj.transform() and then standardize the features with scaler.transform() to get the X_values_rescaled. We can then pass the results to our model to get the predictions y_values_lr.

Here is the result.

# Plot the model
plt.scatter(X_rescaled[:, 0], y)
plt.plot(X_values_rescaled[:, 0], y_values_lr, c="C3", label="linear regression")
plt.legend()
plt.show()

As we can see, the model is clearly overfitting. Let’s try to reduce overfitting with regularization

Ridge regression
Scikit-learn implements ridge regressions with its Ridge object from the linear_model module. This object is an estimator which means that it implements the usual fit(), predict() and score() functions.

from sklearn.linear_model import Ridge

# Ridge regression
ridge = Ridge()
ridge.fit(X_rescaled, y)

# Compute predictions
y_values_ridge = ridge.predict(X_values_rescaled)
In this code, we replaced the LinearRegression object with a Ridge one. The rest of the code is the same. We get the following curve.

# Plot the model
plt.scatter(X_rescaled[:, 0], y)
plt.plot(X_values_rescaled[:, 0], y_values_ridge, c="C3", label="ridge regression")
plt.legend()
plt.show()

By default, the Ridge object sets the regularization strength 
α
 to one. In our case, the curve is too rigid to model the data points, and the model is underfitting.

We can decrease the constraint on the coefficients by setting a lower alpha value.

# Ridge regression
ridge2 = Ridge(alpha=1e-4)
ridge2.fit(X_rescaled, y)

# Compute predictions
y_values_ridge2 = ridge2.predict(X_values_rescaled)
In this code, we set alpha to 
10
−
4
 (1e-4 in e-notation). We will see how to tune the regularization strength and compute this alpha value in the next unit.

We get the following curve.

# Plot the model
plt.scatter(X_rescaled[:, 0], y)
plt.plot(X_values_rescaled[:, 0], y_values_ridge2, c="C3", label="tuned ridge")
plt.legend()
plt.show()

We can also compare the coefficients before and after adding the regularization term using the coef_ attribute.

# Linear regression coefficients
features = poly_obj.get_feature_names_out()
for feature, coef in zip(features, lr.coef_):
    print("{:<6}: {:>10.1f}".format(feature, coef))
x0    :        6.3
x0^2  :     -156.5
x0^3  :     1604.8
x0^4  :    -8250.8
x0^5  :    24145.5
x0^6  :   -42951.3
x0^7  :    47245.0
x0^8  :   -31285.6
x0^9  :    11373.1
x0^10 :    -1731.0
In this code, we pair the features with their coefficient using the Python zip() built-in function. Here are the results for our ridge2 estimator.

# Ridge regression coefficients
for feature, coef in zip(features, ridge2.coef_):
    print("{:<6}: {:>4.1f}".format(feature, coef))
x0    :  1.4
x0^2  : -1.6
x0^3  : -6.8
x0^4  :  3.9
x0^5  :  4.5
x0^6  :  0.5
x0^7  : -1.7
x0^8  : -1.1
x0^9  :  0.2
x0^10 :  0.1
As we can see, the coefficients are much smaller with ridge regression than with linear regression.

Using the analogy from the previous unit, the coefficients of the linear regression model minimize the loss function 
L
(
⃗
w
)
 in blue. On the other hand, the coefficients of the ridge regression model correspond to the optimal solution 
w
∗
 inside the circle of radius 
c
.


Adapted from Bishop, C. Pattern Recognition and Machine Learning Figure 3.4

Lasso regressions
One of the advantages of using Scikit-learn is that it’s easy to test different estimators. For instance, we can change the regularization to 
L
1
 using the Lasso estimator.

from sklearn.linear_model import Lasso

# Lasso regression
lasso = Lasso(alpha=1e-4, max_iter=1e5)
lasso.fit(X_rescaled, y)

# Compute predictions
y_values_lasso = lasso.predict(X_values_rescaled)
In this code, we simply replaced the Ridge object by a Lasso one. Scikit-learn uses an iterative algorithm called coordinate descent to find the optimal parameters of the Lasso regression. For this reason, we need to set the number of iterations with max_iter. We will simply set it to 100k iterations.

We get the following result.

# Plot the model
plt.scatter(X_rescaled[:, 0], y)
plt.plot(X_values_rescaled[:, 0], y_values_lasso, c="C3", label="lasso")
plt.legend()
plt.show()

The curve is very similar to the ridge one. However, the coefficients are different. In the last unit, we saw that 
L
1
 regularization produces sparse solutions which means that only a few parameters are non-zero. We can verify that by printing the coefficients of the lasso estimator.

# Lasso regression coefficients
for feature, coef in zip(features, lasso.coef_):
    print("{:<6}: {:>4.1f}".format(feature, coef))
x0    :  1.8
x0^2  : -4.4
x0^3  : -0.2
x0^4  :  0.0
x0^5  :  2.2
x0^6  :  0.9
x0^7  :  0.0
x0^8  :  0.0
x0^9  : -0.0
x0^10 : -0.7
In this case, only six coefficients are non-zero. This is why we say that Lasso performs feature selection - it automatically selects a subset of the features to make predictions and sets the others coefficients to zero.

Summary
In this unit, we applied regularization to reduce overfitting of a polynomial regression model. However, it can help in many other situations. For instance, we saw in a previous unit that we can use it when there is ill-conditioning.

In the next unit, we will see how to tune the regularization term 
α
 using a simple algorithm called grid search.

05. Tuning hyperparameters with grid search
Content
Resources 1
Questions 8
Remember the 
L
2
 regularization of ridge regression from the previous unit:

min
⃗
w
  
n
∑
i
=
1
 
(
y
i
−
^
y
i
)
2
+
α
p
∑
j
=
1
 
w
2
j
It adds a new parameter - the regularization strength 
α
. Unlike model coefficients which are learned during training, we set its value before fitting the model. For this reason, it’s called a hyperparameter. In order to assess and validate the choice of a value for the hyperparameter we can hold-out part of the training set. For instance, for a certain value of the hyperparameter we can train the model on 80% of the training set and evaluate the model performance on the remaining 20%. For this reason, the hold-out training data is called validation data. Once the best hyperparameter is selected, we train the model on the entire dataset (training and validation) and test the model on the unseen data. The image below shows the training, validation and testing data:


Note that we use the terms coefficients and parameters interchangeably. They both refer to the same thing, i.e. 
w
1
,
w
2
,
⋯
,
w
p
 in the following model. Intercept is also a parameter and can be named as 
w
0
, but we kept it here aside for the reason you will see later on in this unit.

^
y
=
i
n
t
e
r
c
e
p
t
+
w
1
x
1
+
w
2
x
2
+
w
3
x
3
+
⋯
+
w
p
x
p
In this unit, we will see how to tune the hyperparameter using a simple technique called grid search. This time, we will work with the house prices data set.

The hyperparameter is set by the user before training the model and it can be tuned using validation data, whereas the parameters are internal to the model and learned during training.

House prices data set
Let’s start by loading the data.

import pandas as pd

# Load the data
data_df = pd.read_csv("c3_house-prices.csv")
data_df.head()

The data contains information about different house sales including the size of the house (number of rooms, lot size), the sale type/condition and the price.

Let’s start by looking a the price.

%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

fig, axs = plt.subplots(1, 2, figsize=(10, 4))

# Plot the histogram of SalePrice
data_df.hist(column="SalePrice", bins=20, grid=False, xrot=45, ax=axs[0])
axs[0].set_title("SalePrice Distribution")

# Plot the histogram of log10(SalePrice)
axs[1].hist(np.log10(data_df.SalePrice), bins=20)
axs[1].set_title("log10(SalePrice) Distribution")

plt.tight_layout()
plt.show()

Most of the houses in the data set have a price between 100 and 300 thousand dollars (left plot). However, there are a few expensive houses with prices well above that. In practice, this can lead to biased models that favor accurate predictions of expensive houses, i.e., a 10% error on a 500 thousand dollars house is equivalent to a 20% error on a 250 thousand dollars one. To avoid building a model that is biased toward more expensive houses, we will apply the logarithm transformation and build a model that predicts the log of the sale price (right plot).

Preprocessing the data
As often, the raw data isn’t ready for our machine learning algorithms. In the final project, you will work on this house prices data set and apply all the necessary data cleaning. However, for this unit, we will apply the minimal steps: encoding non-numerical entries with one-hot encoding, and replacing missing entries with the median value.

Let’s create a preprocess(df) function to perform those steps.

def preprocess(df):
    # Work on a copy
    df = df.copy()

    # One-hot encoding
    df = pd.get_dummies(df, dummy_na=True)

    # Fill missing values
    for c in df.columns:
        df[c] = df[c].fillna(df[c].median())

    return df


preprocessed_df = preprocess(data_df)
preprocessed_df.head()

As we can see, our preprocessed_df DataFrame contains all the numerical columns on the left and created the necessary one-hot encoded columns for non-numerical variables. The total number of feature is now 347.

Note that the goal of this unit is to showcase the grid search and cross-validation techniques. The preprocessing steps and feature engineering are kept simple as they are not the focus of this unit.

Before applying the model, we still need to create the X/y variables and split the data into training/validation sets.

from sklearn.model_selection import train_test_split

# Create X, y
X = preprocessed_df.drop("SalePrice", axis=1).values
y = np.log10(preprocessed_df.SalePrice).values

# Split into train/validation sets
X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.5, random_state=0)

print("Train:", X_tr.shape, y_tr.shape)
print("Validation:", X_val.shape, y_val.shape)
Train: (1215, 347) (1215,)
Validation: (1215, 347) (1215,)
The features have a different scale, so we also need to standardize them before applying ridge regression in the next part.

from sklearn.preprocessing import StandardScaler

# Standardize features
scaler = StandardScaler()
X_tr_rescaled = scaler.fit_transform(X_tr)
X_val_rescaled = scaler.transform(X_val)
We are now ready to apply ridge regression to the data. But it’s worth to pause for a moment and check the effect of scaling on the features. Let’s plot the distribution of some features before and after scaling.

from scipy.stats import skew, kurtosis

# Extract the Lot Frontage and Lot Area features before and after scaling
lot_frontage = X_tr[:, 3]
lot_area = X_tr[:, 4]

lot_frontage_scaled = X_tr_rescaled[:, 3]
lot_area_scaled = X_tr_rescaled[:, 4]

# Create subplots
fig, axs = plt.subplots(2, 2, figsize=(12, 10), gridspec_kw={'hspace': 0.6,})

# Plot the distribution of Lot Frontage before scaling
axs[0, 0].hist(lot_frontage, bins=20, color='blue', )
axs[0, 0].set_title(f'Lot Frontage (before scaling)\nMean: {lot_frontage.mean():.0f}, Std: {lot_frontage.std():.0f}\nSkew: {skew(lot_frontage):.1f}, Kurt: {kurtosis(lot_frontage):.1f}')
axs[0, 0].set_xlabel('feet')
axs[0, 0].set_ylabel('Frequency')

# Plot the distribution of Lot Frontage after scaling
axs[0, 1].hist(lot_frontage_scaled, bins=20, color='blue', )
axs[0, 1].set_title(f'Lot Frontage (after scaling)\nMean: {lot_frontage_scaled.mean():.2f}, Std: {lot_frontage_scaled.std():.2f}\nSkew: {skew(lot_frontage_scaled):.1f}, Kurt: {kurtosis(lot_frontage_scaled):.1f}')
axs[0, 1].set_xlabel('feet (scaled)')

# Plot the distribution of Lot Area before scaling
axs[1, 0].hist(lot_area, bins=20, color='green', )
axs[1, 0].set_title(f'Lot Area (before scaling)\nMean: {lot_area.mean():.0f}, Std: {lot_area.std():.0f}\nSkew: {skew(lot_area):.1f}, Kurt: {kurtosis(lot_area):.1f}')
axs[1, 0].set_xlabel('square feet')
axs[1, 0].set_ylabel('Frequency')

# Plot the distribution of Lot Area after scaling
axs[1, 1].hist(lot_area_scaled, bins=20, color='green', )
axs[1, 1].set_title(f'Lot Area (after scaling)\nMean: {lot_area_scaled.mean():.2f}, Std: {lot_area_scaled.std():.2f}\nSkew: {skew(lot_area_scaled):.1f}, Kurt: {kurtosis(lot_area_scaled):.1f}')
axs[1, 1].set_xlabel('square feet (scaled)')

plt.show()

Note that the distributions of the features remained the same after scaling. However, the ranges of the values shown on the 
x
-axes have changed. Statistically speaking, in the case of ‘Lot Frontage’ the location or central tendency of distribution has shifted from 69 to 0, and the scale or spread of the distribution has changed from 22 to 1. However, the shape of the distribution (measured by skewness and kurtosis) is unchanged.

Linear regression
Before using ridge regression, let’s do a quick test and fit a linear regression model.

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error as MSE

# Try with a linear regression
lr = LinearRegression()
lr.fit(X_tr_rescaled, y_tr)

print("Train MSE: {:.4f}".format(MSE(y_tr, lr.predict(X_tr_rescaled))))
print("Validation MSE: {:.4f}".format(MSE(y_val, lr.predict(X_val_rescaled))))
Train MSE: 0.0014
Validation MSE: 137964618994860949504.0000
As we can see, there is an issue with the model - the validation MSE score is extremely high. If we take a look at the model coefficients, we can see that they are extremely large.

plt.hist(lr.coef_, bins=30)
plt.xlabel("coefficients")
plt.show()

Note the 1e10 factor on the x-axis (!) The high validation score probably comes from numerical issues due to those large coefficients. Let’s try to fix the issue with regularization!

Grid search
This time, we will search for the best regularization strength using a technique called grid search. You can think of this technique as a simple exhaustive searching over a set of hyperparameters.

In our case, we only have one hyperparameter: the regularization strength. So we will simply iterate over a list of alpha values and pick the one with the best validation score. However, in the next course, we will see cases where we need to tune several hyperparameters. In this case, we will generate a list of values for each one and evaluate the different combinations i.e. searching over a grid of hyperparams. As you can guess, the number of combinations to evaluate can quickly get very large. You will experiment with this issue in the next course, but you can already read about it in the Appendix at the end of this unit.

Ridge regression
Let’s first create the list of alpha values and evaluate a ridge regression model for each one. Regularization strength affects the generalization error on a log-scale. So we will use np.logspace(a, b) to generate sample alpha values i.e. instead of np.linspace(a, b).

It’s common to start with a range between a=1e-4 and b=1e4 for this hyperparameter, but we will use a larger range this time to see the effect of regularization with very large/small values.

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error as MAE

# Variable to store the results
gs_results = []
coefficients = []
alphas = np.logspace(-8, 8, num=50)

# Grid search
for alpha in alphas:
    # Create and fit ridge regression
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_tr_rescaled, y_tr)
    coefficients.append(ridge.coef_)

    # Save model and its performance on train/validation sets
    gs_results.append(
        {
            "alpha": alpha,
            "train_mse": MSE(y_tr, ridge.predict(X_tr_rescaled)),
            "train_mae": MAE(10 ** y_tr, 10 ** ridge.predict(X_tr_rescaled)),
            "validation_mse": MSE(y_val, ridge.predict(X_val_rescaled)),
            "validation_mae": MAE(10 ** y_val, 10 ** ridge.predict(X_val_rescaled)),
        }
    )
Note that we raise the predictions to power 10 for the MAE metric since we want to get a score in dollars and not log-dollars - remember that our model predicts the logarithm of the sale prices base 10.

Finally, we collect the results in a DataFrame and print the first five rows.

# Convert results to DataFrame
gs_results = pd.DataFrame(gs_results)
gs_results.head()

Validation curve
A good way to visualize the results is by plotting the validation curve. For each alpha, we plot the train and validation errors to see which value minimizes the error.

# Plot the validation curves
plt.semilogx(gs_results["alpha"], gs_results["train_mae"], label="training error")
plt.semilogx(
    gs_results["alpha"], gs_results["validation_mae"], label="validation error"
)
plt.xlabel("$alpha$")
plt.ylabel("MAE")
plt.legend()
plt.show()

There are a few things to observe about this result.

The optimal range of alpha values seems to be around 10e3, where we get the smallest validation error.
The model starts overfitting when alpha is smaller than 10e2 as the training error becomes small but we create a larger gap to the validation error.
The model starts underfitting when alpha is larger than 10e4 as training and validation errors agree but the model performs worse than for other alpha values.
Let’s retrieve the alpha with the best generalization i.e. the one that minimizes the validation score.

# Get entry with the best validation MSE
best_result = gs_results.loc[gs_results.validation_mae.idxmin()]

# Print the details
print("Best alpha: {:.1e}".format(best_result.alpha))
print("Validation MSE: {:.4f}".format(best_result.validation_mse))
print("Validation MAE: {:,.0f}$".format(best_result.validation_mae))
Best alpha: 6.0e+02
Validation MSE: 0.0048
Validation MAE: 15,548$
As we can see, the best alpha is 6.2e+2 with an validation MSE score of 0.0048. We cannot really interpret this result, so we also retrieve the corresponding MAE score - as we can see, the predictions from this model are, on average, 15,548 dollars away from the true price!

We achieved our goal of training and tuning a model. The model learned the parameters 
⃗
w
that minimize the following loss function, and we found the hyperparameter 
α
 that minimizes the validation error.

n
∑
i
=
1
 
(
y
i
−
^
y
i
)
2
+
α
p
∑
j
=
1
 
w
2
j
But note that our choice of the hyperparameter influences the model’s choice of parameters. This is worth clarifying further. In fact, there are two very important and interesting points to discuss, and they are related to the interplay between the hyperparameter 
α
 and the model parameters 
⃗
w
.

Observation 1: Stronger regularization lead to smaller parameters.

Let’s see this in practice by plotting the model coefficients for different alpha values.

# plot the coefficients against alpha
plt.semilogx(alphas, coefficients, color='gray', alpha=0.4)
# Highlight the best alpha
plt.axvline(x=best_result.alpha, color='b', linestyle='--', label='best alpha')
# Add a horizontal line at 0
plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)

plt.xlabel("$alpha$")
plt.ylabel("Estimated parameters")
plt.title("impact of regularization strength on the parameters")
plt.legend()
plt.show()

This plot shows that the model coefficients decrease as the regularization strength increases: - On the left end of the plot, parameters are very large, making the model sensitive to the changes in the new input data. In this case, the model overfits or is said to have high variance. - On the right end of the plot, parameters are almost zero, making the model less sensitive or insensitive to the changes in the new input data. In this case, the model underfits or is said to have high bias. - The best model is the one that minimizes the validation error, which is the one that balances the trade-off between bias and variance.

Let’s also see how regularized coefficients are different from the non-regularized ones.

# use the best alpha to fit the model 
ridge = Ridge(alpha=best_result.alpha)
ridge.fit(X_tr_rescaled, y_tr)
Ridge(alpha=596.3623316594636)
# Create subplots
fig, axs = plt.subplots(1, 2, figsize=(8, 4), sharey=True)

# Plot histogram of simple linear regression coefficients
axs[0].hist(lr.coef_, bins=30)
axs[0].set_title('Simple Linear Regression Coefficients')
axs[0].set_xlabel('Coefficient Value')
axs[0].set_ylabel('Frequency')

# Plot histogram of ridge regression coefficients
axs[1].hist(ridge.coef_, bins=30)  # Use the coefficients corresponding to the best alpha
axs[1].set_title('Ridge Regression Coefficients')
axs[1].set_xlabel('Coefficient Value')

plt.tight_layout()
plt.show()


As expected, the coefficients are much smaller when using regularization. This is because the model is penalized for having large coefficients.

Observation 2: Very strong regularization leads to null parameters.

We just saw that the model coefficients approach zero as the regularization strength increases. However, if the regularization strength is too high, the model coefficients 
w
 will be near zero and only the intercept survives i.e. the model always predicts the intercept term.

^
y
=
i
n
t
e
r
c
e
p
t
+
w
1
x
1
+
w
2
x
2
+
w
3
x
3
+
⋯
+
w
p
x
p
→
^
y
≈
i
n
t
e
r
c
e
p
t
In fact, 
^
y
≈
i
n
t
e
r
c
e
p
t
 is a very naive model, it is not learning the relationship between the features 
x
 and the target 
y
.

Let’s see this in numbers.

# Fit a model with a very strong regularization
strong_ridge = Ridge(alpha=alphas.max()).fit(X_tr_rescaled, y_tr)

print("Largest coefficient is near zero: {:.1e}".format(np.max(np.abs(strong_ridge.coef_))))
print("Intercept: {:.2f}".format(strong_ridge.intercept_))
Largest coefficient is near zero: 1.8e-06
Intercept: 5.23
It’s interesting to observe that intercept is equal to the mean of the target - which is, as we saw in a previous unit the baseline model.

print("Mean target value: {:.2f}".format(np.mean(y_tr)))
Mean target value: 5.23
As a final word, it’s also interesting to see that the training and validation errors reach a plateau for very large alpha values. This is because the model shrinks to the intercept term which is not enough to capture the relationship between the features and the target. So errors become large and constant.

Introduction to cross-validation
In the code from above, we found the best alpha value for a single train/validation split. Since the train/validation scores are evaluated on a large set of data points this result shouldn’t be too far from the true optimal alpha value. However, it could be that the split of the data was particularly (un)favourable for our model and our score. How can we be sure that our results are stable and reliable and not just a fluke?

It’s often a good idea to test the robustness of our model by collecting the scores on different train/validation splits and calculating the average score. This is called cross-validation.

There are several different cross-validation variants. We will learn more about them in the next course, but for now let’s implement our own simple cross-validation strategy with a simple for loop.

# Fit/validate N models
gs_results = []
for run_idx in range(10):

    # Split into train/validation sets
    X_tr, X_val, y_tr, y_val = train_test_split(
        X, y, test_size=0.5, random_state=run_idx
    )

    # Standardize features
    X_tr_rescaled = scaler.fit_transform(X_tr)
    X_val_rescaled = scaler.transform(X_val)

    # Grid search
    for alpha in np.logspace(1, 4, num=20):
        # Create and fit ridge regression
        ridge = Ridge(alpha=alpha)
        ridge.fit(X_tr_rescaled, y_tr)

        # Save model and its performance on train/validation sets
        gs_results.append(
            {
                "model": ridge,
                "alpha": alpha,
                "run_idx": run_idx,
                "train_mse": MSE(y_tr, ridge.predict(X_tr_rescaled)),
                "train_mae": MAE(10 ** y_tr, 10 ** ridge.predict(X_tr_rescaled)),
                "validation_mse": MSE(y_val, ridge.predict(X_val_rescaled)),
                "validation_mae": MAE(10 ** y_val, 10 ** ridge.predict(X_val_rescaled)),
            }
        )
In this code, we perform grid search on 10 different train/validation splits by changing the random_state attribute. For each different split, we standardize the data using the train mean and standard deviations.

The gs_results now contains 10 times more entries than if we had used a single train/validation set split.

# Convert results to DataFrame
gs_results = pd.DataFrame(gs_results)
gs_results.head()

Let’s group the results by alpha value and compute the average scores.

# Group results by alpha value
gb_alpha = gs_results.groupby("alpha")

# Compute train/validation mean scores with std
mean_tr = gb_alpha.train_mae.mean()
mean_val = gb_alpha.validation_mae.mean()
std_tr = gb_alpha.train_mae.std()
std_val = gb_alpha.validation_mae.std()
alphas = mean_tr.index.values

# Get entry with the best mean validation MaE
best_alpha = mean_val.idxmin()
best_result = gb_alpha.get_group(best_alpha)

# Print the details
print("Best alpha: {:.1e}".format(best_alpha))
print("Validation MSE: {:.4f}".format(best_result.validation_mse.mean()))
print("Validation MAE: {:,.0f}$".format(best_result.validation_mae.mean()))
Best alpha: 2.6e+02
Validation MSE: 0.0039
Validation MAE: 15,401$
This time, the best alpha value is 2.6e+02 with a mean validation MSE of 0.0039 and a mean validation MAE of 15,401 dollars which is not far from the values computed above.

Note that we also computed the standard deviation for the different alpha values. This tells us how much the scores vary around the mean, i.e. how robust our model is. We can now include this information in the plot with the validation curves.

# Plot mean scores
plt.plot(np.log10(alphas), mean_tr, label="train")
plt.plot(np.log10(alphas), mean_val, label="validation")

# Quantify variance with ±std curves
plt.fill_between(np.log10(alphas), mean_tr - std_tr, mean_tr + std_tr, alpha=0.2)
plt.fill_between(np.log10(alphas), mean_val - std_val, mean_val + std_val, alpha=0.2)

# Add marker for best score
plt.scatter(np.log10(best_alpha), mean_val.min(), marker="x", c="red", zorder=10)

plt.title("Validation curves with {} runs".format(len(gs_results.groupby("run_idx"))))
plt.xlabel("$log_{10}(alpha)$")
plt.ylabel("MAE")
plt.legend()
plt.show()

We can see that for smaller alpha we have a bit more variation in the validation scores. So these models are a bit less stable.

Summary
In this unit, we saw the basic idea behind hyperparameters and grid search. Here are a few takeaways.

Ridge regression has a hyperparameter (
α
) and a vector of parameters or coefficients (
⃗
w
).
The hyperparameter is set by the user before training the model, whereas the parameters are internal to the model and learned during training.
We want to find hyperparameters that minimize the generalization error.
Grid search is an exhaustive search over the set of all possible combinations of hyperparameters.
Plotting the train/validation curves is a way to analyze and validate our models.
The goal of the next exercise is to improve the house prices from above by doing some basic feature engineering.

Appendix - Note about complexity
The main issue with grid search is that it doesn’t scale well to many hyperparameters. In fact, each time we add a parameter with 
n
 values, we need to test 
n
 times more models than before.

For instance, say that we want to tune three hyperparameters. Each one with two values

hyperparam_1 = [1,2]
hyperparam_2 = [3,4]
hyperparam_3 = [5,6]
Hence, we have to test 
2
3
=
8
 combinations in total.

combinations = [
    (1, 3, 5),
    (1, 3, 6),
    (1, 4, 5),
    (1, 4, 6),
    (2, 3, 5),
    (2, 3, 6),
    (2, 4, 5),
    (2, 4, 6),
]
In general, we need to test 
n
p
 combinations when there are 
p
 parameters with 
n
 values each. Hence, it’s better to have many values but few parameters than the opposite. For instance, it’s better to have two hyperparameters with 
n
 values each than 
n
 hyperparameters with two values each. Here is a comparison of the two cases.


For 
n
=
10
, we need to validate 100 combinations in the first case but more than a thousand in the second one.

There are several more advanced strategies such as randomized search that try to solve this issue and reduce the time needed to find the optimal combination of hyperparameters. We will learn more about this technique in the next course when tuning models with several hyperparameters.

Appendix - Note about grid search range and border effects
The goal of a grid search is to find the optimal hyperparameter. But because of the complexity issue mentioned above, the grid search space needs to be restricted to a certain number of grid points, between a particular grid search range. For example, in the Ridge Regression example above, we specified the grid search range of the alpha value to be between 10 and 10’000 (e.g. np.logspace(1, 4, num=20).

In the example above this approach works well. The optimal alpha seems to lay somewhere around 5.5e+02. We either can now be happy with this result, or we could restrict the grid search further and fine tune the hyperparameters even more. In the Ridge Regression example this does not seem to be necessary.

But let’s imagine the optimal alpha would have been found at 1, i.e. at the border of our grid search range. In this case it is not clear if 1 is the best alpha value or if we could minimize the error even more, if we would move the grid search range further to the left, i.e. below 1.

For a visual example of this thematic, let’s imagine we want to fine tune the hyperparameters for a support vector machine (SVM) with an RBF kernel. We will explain this classifier in more detail in a later unit, for now you just need to know that this classifier has two hyperparameters that we need to fine tune: C and gamma

Let’s assume we go ahead and run a grid search for gamma between 0.1 to 1 and for C between 100 to 1000 and we fine tune according the best classification accuracy. Looking at the following figure we could say that the optimal classification accuracy can be reached with gamma=0.3 and C=100. But as just mentioned above, this grid search point is at the border of our search range. By increasing the range we might be able to further improve our classification accuracy.


So let’s extend our grid search range to a value between 0.001 and 100 for gamma and to a value between 0.01 to 1000 for C. Performing another grid search with this far wider range reveals that the optimal value is much likely closer to gamma=0.82 and C=8.21.

Note: The white box in the figure below represents the grid search range from the previous figure and the marked x represents the new optimal grid point.

08. Post-modeling analysis for regressors
Content
Resources 1
Questions 1
As discussed in earlier units, the score on the test set gives you an estimate of the generalization error. In previous units, we encountered some common metrics for regression models such as RMSE, MAE, or R
2
. However, these metrics reduce the model performance to a single number, and thus do not provide much information on how we could improve the model.

In post-modeling analysis we examine the errors of our models, the difference between our predictions and true values, in order to get a better understanding of the strength and weaknesses of our models. This information can then give us ideas on how to improve our model.

Seaborn provides a number of smaller data sets that you can load via the command .load_dataset(). One of these is the penguins data set which contains data on 333 penguins.


We split off 100 random samples as a test set and trained 5 different regressor models on the remaining samples to predict the feature "body_mass_g". The true body masses, the predictions of our 5 models as well as the species labels are provided in a CSV file stored in the resources tab.

The 5 models we used were linear regression, Huber regressor, decision tree, random forest, and a k-nearest neighbors model. You will learn more about the last three models in course 4. For now, we only care about their predictions. Finally, none of the models were fine-tuned as the purpose of this unit is not to find the best model but to analyze model performances and errors in more detail.

After loading the predictions and the packages, we will focus on how to analyze an individual model in greater detail. We will look at the distributions of the predictions and their errors and also compare them against the ground truth. In the second part, we will compare predictions from different models against each other in order to identify samples that are generally more difficult to predict.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df_pred = pd.read_csv("c3_model_predictions.csv")
df_pred.head()

The analysis of individual models
In this section, we look at different visual analyses of a single model. We will compare the predictions of the simple linear regression y_pred_lr against the true values y_true by looking at histograms and scatterplots. Afterward, we will take a look at the prediction errors.

Histograms and KDE plots
We plot histograms for both the true and the predicted target values in one plot. However, it can be hard to judge whether the differences in certain places are statistically significant or not. Therefore we add the kernel density estimates (KDE) to the plot. They allow us to compare the estimated distributions of the true and predicted target values.

# Filter data
y_true = df_pred.y_true
y_pred_lr = df_pred.y_pred_lr

# Setup bins of width 250g
y_min, y_max = y_true.min(), y_true.max()
bins = np.arange(y_min, y_max, 250)

# Plot histogram
plt.figure(figsize=(7, 5))
sns.histplot(data=df_pred.loc[:, ["y_true", "y_pred_lr"]], bins=bins, kde=True)

# Add title and labels
plt.title("Comparing predictions and ground truth", fontsize=14)
plt.xlabel("body mass")
plt.show()

We can see that the two KDE are very similar, indicating that the predictions are similarly distributed to the actual values.

If we don’t have the ground truth of our test data, but would still like to compare the distributions of our predictions to real data, then we can compare the predictions against the ground truth of our training data.

Scatterplot
Next, let’s plot our predictions against the ground truth in a scatterplot. We add the straight line 
y
=
x
 in red to highlight where the perfect predictions would be located. This allows us to spot over or underestimations by the model. In this plot, we also colored the samples by "species" to check whether one species might be harder to predict than another.

# Plot scatter plot
sns.relplot(x=y_true, y=y_pred_lr, hue=df_pred.species, height=5)

# Plot diagonal line
line_x = np.arange(y_min, y_max, 100)
plt.plot(line_x, line_x, c="r")

# Add title and labels
plt.title("Plotting predictions against ground truth", fontsize=14)
plt.xlabel("true body mass")
plt.ylabel("predicted body mass")
plt.show()

The red line indicates when the true body mass and the predicted body mass are equal, i.e. it is the line of the perfect predictions. If points are above (below) the line, then our model overestimated (underestimated) the body mass. We are looking for a general trend here.
We can see that our model equally over and underestimates the body mass for light as well as heavy birds. This is the case for all three species. Thus there is nothing particularly worrying in this plot.


Above we see 4 plots for 4 potential scenarios that we might have encountered instead:

The model generally underestimates the target, as most points are below the line.
The model generally overestimates the target, as most points are above the line.
The model underestimates lighter birds and overestimates heavier birds (or vice versa).
The prediction errors become larger (smaller) as the body weight increases.
For linear models, the first three scenarios might have arisen due to outliers in the training data. The final scenario might arise if target values are skewed, and hence the target would benefit from a transformation before training models.

Analyzing the errors
A common assumption is that the measured values are a combination of the theoretical values and some noise and that this noise is approximately normally distributed. So let’s calculate the errors of our predictions and check their distribution.

# Calculate errors
errors = y_pred_lr - y_true

# Plot histogram with bins of 100g
sns.histplot(x=errors, bins=np.arange(-700, 700, 100), kde=True)
plt.title("Analyzing the distribution of the errors", fontsize=14)
plt.xlabel("Error size")
plt.show()

The KDE may not look like a picture-perfect Gaussian, but it is fairly symmetric around zero and exhibits no skewness. We should also keep in mind that we are only dealing with 100 samples here.

However, the histogram can not tell us whether errors occur randomly across all body masses. We can change our perspective by plotting the errors directly against the true target.

# Plot scatter plot
sns.relplot(x=y_true, y=errors, height=5)

# For horizontal line
y_min, y_max = y_true.min(), y_true.max()
line_x = np.arange(y_min, y_max, 100)
plt.plot(line_x, 0 * line_x, c="gray")

# Add title and labels
plt.title("Plotting errors against their target", fontsize=14)
plt.xlabel("True body mass")
plt.ylabel("Prediction error")
plt.show()

There appears to be no obvious trend. Errors may have some outliers but otherwise appear randomly distributed against the target values.

In other scenarios, we might observe that the model has a tendency to underestimate samples at the lower end of the target values while it overestimates those at the higher end. Or the absolute errors might be generally larger for higher target values.

We have only highlighted a few approaches for analyzing the predictions and errors of our models. Any analysis that we used in the EDA of our original data may also be considered for the post-model analysis. For example, you could look for correlations between features and errors.

Outliers
While we might have excluded outliers from the training set, it was important to keep outliers in the test set in order to obtain a representative estimate of the generalization error. However, once we have got that estimate, we can still look for outliers in the test set and check whether their predictions appear to have particularly high errors compared to other samples. As we don’t have the original feature values we won’t pursue this direction here.

Comparison of multiple models against each other
Above we have seen how to investigate individual models in more detail. Now let’s look at comparing models against each other. We start with a simple pairplot and a closer look at the KDEs again. Then we will compare the predictions of all models on individual samples and identify which samples might have been hardest to predict. And finally, we will address the most interesting question: Which model performed best?

# Setup bins of width 250g
bins = np.arange(y_min, y_max, 250)
sns.pairplot(
    df_pred,
    diag_kws={"bins": bins, "kde": True},
)
plt.show()

At first glance, the KDEs of all our models appear to have similar shapes. Let’s investigate this more explicitly.

for c in df_pred.columns[df_pred.columns.str.contains("y_")]:
    sns.kdeplot(df_pred[c], label=c)
plt.legend()
plt.show()

Indeed, apart from the decision tree, all KDEs estimate the KDE of the true values pretty well.

Next, let’s look at the scatterplots in our pairplot. The predictions of the linear regression and the Huber regressor are in very good agreement. This is not surprising given the models are very similar. The random forest (y_pred_rf) and the k-nearest neighbors model (y_pred_knn) make quite similar predictions too, whereas the decision tree is the model that deviates most from the other models, as well as from the ground truth.

Comparing model predictions sample by sample
In order to see whether the same samples cause problems to all the models, let’s start by plotting all our model predictions grouped by samples. For this, we first order the predictions by the size of their true value. Otherwise, the graph would become messy and hard to interpret.

df_pred2 = df_pred.sort_values("y_true").reset_index(drop=False)
# We retain the original index so we can easily identify the original samples whenever necessary.
# Setup
plt.figure(figsize=(15, 8))
ind = df_pred.index
s = 10

# Plots
plt.scatter(ind, df_pred2.y_true, color="C0", marker="d", s=3 * s, label="True")
plt.scatter(ind, df_pred2.y_pred_lr, color="C1", s=s, label="Linear")
plt.scatter(ind, df_pred2.y_pred_hub, color="C2", s=s, label="Huber")
plt.scatter(ind, df_pred2.y_pred_dt, color="C3", s=s, label="DecisionTree")
plt.scatter(ind, df_pred2.y_pred_rf, color="C4", s=s, label="RandomForest")
plt.scatter(ind, df_pred2.y_pred_knn, color="C5", s=s, label="kNN")

# Add title, labels, etc.
plt.title("Regressor predictions and their average", fontsize=14)
plt.xlabel("test sample index")
plt.ylabel("predicted body mass")

plt.grid(True)
plt.xticks(ticks=np.arange(0, 101, 5))
plt.legend(loc="best")

plt.show()

Looking at the graph there are 5 groups of samples that may be of interest to us. Samples for which

all predictions are close to the ground truth and all models work fairly well for these samples
all predictions are far below the ground truth, i.e. all models underestimate for these samples
all predictions are far above the ground truth, i.e. all models overestimate for these samples
the predictions mostly agree
the predictions mostly disagree
We can also see that there are samples with very similar target values for which the predictions amongst these samples vary (samples 41-50 or 61-65)

The plot also reconfirms our earlier observation that the decision tree often gives the worst predictions and its predictions are often an outlier amongst the models. Hence we drop the decision tree for the next step and focus on the remaining 4 models that have a better performance.

Below we use the mean and the standard deviation of the predictions to identify which samples are related to the 5 groups outlined above.

# Selecting the 4 best models
df_best_4 = df_pred2.loc[
    :, ["y_pred_lr", "y_pred_hub", "y_pred_rf", "y_pred_knn"]
].copy()

# Calculating the prediction errors and calculating their stats per sample
df_best_4_errors = df_best_4.subtract(df_pred2.y_true, axis=0)
df_best_4_avg_errors = df_best_4_errors.mean(axis=1)
df_best_4_avg_errors.name = "avg error"
df_best_4_std_errors = df_best_4_errors.std(axis=1)
df_best_4_std_errors.name = "std error"
Now let’s have a look:

The samples with the best average prediction
np.abs(df_best_4_avg_errors).sort_values().to_frame().head()

Compared to the body masses (3000-6000g) these errors are almost negligible.

The samples with the strongest average underestimation
df_best_4_avg_errors.sort_values().to_frame().head()

The samples with the strongest average overestimation
df_best_4_avg_errors.sort_values(ascending=False).to_frame().head()

Using the fact that our indices are sorted by true weight, it appears overestimation may be more common for lighter birds, while underestimation might be more common for heavier birds.

The samples with the best agreement in their predictions
df_best_4_std_errors.sort_values().to_frame().head()

Careful: This tells us nothing about the accuracy of the 5 predictions. For that, we have to check the mean of the predictions. Or we can take a look at our plot.

The samples with the worst agreement in their predictions
df_best_4_std_errors.sort_values(ascending=False).to_frame().head()

As a next step, we could analyze what these samples have in common that makes their predictions so challenging.

Is it due to a particular feature?
Do the species play a role?
Are these samples outliers or errors in the test data?
…
The post-modeling analysis is a new cycle of EDA and we can use any of the tools we have encountered before to find ways of improving our models. But we won’t pursue this any further here.

And the winner is?
Well, first we need to pick a metric with which we compare our models’ performance. Let’s use the RMSE, calculate it for each model, and plot the scores in a bar plot.

from sklearn.metrics import mean_squared_error

# Get the column names
trained_models = list(df_pred)[1:-1]

# Calculate the RMSE per model
rmse_list = []
for model in trained_models:
    model_rmse = mean_squared_error(df_pred["y_true"], df_pred[model], squared=False)
    # Note squared=False returns RMSE instead of MSE
    rmse_list.append(np.round(model_rmse, 1))

# The plot
sns.barplot(x=trained_models, y=rmse_list, color="C0")
# Add the RMSE values as labels to each bar
for i in range(len(rmse_list)):
    plt.text(
        x=i,  # the horizontal position, i.e. the ith bar
        y=rmse_list[i] - 30,  # the vertical position
        s=rmse_list[i],  # the text
        color="white",
        horizontalalignment="center",
    )
plt.show()

So we have reconfirmed that the decision tree was indeed the worst model and that Linear Regression and Huber Regression perform very similarly. They ultimately also came out on top in terms of overall performance.
We should keep in mind that we have not tuned our models, so maybe the default values were unfavorable for this data set. There is only one way to find out…

Summary
In this unit, we have seen a few ways in which we can get more insight into our models’ performances. We can analyze the predictions and their errors within a single model or we can compare them across models to identify for which samples our models struggle most. Ultimately predictions are data in their own right and hence we can consider any data analysis tools encountered so far to dig deeper.

02. Pipelines
Content
Resources 1
Questions 4
So far, we used Scikit-learn for its ML estimators like the LinearRegression or Ridge ones and saw a few examples of transformers with the StandardScaler and PolynomialFeatures objects. In this unit and the next ones, we will see how to assemble Scikit-learn estimators and transformers into a full ML pipeline.

In this unit, we will see how to use the Pipeline object from Scikit-learn using the bike sharing data.

Encapsulating preprocessing steps
Let’s start by loading the data.

import pandas as pd

data_df = pd.read_csv("c3_bike-sharing-data.csv")
data_df.head()

As we can see, the data contains a few categorical variables. Let’s encode them with the get_dummies() function from Pandas and create the X and y train/test variables.

from sklearn.model_selection import train_test_split

# One-hot encoding
encoded_df = pd.get_dummies(data_df)

# Split into train/test sets
X = encoded_df.drop("casual", axis=1).values
y = data_df.casual.values
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)
The data is not ready yet! We are going to fit a ridge regression model, so we first need to standardize the data with a StandardScaler transformer.

from sklearn.preprocessing import StandardScaler

# Standardize data
scaler = StandardScaler()
X_tr_rescaled = scaler.fit_transform(X_tr)
X_te_rescaled = scaler.transform(X_te)
It’s important to note that we fit the scaler on the train data by calling the fit() operation on X_tr and then use the computed train mean and standard deviations (featurewise) to standardize both train/test sets with the transform() one.

Let’s now fit the ridge regression model and see how it performs.

from sklearn.metrics import mean_absolute_error as MAE
from sklearn.linear_model import Ridge

ridge = Ridge()
ridge.fit(X_tr_rescaled, y_tr)
print("MAE: {:.2f}".format(MAE(y_te, ridge.predict(X_te))))
MAE: 255890.12
Using this MAE metric, we can see that the predictions are, on average, 255,890 users away from the observed value! If we compare this result to the median baseline

import numpy as np

median_predictions = np.full_like(y_te, np.median(y_tr))
print("Median baseline: {:.2f}".format(MAE(y_te, median_predictions)))
Median baseline: 523.47
we can clearly see that there is an issue with our code! Can you find it?

The mistake isn’t with our model - this part is correct. In fact, we simply used the wrong variable when computing predictions. Instead of passing X_te_rescaled to our model, we passed the untransformed X_te test input data.

print("MAE: {:.2f}".format(MAE(y_te, ridge.predict(X_te_rescaled))))
MAE: 279.80
After fixing this issue, we get the correct predictions with an MAE of 280.

Applying different preprocessing steps to the train/test data is a common mistake which can be hard to detect when we have a long sequence of transformations. To solve this issue, we can use the Pipeline object to encapsulate them with the estimator. Let’s see how to use this object.

Pipeline object
To create a Scikit-learn pipeline, we simply need to list the sequence of preprocessing steps and the final model.

from sklearn.pipeline import Pipeline

# Create pipeline
pipe = Pipeline([("scaler", StandardScaler()), ("ridge", Ridge())])
The steps are simply (name, object) pairs. For each object, we can choose a name that we can then use later to refer to the step. In this case, we simply name the StandardScaler() step scaler and the Ridge() one ridge.

We can then retrieve the list of steps with the named_steps attribute.

# Get a dictionary with each step
pipe.named_steps
{'scaler': StandardScaler(), 'ridge': Ridge()}
The pipeline object can be used as a standard Scikit-learn estimator i.e. it implements the usual fit(), predict() and score() functions from the estimator API.

# Fit on the train set
pipe.fit(X_tr, y_tr)

# Evaluate on the test set
print("MAE: {:.2f}".format(MAE(y_te, pipe.predict(X_te))))
MAE: 279.80
Note that this time, we pass the unstandardized data to the pipeline which implicitly applies the scaler before ridge regression without creating intermediate variables that can lead to mistakes!

Multiple preprocessing steps
In the example above, we have a single preprocessing step. The pipe.fit() call is equivalent to applying the following handmade function

def pipe_fit(X, y):
    # Fit and apply the transformation
    X1 = scaler.fit_transform(X)

    # Fit the estimator
    ridge.fit(X1, y)


# Fit to the train data
pipe_fit(X_tr, y_tr)
Similarly, the pipe.predict() call above is equivalent to

def pipe_predict(X):
    # Apply the transformation
    X1 = scaler.transform(X)

    # Make predictions
    return ridge.predict(X1)


# Evaluate on the test set
print("MAE: {:.2f}".format(MAE(y_te, pipe_predict(X_te))))
MAE: 279.80
In practice, we sometimes need to encapsulate several transformers into a more complex pipeline.

# Pipeline with three transformations
pipe = Pipeline([
    ('transform1', ...),
    ('transform2', ...),
    ('transform3', ...),
    ('estimator', ...)
])
In such cases, Scikit-learn simply fits and applies each transformer one after the other before fitting the estimator to the result. To illustrate this, here is the equivalent function to pipe.fit()

# Example with three transformations
def pipe_fit(X, y):

    # Fit and apply the transformations
    X1 = transform1.fit_transform(X)
    X2 = transform2.fit_transform(X1)
    X3 = transform3.fit_transform(X2)

    # Fit the estimator
    estimator.fit(X3, y)
At prediction time, the pipeline would apply the transformations and call the predict() function with the result.

def pipe_predict(X, y):

    # Apply the transformations
    X1 = transform1.transform(X)
    X2 = transform2.transform(X1)
    X3 = transform3.transform(X2)

    # Make predictions
    return estimator.predict(X3)
Optional steps
It can be interesting to try fitting an estimator with and without a preprocessing step to see if it improves to results. We will see a few scenarios in the next course where this is helpful. To achieve this, we can simply set the step to None

# Create a pipeline
pipe = Pipeline([("scaler", None), ("ridge", Ridge())])  # Disable this step

# Fit pipeline to the train set
pipe.fit(X_tr, y_tr)

# Accuracy on the test set
print("MAE: {:.2f}".format(MAE(y_te, pipe.predict(X_te))))
MAE: 282.48
In this case, we cannot really conclude that we get worse results since we didn’t tune the regularization strength hyperparameter of our ridge regression model.

Grid search with pipelines
Let’s see how we would tune the alpha hyperparameter using grid search on our pipeline

import numpy as np

# Variable to store the results
gs_results = []

# Grid search
for alpha in np.logspace(-4, 4, num=100):
    # Create/fit the pipeline
    pipe = Pipeline([("scaler", StandardScaler()), ("ridge", Ridge(alpha))])
    pipe.fit(X_tr, y_tr)

    # Save model and its performance on train/test sets
    gs_results.append(
        {
            "alpha": alpha,
            "train_mae": MAE(y_tr, pipe.predict(X_tr)),
            "test_mae": MAE(y_te, pipe.predict(X_te)),
        }
    )

# Convert results to DataFrame
gs_results = pd.DataFrame(gs_results)
gs_results.head()

In this code, we simply create a new pipe object for each value and get the table above.

Let’s visualize the train/test scores by plotting the validation curves.

%matplotlib inline
import matplotlib.pyplot as plt

# Plot the validation curves
plt.semilogx(gs_results["alpha"], gs_results["train_mae"], label="train curve")
plt.semilogx(gs_results["alpha"], gs_results["test_mae"], label="test curve")
plt.xlabel("$alpha$")
plt.ylabel("MAE")
plt.legend()
plt.show()

The model is not overfitting: the test curve remains flat when releasing the regularization constraint i.e. alpha values close to zero. On the other hand, we can see that the performance quickly drops when increasing alpha. This corresponds to the underfitting case.

If we don’t want to create a new pipeline object in the for loop (ex. for readability), we can also use the get_params() and set_params() functions which are available in every Scikit-learn estimator including the Pipeline one.

# Create the pipeline
pipe = Pipeline([("scaler", StandardScaler()), ("ridge", Ridge())])
pipe.get_params()
{'memory': None,
 'steps': [('scaler', StandardScaler()), ('ridge', Ridge())],
 'verbose': False,
 'scaler': StandardScaler(),
 'ridge': Ridge(),
 'scaler__copy': True,
 'scaler__with_mean': True,
 'scaler__with_std': True,
 'ridge__alpha': 1.0,
 'ridge__copy_X': True,
 'ridge__fit_intercept': True,
 'ridge__max_iter': None,
 'ridge__normalize': 'deprecated',
 'ridge__positive': False,
 'ridge__random_state': None,
 'ridge__solver': 'auto',
 'ridge__tol': 0.001}
In this case, the dictionary of parameters from our pipe estimator contains all the steps with their hyperparameters prefixed by the name of the step and two underscores _. For instance, ridge__alpha corresponds to the regularization strength of the ridge step.

We can use the same syntax to set each hyperparameter value. For instance, let’s set the alpha one in our grid-search loop.

# Variable to store the results
gs_results = []

# Grid search
for alpha in np.logspace(-4, 4, num=100):
    # Fit the pipeline
    pipe.set_params(ridge__alpha=alpha)
    pipe.fit(X_tr, y_tr)

    # Save model and its performance on train/test sets
    gs_results.append(
        {
            "alpha": alpha,
            "train_mae": MAE(y_tr, pipe.predict(X_tr)),
            "test_mae": MAE(y_te, pipe.predict(X_te)),
        }
    )

# Convert results to DataFrame
gs_results = pd.DataFrame(gs_results)

# Plot the validation curves
plt.semilogx(gs_results["alpha"], gs_results["train_mae"], label="train curve")
plt.semilogx(gs_results["alpha"], gs_results["test_mae"], label="test curve")
plt.xlabel("$alpha$")
plt.ylabel("MAE")
plt.legend()
plt.show()

Both approaches are correct and produce the same results. However, the set_params() one can be useful to quickly test different hyperparameter values without having to define new pipeline objects.

Summary
In this unit, we saw how to encapsulate a set of preprocessing steps and an estimator into a single Pipeline object. It’s interesting to note that we included only the standardization step in the pipeline and not the one-hot encoding one which was performed outside Scikit-learn with the get_dummies() function from Pandas.

In the next unit, we will see how to encode categorical variables directly in Scikit-learn. To achieve this, we will use the ColumnTransformer object which is part of a new workflow that tries to consolidate the data manipulation steps, usually done in Pandas, with the modeling part from Scikit-learn.

03. Column transformations
Content
Resources 1
Questions 3
This unit is optional
TL;DR - Perform the data preparation, exploratory data analysis (EDA), feature encoding/engineering, outliers removal in Pandas before the ML modeling as we have seen so far in this course. When the data is in a good format and ready for the ML models, convert the DataFrame to a Numpy 2d float array with .values and pass it to the sklearn estimator .fit/predict/score methods or Pipeline object if a StandardScaler is needed.

Explanation: So far in this program, we have seen how to structure our data analysis into (1) data preparation (2) exploratory data analysis (EDA) and (3) machine learning parts. The first two steps (1) and (2) involve a lot of data manipulation and are done in Pandas and the last one (3) with Scikit-learn. Because ML models only work with numerical data, we usually convert our DataFrame into a Numpy float 2d array only at step (3) for sklearn estimators. It’s important to understand that the data preprocessing (1) and (2) are done in Pandas. The only exception is for common preprocessing steps that are very specific to ML such as StandardScaler or dimensionality reduction such as PCA (more about this in the next course) - those are usually encapsulated into a Pipeline object as shown in the last unit. The reason for this exception is simple: those ML operations are independent of the nature of the column unlike the data manipulation steps from (1) and (2) that are usually very specific to each variable. For instance, feature engineering and outliers removal are done in Pandas because they depend on the type of variable and their meaning ex. it doesn’t make sense to create polynomial features for categories or apply z-score outliers removal to ordinal variables or skewed ones.

In this unit and the next ones: Jupyter notebooks are a great way to develop/share a data analysis pipeline and document each step with Markdown cells and plots in an iterative way. At the end of this “prototyping” work, we sometimes want to encapsulate our code from (1) and (2) into a “clean” ML pipeline. In this unit and the next ones from this Advanced Scikit-learn chapter, we will see tools to achieve this. However, note that it’s not required to use those tools - they are only helpful to do this extra step of encapsulating the Pandas code from steps (1) and (2) into Scikit-learn objects at the end of the analysis/prototyping work. For this reason, this unit and the next ones from this chapter are entirely optional. You can skip those units and start working now on the final course project. When you are happy with your work, you can optionally read this unit and the next ones and think about how you could apply those tools to your analysis. However, this is entirely optional and requires good programming/debugging experience.

Column transformations
In this unit, we will see how to use the ColumnTransformer object from Scikit-learn to perform a few common preprocessing steps such as ordinal and one-hot encoding.

Before going into the code, it’s important to understand that this tool is part of a new workflow in Scikit-learn that tries to consolidate the data manipulation steps, usually done in Pandas, with the modeling part. As we will see in this unit and the next ones, this new Pandas/Scikit-learn workflow can be very powerful - however - Scikit-learn only provides partial support for DataFrames at the moment, so it can be difficult to model complex sequences of data manipulations with it.

In such cases, don’t hesitate to do part or all of the data manipulation work in Pandas as we saw previously. Also, keep an eye on the upcoming Scikit-learn releases to see how these new features evolve.

One-hot encoding with Scikit-learn
Let’s start by loading the data.

import pandas as pd

data_df = pd.read_csv("c3_bike-sharing-data.csv")
data_df.head()

Scikit-learn implements a OneHotEncoder transformer to handle categorical variables. Like the other objects from Scikit-learn, it accepts array-like objects, including DataFrames, as input but always returns Numpy arrays or related objects as we are will see below.

Let’s test it on our data_df DataFrame.

from sklearn.preprocessing import OneHotEncoder

# Create encoder
encoder = OneHotEncoder()
encoder.fit_transform(data_df)
<731x1714 sparse matrix of type '<class 'numpy.float64'>'
	with 7310 stored elements in Compressed Sparse Row format>
The result can be a bit surprising at first sight: we pass a DataFrame object and get a sparse matrix with 1,714 columns! In fact, the transformer encodes all the columns from the input data, including the numerical ones. So it creates a new one-hot encoded column for each distinct value in the DataFrame.

Let’s see how to fix this.

ColumnTransformer object
So far, we always converted the input data into Numpy arrays to avoid any issues during the ml part. However, Scikit-learn recently released a ColumnTransformer object that can apply different transformations to the columns of a Pandas DataFrame object.

In our case, we can use it to apply one-hot encoding to the categorical variables.

from sklearn.compose import ColumnTransformer

# Handle categorical variables
cat_columns = ["yr", "workingday", "holiday", "weekday", "season", "weathersit"]
cat_transformer = OneHotEncoder(sparse=False)

# Create the column transformer
preprocessor = ColumnTransformer(
    [("categorical", cat_transformer, cat_columns)], remainder="passthrough"
)
In this code, we first list the categorical columns in a cat_columns variable and create the OneHotEncoder() object. This time, we specify sparse=False when creating the encoder to get Numpy arrays instead of sparse matrices. We then create the ColumnTransformer object and specify the different transformations - one in our case - by defining (name, transformer, vars) triplets. We pass the list of categorical variables with the one-hot encoder and tell the object to leave the other columns unchanged by setting its remainder attribute to 'passthrough'.

Let’s test it on our input DataFrame

encoded = preprocessor.fit_transform(data_df)
encoded
array([[1.00e+00, 0.00e+00, 1.00e+00, ..., 8.06e-01, 1.60e-01, 3.31e+02],
       [1.00e+00, 0.00e+00, 1.00e+00, ..., 6.96e-01, 2.49e-01, 1.31e+02],
       [1.00e+00, 0.00e+00, 0.00e+00, ..., 4.37e-01, 2.48e-01, 1.20e+02],
       ...,
       [0.00e+00, 1.00e+00, 1.00e+00, ..., 7.53e-01, 1.24e-01, 1.59e+02],
       [0.00e+00, 1.00e+00, 1.00e+00, ..., 4.83e-01, 3.51e-01, 3.64e+02],
       [0.00e+00, 1.00e+00, 0.00e+00, ..., 5.78e-01, 1.55e-01, 4.39e+02]])
It’s important to note that we pass a DataFrame object as input and get a Numpy array. By looking at the values, we can see that the first columns correspond to the one-hot encoded columns and the last ones to the untransformed ones.

Let’s check the type and size of our encoded data.

print("Shape:", encoded.shape)
print("Type:", type(encoded))
print("Data type:", encoded.dtype)
Shape: (731, 24)
Type: <class 'numpy.ndarray'>
Data type: float64
This time, we get a reasonable number of columns. The result is now a (731, 24) Numpy array with the usual uniform float data type.

The encoded data is ready for the ML estimators, but not for additional Pandas data manipulation steps since we lost the column names in the conversion. This is why we said above that it can be complex to model sequences of data manipulation steps in Scikit-learn.

If we want to convert the result back into a DataFrame, we need to use the get_feature_names_out() method from our encoder.

try:
    cat_transformer.get_feature_names_out()
except Exception as e:
    print(e)
This OneHotEncoder instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
We get an error saying that the transformer is not fitted yet. Just like Pipeline objects, the ColumnTransformer works on copies and not on the original objects directly. To access the copies, we need to use the named_transformers_ attribute which returns the steps. This is similar to the named_steps attribute from Pipeline objects

preprocessor.named_transformers_
{'categorical': OneHotEncoder(sparse=False), 'remainder': 'passthrough'}
To get the feature names, we simply need to retrieve the encoder copy and call its get_feature_names_out()

preprocessor.named_transformers_["categorical"].get_feature_names_out()
array(['yr_2011', 'yr_2012', 'workingday_no', 'workingday_yes',
       'holiday_no', 'holiday_yes', 'weekday_0', 'weekday_1', 'weekday_2',
       'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'season_fall',
       'season_spring', 'season_summer', 'season_winter',
       'weathersit_clear', 'weathersit_cloudy', 'weathersit_rainy'],
      dtype=object)
Scikit-learn names the columns by order: x0 corresponds to the first column in cat_columns which is yr.

Issue with missing categories
The one-hot encoder creates a new column for each categorical value. A common issue is to have new, previously unknown, categories in the test data. For instance, let’s see what happens if we create a new storm category for the weathersit feature.

new_data = data_df.iloc[:1].copy()
new_data["weathersit"] = "storm"
new_data

If you take a look at the column names retrieved with the get_feature_names_out() call from above, you can see that the encoder only knows about the clear, cloudy and rainy categories. Let’s see how it handles this new storm category.

try:
    preprocessor.transform(new_data)
except Exception as e:
    print(e)
Found unknown categories ['storm'] in column 5 during transform
The one-hot encoder returns an exception saying that 'storm' is an unknown value. A common practice is to simply ignore unseen values and set all the corresponding one-hot encoded variables to zero i.e. x5_clear, x5_cloudy and x5_rainy.

We can specify this behavior by setting the handle_unknown attribute of our OneHotEncoder

# Handle categorical variables
cat_transformer = OneHotEncoder(handle_unknown="ignore", sparse=False)

# Create the column transformer
preprocessor = ColumnTransformer(
    [("categorical", cat_transformer, cat_columns)], remainder="passthrough"
)
preprocessor.fit_transform(data_df)
preprocessor.transform(new_data)
array([[1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,
        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,
        1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,
        0.00e+00, 0.00e+00, 3.44e-01, 8.06e-01, 1.60e-01, 3.31e+02]])
If we look at the entries with index 17, 18, 19 that correspond to the weathersit variable, we can see that they all have a value of zero.

Ordinal encoding with Scikit-learn
Scikit-learn also provides an OrdinalEncoder object to encode ordinal variables. It takes the list of ordinal values and encodes them using a 0 to N integer scale. Let’s test it on the weathersit variable.

from sklearn.preprocessing import OrdinalEncoder

# Handle ordinal variables
ord_columns = ["weathersit"]
ord_transformer = OrdinalEncoder(categories=[["clear", "cloudy", "rainy"]])
In this case, the encoder will simply map clear, cloudy and rainy to respectively 0, 1 and 2.

FunctionTransformer object
Ordinal and one-hot encoding are two common transformations which have their dedicated Scikit-learn transformers. However, we can also create new transformers with the FunctionTransformer object.

For instance, let’s create polynomial features with continuous variables.

from sklearn.preprocessing import StandardScaler, FunctionTransformer
from sklearn.pipeline import Pipeline
import numpy as np

# Add polynomial features
poly_columns = ["temp", "hum", "windspeed"]
poly_transformer = Pipeline(
    [
        ("scaler", StandardScaler()),
        ("poly", FunctionTransformer(lambda X: np.c_[X, X ** 2, X ** 3])),
    ]
)
The FunctionTransformer takes a function to apply as a parameter. In the code from above, we create an anonymous one with the lambda notation. The function simply adds the degree 2 and 3 to the input array X with the np.c_[] concatenation operation.

Note that our transformer from above is not equivalent to the PolynomialFeatures one which adds all the interaction terms in addition to the polynomial features.

from sklearn.preprocessing import PolynomialFeatures

polyfeat = PolynomialFeatures(degree=3, include_bias=False)
polyfeat.fit(data_df[poly_columns])
polyfeat.get_feature_names_out()
array(['temp', 'hum', 'windspeed', 'temp^2', 'temp hum', 'temp windspeed',
       'hum^2', 'hum windspeed', 'windspeed^2', 'temp^3', 'temp^2 hum',
       'temp^2 windspeed', 'temp hum^2', 'temp hum windspeed',
       'temp windspeed^2', 'hum^3', 'hum^2 windspeed', 'hum windspeed^2',
       'windspeed^3'], dtype=object)
As we can see, with the interaction terms, the PolynomialFeatures object creates a total of 19 features instead of just the 9 polynomial ones.

Complete pipeline
Let’s assemble the different transformations into a final ColumnTransformer

# Create the column transformer
preprocessor = ColumnTransformer(
    [
        ("categorical", cat_transformer, cat_columns),
        ("ordinal", ord_transformer, ord_columns),
        ("poly", poly_transformer, poly_columns),
    ],
    remainder="drop",
)

encoded = preprocessor.fit_transform(data_df)
encoded.shape
(731, 30)
This time, we apply the three different transformations and make sure that any additional columns, if any, are dropped by setting remainder to 'drop'.

If you execute the code from above, you will probably get a FutureWarning. Scikit-learn is simply warning us that the default value for one of the object parameters will change in a future release of the library. We can ignore such warnings by adding a simplefilter using the Python warnings module

import warnings

warnings.simplefilter("ignore", FutureWarning)
Let’s encapsulate our preprocessor with a LinearRegression estimator into a pipeline

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression

# Create Pipeline
pipe = Pipeline([("preprocessor", preprocessor), ("regressor", LinearRegression())])
and use the usual train/test split methodology to evaluate it

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error as MAE

# Split into train/test sets
X = data_df.drop("casual", axis=1)
y = data_df.casual
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)

# Fit/evaluate pipeline
pipe.fit(X_tr, y_tr)
print("MAE: {:.2f}".format(MAE(y_te, pipe.predict(X_te))))
MAE: 253.03
This time, we get a slightly better MAE score than what we obtained in the previous unit with only one-hot encoding: 253 vs. 280.

Summary
In this unit, we saw how to perform feature-specific preprocessing steps such as OneHotEncoder, OrdinalEncoder or any FunctionTransformer handmade ones with the ColumnTransformer object.

In the next unit, we will see how to encapsulate complex transformations into a custom transformer object that can be used with the tools from Scikit-learn.

04. Custom transformers
Content
Resources 1
Questions 2
This unit is optional
In this unit, we will see how to encapsulate a set of more advanced preprocessing steps done in Pandas into a Scikit-learn custom transformer.

The goal of this unit is to see how we can implement complex transformations in Scikit-learn. However, custom transformers are quite advanced tools and everything that we will see in this unit can be implemented outside Scikit-learn as a separate Pandas preprocessing step - it’s perfectly fine to do it with Pandas.

Messy bikes data
This time, we will work with a variant of the bike sharing data. The dataset is similar to the one from the previous unit but has missing values in the features.

import pandas as pd

data_df = pd.read_csv("c3_messy-bikes.csv")
data_df.head()

Note that the year yr and weekday values are encoded as floating point numbers instead of integers. We will also need to fix this.

To get a sense of the proportion of NaN entries, let’s run

data_df.isnull().mean()
temp          0.095759
hum           0.109439
windspeed     0.112175
yr            0.087551
workingday    0.088919
holiday       0.102599
weekday       0.097127
season        0.102599
weathersit    0.103967
casual        0.000000
dtype: float64
As we can see, input features contain approximately 10% missing values.

Custom preprocessing
Let’s write a preprocess_f(df) function to perform the necessary preprocessing steps. To avoid any issues, we will work on a copy of the df DataFrame

import numpy as np


def preprocess_f(df):
    # Work on a copy
    df = df.copy()

    # Missing values in continuous features
    cont_vars = ["temp", "hum", "windspeed"]
    for c in cont_vars:
        df[c] = df[c].fillna(df[c].mean())  # replace by mean

    # Explicitly convert to string values
    to_convert = ["yr", "weekday"]
    convert_f = lambda x: str(int(x)) if not np.isnan(x) else np.nan
    df[to_convert] = df[to_convert].applymap(convert_f)

    # .. in categorical ones: create 'missing' category
    cat_vars = ["yr", "workingday", "holiday", "weekday", "season", "weathersit"]
    df[cat_vars] = df[cat_vars].fillna("missing")

    # One-hot encoding
    df = pd.get_dummies(df)

    return df


preprocessed = preprocess_f(data_df)
In this code, we replace missing values in the continuous variables with their mean and add a missing category for the categorical ones.

We get the following preprocessed DataFrame

preprocessed.head()

Let’s take a look at the different columns

preprocessed.columns
Index(['temp', 'hum', 'windspeed', 'casual', 'yr_2011', 'yr_2012',
       'yr_missing', 'workingday_missing', 'workingday_no', 'workingday_yes',
       'holiday_missing', 'holiday_no', 'holiday_yes', 'weekday_0',
       'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5',
       'weekday_6', 'weekday_missing', 'season_fall', 'season_missing',
       'season_spring', 'season_summer', 'season_winter', 'weathersit_clear',
       'weathersit_cloudy', 'weathersit_missing', 'weathersit_rainy'],
      dtype='object')
As we have seen in the last unit, we can encapsulate such preprocessing functions into a FunctionTransformer object which can then be used with Scikit-learn tools such as pipelines.

from sklearn.preprocessing import FunctionTransformer

preprocessor = FunctionTransformer(preprocess_f, validate=False)
preprocessed = preprocessor.fit_transform(data_df)
As we saw in the last unit, Scikit-learn transformers work with Numpy arrays and not Pandas DataFrames as in our preprocess_f() function. To avoid any implicit conversion, we need to set the validate parameter to False.

You should get the same result as above.

preprocessed.head()

FunctionTransformer limitations
The FunctionTransformer has an important limitation: it can only encapsulate stateless transformations. For instance, let’s see what happens if we pass a single row to the preprocessor fitted above

preprocessor.transform(data_df.iloc[:1])

This time, the output only has 10 columns instead of 30. This is because the get_dummies() call inside our preprocess_f() function creates a column for each categorical value. In this case, we only pass a single data point so get_dummies() creates a single column for each categorical variable ex. only weathersit_cloudy since weathersit='cloudy' for this first entry, but no weathersit_rainy as above.

TransformerMixin object
To fix this, we need to create a custom transformer that saves the column names during fitting. This can be done by defining a subclass of the Scikit-learn BaseEstimator and TransformerMixin classes and by implementing the __init__(), fit() and transform() functions.

from sklearn.base import BaseEstimator, TransformerMixin


class PandasPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self, preprocess_f):
        self.preprocess_f = preprocess_f

    def fit(self, X_df, y=None):
        # Check that we get a DataFrame
        assert type(X_df) == pd.DataFrame

        # Preprocess data
        X_preprocessed = self.preprocess_f(X_df)

        # Save columns names/order for inference time
        self.columns_ = X_preprocessed.columns

        return self

    def transform(self, X_df):
        # Check that we get a DataFrame
        assert type(X_df) == pd.DataFrame

        # Preprocess data
        X_preprocessed = self.preprocess_f(X_df)

        # Make sure to have the same features
        X_reindexed = X_preprocessed.reindex(columns=self.columns_, fill_value=0)

        return X_reindexed
In this implementation, we create a PandasPreprocessor transformer that takes a preprocessing preprocess_f function and saves it.

In its fit() method, we check that the input is a DataFrame object and preprocess it with the function. We then save the columns in a new columns_ attribute that we use in the transform() method to make sure that the transformed output has the same set of columns as the input data.

Concretely, this is done with a simple df.reindex(columns, fill_value=0) operation which reindexes the DataFrame df such that it has the same columns as columns and in the same order. If the column is missing, it simply creates it and set its values to zeros.

Let’s see if this fixes our issue.

preprocessor = PandasPreprocessor(preprocess_f)
preprocessor.fit(data_df)
preprocessor.transform(data_df.iloc[:1])

This time, we get a DataFrame with the correct columns, but the missing entry in windspeed wasn’t replaced by the feature mean.

Again, the issue comes from our preprocess_f() implementation which is stateless - missing values are replaced by the mean of the current DataFrame which, in this case, contains a single entry. Since the mean of NaN is NaN, it didn’t impute the missing value. To solve the issue, we need to store the train mean in the fit() step.

class PandasPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.cat_vars_ = [
            "yr",
            "workingday",
            "holiday",
            "weekday",
            "season",
            "weathersit",
        ]
        self.cont_vars_ = ["temp", "hum", "windspeed"]
        self.to_convert_ = ["yr", "weekday"]

    def preprocess_f(self, X_df, train_mean):
        # Work on a copy
        X_df = X_df.copy()

        # Missing values in continuous features
        for c in self.cont_vars_:
            X_df[c] = X_df[c].fillna(train_mean[c])

        # Explicitly convert to string values
        convert_f = lambda x: str(int(x)) if not np.isnan(x) else np.nan
        X_df[self.to_convert_] = X_df[self.to_convert_].applymap(convert_f)

        # .. in categorical ones: create 'missing' category
        X_df[self.cat_vars_] = X_df[self.cat_vars_].fillna("missing")

        # One-hot encoding
        X_df = pd.get_dummies(X_df)

        return X_df

    def fit(self, X_df, y=None):
        # Check that we get a DataFrame
        assert type(X_df) == pd.DataFrame

        # Save train mean for continuous variables
        self.train_mean_ = X_df[self.cont_vars_].mean()

        # Preprocess data
        X_preprocessed = self.preprocess_f(X_df, self.train_mean_)

        # Save columns names/order for inference time
        self.columns_ = X_preprocessed.columns

        return self

    def transform(self, X_df):
        # Check that we get a DataFrame
        assert type(X_df) == pd.DataFrame

        # Preprocess data
        X_preprocessed = self.preprocess_f(X_df, self.train_mean_)

        # Make sure to have the same features
        X_reindexed = X_preprocessed.reindex(columns=self.columns_, fill_value=0)

        return X_reindexed
Let’s look at the main differences with our previous implementation. First, the preprocess_f function and the list of columns are now part of the object. We also pass a train_mean argument to the function. Those values are computed during training time in the fit() step and stored as a train_mean_ attribute. Finally, in the transform() step, we reuse our preprocessing function with the train mean values.

Let’s test this new implementation

preprocessor = PandasPreprocessor()
preprocessor.fit(data_df)
preprocessor.transform(data_df.iloc[:1])

The windspeed value now corresponds to the mean wind speed computed during the fit() call.

Complete Pipeline
Let’s build a final pipeline with our new PandasPreprocessor custom transformer.

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression

# Use our custom transformer in a pipeline
pipe = Pipeline(
    [("preprocessor", PandasPreprocessor()), ("estimator", LinearRegression())]
)
We can now evaluate the pipe object as if it was a standard estimator. We will use the train/test set methodology.

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error as MAE

# Split data
X = data_df.drop("casual", axis=1)
y = data_df.casual
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)

# Evaluate estimator
pipe.fit(X_tr, y_tr)
print("MAE: {:.2f}".format(MAE(y_te, pipe.predict(X_te))))
MAE: 293.58
It’s important to understand that our implementation handles the separation between train and test sets. The mean values are computed on the train set X_tr and used to replace missing values in both sets.

This time, we get a slightly larger MAE score than in the previous units 295 vs. 280. The difference is due to the missing values in the data - our estimator was not able to perfectly recover the information loss!

Summary
In this unit, we experimented with custom transformers from Scikit-learn and used them to encapsulate a complex set of Pandas preprocessing steps.

It’s interesting to note that Scikit-learn transformers can only transform the features column axis, but not the data points row axis. For example, we cannot create transformers that drop data points as it’s done with e.g. outliers removal. In the next unit, we will see how to do this by defining custom estimators.

05. Custom estimators
Content
Resources 1
Questions 2
This unit is optional
In the last unit, we saw how to define our own transformers. In this unit, we will see how to implement custom estimators with the scenario of outliers removal.

Use case - outliers removal
Let’s start by loading the dataset.

import pandas as pd

data_df = pd.read_csv("c3_house-prices.csv")
data_df.head()

We learned about skewed distributions in the last subject and saw how log-transforms can help in such cases. However, it’s still possible that some values remain far from the mean after the transformation.

For instance, let’s plot the z-scores distribution of the Lot Area variable before and after the log-transform. This time, we use the zscore() function from the scipy.stats module.

%matplotlib inline
import matplotlib.pyplot as plt
from scipy.stats import zscore
import numpy as np

# Check for outliers in the continuous features
c = "Lot Area"
x = data_df[c].dropna()

# Plot histograms
fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))
ax1.hist(zscore(x), bins=50)
ax2.hist(zscore(np.log1p(x)), bins=50)
ax1.set_title(c)
ax2.set_title("log({})".format(c))
plt.show()

We can see that the transformation helps, but there are still many values with a z-score above +3 or below -3. We could simply remove them with Pandas before applying our ML models in Scikit-learn. However, let’s see how to encapsulate this preprocessing step into a Scikit-learn object.

The Lot Area isn’t the only variable with a skewed distribution in this dataset. In the following code, we list them and create the train and test sets.

from sklearn.model_selection import train_test_split

# Create X, y
X = data_df.drop("SalePrice", axis=1)
y = np.log10(data_df.SalePrice)

# Split into train/test sets
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.5, random_state=0)

# Continuous variables to check
to_check = ["Lot Area", "Lot Frontage", "Total Bsmt SF", "Gr Liv Area", "Garage Area"]
X_tr[to_check].head()

Note that we still have missing values in the data. In this unit, we ignore them during the outliers removal part and will then see later in this unit how to replace them using a SimpleImputer transformer object.

ClassifierMixin object
In the last unit, we defined our own transformers by creating a subclass of the BaseEstimator and TransformerMixin. Similarly, we can define custom estimators by creating a subclass of BaseEstimator and

RegressorMixin for regression tasks
ClassifierMixin for classification ones
Let’s see how to create a ZScoresOutlierClassifier class that takes a set of column names to check for outliers and a removal threshold. This custom estimator will predict whether a point is an outlier or not - since this is a classification task, we need to extend ClassifierMixin.

from sklearn.base import BaseEstimator, ClassifierMixin

# Custom outliers detector base on z-scores
# Adapted from https://github.com/scikit-learn/scikit-learn/issues/9630#issuecomment-325202441
class ZScoresOutlierClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, columns, threshold=3):
        self.columns = columns
        self.threshold = threshold

    def fit(self, X_df, y):
        # Check that X_df is a DataFrame
        assert type(X_df) == pd.DataFrame

        # Compute train mean/std
        self.train_mean_ = X_df[self.columns].mean()
        self.train_std_ = X_df[self.columns].std()

        # Return estimator
        return self

    def predict(self, X_df):
        # Check that X_df is a DataFrame
        assert type(X_df) == pd.DataFrame

        # Apply threshold
        z_scores = (X_df[self.columns] - self.train_mean_) / (self.train_std_)
        below_threshold = np.abs(z_scores.fillna(0)) <= self.threshold

        # Find inliners
        mask = below_threshold.all(axis=1)

        # Return predictions: +1 for inliners, -1 for outliers
        return mask.replace({True: 1, False: -1})
In this code, we define two functions: fit() and predict().

The fit function computes the mean and standard deviation of each column of the train DataFrame and stores them in the train_mean_ and train_std_ variables. The “predict” part computes the z-scores using simple Pandas code and checks that the data points are below the defined threshold.

Note the Pandas all(axis=1) call - a point that isn’t an outlier should have all its z-scores below the threshold. Finally, we return 1 for normal points and -1 for outliers: this is the standard encoding for outlier detector objects from Scikit-learn.

Let’s test our custom classifier on the training data

outliers_clf = ZScoresOutlierClassifier(to_check)
outliers_clf.fit(X_tr, y_tr)
outliers_clf.predict(X_tr).head()
339    -1
1557    1
2167    1
706     1
2396    1
dtype: int64
As we can see, the first point is labeled as an outlier and shouldn’t be used to train the model.

Challenge: We didn’t check for outliers in the output variable - can you adapt the code to also handle extreme values in the target?

RegressorMixin object
Now that we have a custom outlier classifier, let’s see how to use it to improve our predictions. Remember: outliers hurt the performance of our models because of the RSS-based cost functions which have bad statistical properties i.e. they don’t handle well statistically extreme values.

Let’s define a WithoutOutliersRegressor object that first removes the outliers from the training data before fitting the estimator.

from sklearn.base import RegressorMixin, clone

# Custom regressor with an embedded outliers detector
# Adapted from https://github.com/scikit-learn/scikit-learn/issues/9630#issuecomment-325202441
class WithoutOutliersRegressor(BaseEstimator, RegressorMixin):
    def __init__(self, outlier_detector, regressor):
        self.outlier_detector = outlier_detector
        self.regressor = regressor

    def fit(self, X, y, verbose=False):
        # Fit outliers detector, use it on X
        self.outlier_detector_ = clone(self.outlier_detector).fit(X, y)
        outliers = self.outlier_detector_.predict(X) == -1

        # Print the number of outliers detected
        if verbose:
            print(
                "Outliers detected: {} ({:.1f}%)".format(
                    outliers.sum(), 100 * outliers.mean()
                )
            )

        # Fit regressor without the outliers
        self.regressor_ = clone(self.regressor).fit(X[~outliers], y[~outliers])

        # Return the estimator
        return self

    def predict(self, X):
        # Make predictions with the regressor (fitted without the outliers)
        return self.regressor_.predict(X)
Again, we need to define the fit() and predict() functions.

In the fit part, we fit our outlier detector to the training data, create an outliers mask and use it to exclude outliers from our model fit() call. In the “predict” part, we simply use our fitted regressor object to make new predictions for all data points i.e. including potential outliers.

So far in this course, we always saw examples where the current state of our estimators was clear ex. is it fitted, on what data are the coefficients computed and so on. However, in this implementation, we are working with an outlier detector and a regressor that are created outside our custom estimator, and we modify them inside it by calling the .fit() method.

To avoid any confusion, we clone the estimators with the clone() function from Scikit-learn - that way, we are sure to leave the original objects unmodified. This issue is very similar to what can happen with Pandas inplace=True operations. If you’re curious about this, you can take a look at this example which illustrate the issue.

Complete pipeline
Let’s use our new estimator in a complete pipeline. First, we need to fill missing values and encode non-numerical variables. This time, we will use the SimpleImputer object from Scikit-learn to handle missing values.

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# One-hot encoding for non-numerical columns
onehot_columns = X.select_dtypes(exclude=np.number).columns
onehot_transformer = Pipeline(
    [
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("encoder", OneHotEncoder(handle_unknown="ignore")),
    ]
)
By setting its strategy parameter to most_frequent, the imputer simply replaces missing values with the most frequent value found in the column. Other possible strategies are mean, median and constant. You can always refer to the documentation if you’re unsure about the different options.

Let’s also define the transformations for the numerical columns.

from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import StandardScaler

# Basic transformations for the others
other_columns = X.columns.difference(onehot_columns)
other_transformer = Pipeline(
    [
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("log", FunctionTransformer(np.log1p)),
        ("scaler", StandardScaler()),
    ]
)
Again, we perform minimal preprocessing steps. We first impute missing values, apply the log-transform and standardize the results.

Let’s collect the preprocessing steps into a final ColumnTransformer

from sklearn.compose import ColumnTransformer

# Create preprocessor
preprocessor = ColumnTransformer(
    [
        ("onehot", onehot_transformer, onehot_columns),
        ("other", other_transformer, other_columns),
    ]
)
Finally, let’s create our WithoutOutliersRegressor custom estimator. We simply need to pass the outlier detector and the regressor

from sklearn.linear_model import Ridge

# Define pipeline
model = WithoutOutliersRegressor(
    outlier_detector=ZScoresOutlierClassifier(to_check, threshold=3),
    regressor=Pipeline([("preprocessor", preprocessor), ("ridge", Ridge())]),
)
It’s important to note that our ZScoresOutlierClassifier works on the untransformed DataFrame - our preprocessor is only applied before the Ridge model! For this reason, the detector is working on the original variables and not the log-transformed ones.

To apply the outliers detector on the log-transformed variables, we cannot simply encapsulate our detector into a ColumnTransformer that applies the log-transform to the columns listed in to_check. The reason is simple: as we saw in the last units, column transformer objects produce Numpy arrays. However, our outliers detector works on DataFrames!

Challenge: Can you adjust the ZScoresOutlierClassifier to accept Numpy arrays instead of Pandas DataFrames to make it fully compatible with Scikit-learn objects?

Final evaluation
Let’s see how our fully-encapsulated model performs.

import warnings

warnings.simplefilter("ignore", FutureWarning)

model.fit(X_tr, y_tr, verbose=True)
Outliers detected: 38 (3.1%)
The outlier detector labeled around 3% of the entries as outliers. Those training points won’t be used to fit the model.

Let’s evaluate it on the test set.

from sklearn.metrics import mean_absolute_error as MAE

# Evaluate predictions
y_pred = model.predict(X_te)
print("MAE: {:,.2f}$".format(MAE(10 ** y_te, 10 ** y_pred)))
MAE: 14,213.04$
This time, we get an MAE score around 14 thousand dollars which is similar to what we obtained in the previous units.

The advantage of our pipeline is that we can easily switch to other outlier detection methods. It’s not part of the course to know how the different detectors work, but it’s important to know that Scikit-learn implements many advanced techniques that can be used out-of-the-box once we are familiar with the transformers and estimators API.

The library also does a great job at documenting the different approaches. If you google sklearn outlier detection, you should get this page which documents them in the top results.

For instance, let’s try with the IsolationForest detector

from sklearn.ensemble import IsolationForest
from sklearn.pipeline import make_pipeline

# Try with Isolation forests
model2 = WithoutOutliersRegressor(
    outlier_detector=make_pipeline(preprocessor, IsolationForest(random_state=0)),
    regressor=Pipeline([("preprocessor", preprocessor), ("ridge", Ridge())]),
)
model2.fit(X_tr, y_tr, verbose=True)

# Evaluate predictions
y_pred = model2.predict(X_te)
print("MAE: {:,.2f}$".format(MAE(10 ** y_te, 10 ** y_pred)))
Outliers detected: 5 (0.4%)
MAE: 14,480.44$
This time, we add our preprocessor before the IsolationForest using the make_pipeline() function to quickly create a Pipeline without having to name each step.

As we can see, the isolation forest labels 10% of the data points as outliers and we get a slightly larger MAE score.

Summary
In this unit, we saw how to create custom estimators with the example of outliers removal. Custom estimators can be particularly useful if we need to encapsulate tools from other libraries into our Scikit-learn workflow. For instance, encapsulate ML methods from NLTK, TensorFlow or even our own algorithms.

In the next unit, we will discuss advanced transformations that can improve the performance of our models. This will end our tour of the Scikit-learn library and machine learning workflow.

06. Advanced transformations
Content
Resources 1
This unit is optional
Transforming a variable to make it more Gaussian-like is a common operation in machine learning. In this unit, we will see different ways to do it with Scikit-learn.

We will take the house prices dataset as an example and build a simple model with two features: the overall quality of the house and its ground living area.

Target transformation
Let’s start by loading the data

import pandas as pd

data_df = pd.read_csv("c3_house-prices.csv")
data_df.head()

Let’s create the X/y variables and split them into train/test sets.

from sklearn.model_selection import train_test_split

# Create X, y
X = data_df[["Overall Qual", "Gr Liv Area"]]
y = data_df.SalePrice

# Split into train/test sets
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.5, random_state=0)
We already saw that the sale prices have a right-skewed distribution which can make the model biased toward expensive houses. One of the simplest ways to handle this problem is to apply a log-transform.

In practice, we don’t really want to do this transformation outside Scikit-learn which would imply that we have to manually transform y before calling model.fit() and apply the inverse transformation to predictions after model.predict() calls.

To solve this issue, Scikit-learn implements a TransformedTargetRegressor which can automatically apply some transformation to the target variable.

from sklearn.compose import TransformedTargetRegressor
from sklearn.linear_model import LinearRegression
import numpy as np

regressor = TransformedTargetRegressor(
    regressor=LinearRegression(), func=np.log, inverse_func=np.exp
)
The func attribute is the function that we want to apply before fitting the model, and the inverse_func one the inverse operation to transform the predictions.

Let’s test this object on our data

# Make predictions
regressor.fit(X_tr, y_tr)
y_pred = regressor.predict(X_te)
In this code, both y_tr and y_pred are prices in dollars - the transformation is done inside our regressor estimator.

Let’s verify that those two variables have more or less the same distribution. This is an easy way to detect when the model is not working properly.

%matplotlib inline
import matplotlib.pyplot as plt

# Plot predictions
plt.hist(
    y_te, bins=50, range=(0, 10 ** 6), density=True, alpha=0.3, label="sale prices"
)
plt.hist(
    y_pred, bins=50, range=(0, 10 ** 6), density=True, alpha=0.3, label="predictions"
)
plt.legend()
plt.show()

In this code, the density attribute normalizes the histograms such that they are proper probability distributions that we can compare. We also focus on houses with an observed and estimated price below 1 million by adjusting the range argument.

Finally, let’s compute the MAE score of our simple two-features model.

from sklearn.metrics import mean_absolute_error as MAE

print("MAE: {:,.2f}$".format(MAE(y_te, y_pred)))
MAE: 25,411.60$
On average, our predictions are around 25 thousand dollars away from the observed price.

Challenge: How good is it compared to the median baseline?

Other transformations
The log-transform works really well when the variables have a skewed distribution. More generally, there are several approaches to make a variable distribution more Gaussian-like. It’s out of the scope of this course to understand them in details, but it’s important to know that we can easily try them since we’re now familiar with the transformer API.

For instance, let’s try the QuantileTransformer one on the sale prices

from sklearn.preprocessing import QuantileTransformer

regressor = TransformedTargetRegressor(
    regressor=LinearRegression(),
    transformer=QuantileTransformer(output_distribution="normal", random_state=0),
)
regressor.fit(X_tr, y_tr)
print("MAE: {:,.2f}$".format(MAE(y_te, regressor.predict(X_te))))
MAE: 24,728.46$
We get a slightly better MAE score of 24,728 dollars. If you are curious about the different available parameters, make sure to check the documentation page which also gives some advice on using the transformer.

Let’s also try adding the PowerTransformer one to the features.

from sklearn.preprocessing import PowerTransformer
from sklearn.pipeline import make_pipeline

regressor = TransformedTargetRegressor(
    regressor=make_pipeline(PowerTransformer(), LinearRegression()),
    transformer=QuantileTransformer(output_distribution="normal", random_state=0),
)
regressor.fit(X_tr, y_tr)
print("MAE: {:,.2f}$".format(MAE(y_te, regressor.predict(X_te))))
MAE: 24,410.30$
Again, we improved a bit the MAE which is now 24,410 dollars.

To get a more exhaustive list of the preprocessing steps available in Scikit-learn, make sure to take a look at the Preprocessing Guide from the official website.





