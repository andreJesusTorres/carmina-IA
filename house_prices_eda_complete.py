# =============================================================================
# HOUSE PRICES - COMPREHENSIVE EXPLORATORY DATA ANALYSIS & DATA CLEANING
# =============================================================================
# This script provides a complete EDA following the assignment requirements:
# - Comprehensive data exploration and visualization
# - Identification and handling of data issues
# - Feature-target relationship analysis
# - Data cleaning decisions with justification
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Configure plotting style
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

print("=== COMPREHENSIVE EXPLORATORY DATA ANALYSIS - HOUSE PRICES ===")
print("=" * 70)

# =============================================================================
# 1. DATA LOADING AND INITIAL EXPLORATION
# =============================================================================

print("\n1. DATA LOADING AND INITIAL EXPLORATION")
print("-" * 50)

# Load data
train_data = pd.read_csv('house-prices.csv')
test_data = pd.read_csv('house-prices-test.csv')

print(f"üìä Training dataset: {train_data.shape}")
print(f"üìä Test dataset: {test_data.shape}")

# Basic information
print("\nüìã Dataset Information:")
print(train_data.info())

# Data types overview
print("\nüîç Data Types Distribution:")
print(train_data.dtypes.value_counts())

# First few rows
print("\nüëÄ First 5 rows:")
print(train_data.head())

# =============================================================================
# 2. MISSING VALUES ANALYSIS
# =============================================================================

print("\n\n2. MISSING VALUES ANALYSIS")
print("-" * 50)

# Calculate missing values
missing_values = train_data.isnull().sum()
missing_percent = (missing_values / len(train_data)) * 100

# Create comprehensive missing values dataframe
missing_df = pd.DataFrame({
    'Missing_Count': missing_values,
    'Missing_Percentage': missing_percent,
    'Data_Type': train_data.dtypes,
    'Unique_Values': train_data.nunique()
}).sort_values('Missing_Percentage', ascending=False)

print("üìä Variables with missing values:")
print(missing_df[missing_df['Missing_Count'] > 0])

# Visualize missing values
plt.figure(figsize=(15, 10))

# Top 15 variables with most missing values
plt.subplot(2, 1, 1)
missing_plot = missing_df[missing_df['Missing_Count'] > 0].head(15)
bars = plt.barh(range(len(missing_plot)), missing_plot['Missing_Percentage'])
plt.yticks(range(len(missing_plot)), missing_plot.index)
plt.xlabel('Missing Values (%)')
plt.title('Top 15 Variables with Most Missing Values')
plt.gca().invert_yaxis()

# Color bars based on missing percentage
for i, bar in enumerate(bars):
    if missing_plot.iloc[i]['Missing_Percentage'] > 50:
        bar.set_color('red')
    elif missing_plot.iloc[i]['Missing_Percentage'] > 20:
        bar.set_color('orange')
    else:
        bar.set_color('blue')

# Missing values heatmap for top variables
plt.subplot(2, 1, 2)
top_missing_vars = missing_df[missing_df['Missing_Count'] > 0].head(10).index.tolist()
missing_subset = train_data[top_missing_vars].isnull()
sns.heatmap(missing_subset, cbar=True, yticklabels=False, cmap='viridis')
plt.title('Missing Values Pattern (Top 10 Variables)')
plt.xlabel('Variables')

plt.tight_layout()
plt.savefig('missing_values_analysis.png', dpi=300, bbox_inches='tight')
plt.close()

print("üìä Missing values visualization saved as 'missing_values_analysis.png'")

# =============================================================================
# 3. DATA CLEANING - HYBRID STRATEGY
# =============================================================================

print("\n\n3. DATA CLEANING - HYBRID STRATEGY")
print("-" * 50)

print("üîß APPLYING HYBRID CLEANING STRATEGY:")
print("   1. Drop columns with >50% missing values")
print("   2. Fill numeric missing values with median")
print("   3. Fill categorical missing values with 'Missing'")

# Create a copy for cleaning
train_data_clean = train_data.copy()

# 1. Drop columns with many missing values
columns_to_drop = ['Pool QC', 'Misc Feature', 'Alley', 'Fence']
print(f"\nüìã Dropping columns: {columns_to_drop}")
train_data_clean = train_data_clean.drop(columns=columns_to_drop)

# 2. Fill numeric missing values with median
numeric_columns = train_data_clean.select_dtypes(include=[np.number]).columns
print(f"\nüìä Filling numeric columns ({len(numeric_columns)} columns):")
for col in numeric_columns:
    if train_data_clean[col].isnull().sum() > 0:
        median_value = train_data_clean[col].median()
        train_data_clean[col] = train_data_clean[col].fillna(median_value)
        print(f"   - {col}: {train_data_clean[col].isnull().sum()} missing values ‚Üí filled with median ({median_value:.2f})")

# 3. Fill categorical missing values with 'Missing'
categorical_columns = train_data_clean.select_dtypes(include=['object']).columns
print(f"\nüìä Filling categorical columns ({len(categorical_columns)} columns):")
for col in categorical_columns:
    if train_data_clean[col].isnull().sum() > 0:
        train_data_clean[col] = train_data_clean[col].fillna('Missing')
        print(f"   - {col}: {train_data_clean[col].isnull().sum()} missing values ‚Üí filled with 'Missing'")

# Verify cleaning results
print(f"\n‚úÖ CLEANING RESULTS:")
print(f"   - Original shape: {train_data.shape}")
print(f"   - Cleaned shape: {train_data_clean.shape}")
print(f"   - Remaining missing values: {train_data_clean.isnull().sum().sum()}")

# Replace original data with cleaned data for rest of analysis
train_data = train_data_clean.copy()

# =============================================================================
# 4. TARGET VARIABLE ANALYSIS (SALEPRICE)
# =============================================================================

print("\n\n4. TARGET VARIABLE ANALYSIS (SALEPRICE)")
print("-" * 50)

# Descriptive statistics
print("üìä SalePrice Descriptive Statistics:")
print(train_data['SalePrice'].describe())

# Distribution analysis
plt.figure(figsize=(20, 12))

# Histogram and density plot
plt.subplot(2, 3, 1)
plt.hist(train_data['SalePrice'], bins=50, alpha=0.7, color='skyblue', edgecolor='black', density=True)
plt.axvline(train_data['SalePrice'].mean(), color='red', linestyle='--', label=f'Mean: ${train_data["SalePrice"].mean():,.0f}')
plt.axvline(train_data['SalePrice'].median(), color='green', linestyle='--', label=f'Median: ${train_data["SalePrice"].median():,.0f}')
plt.title('SalePrice Distribution')
plt.xlabel('Price ($)')
plt.ylabel('Density')
plt.legend()

# Boxplot
plt.subplot(2, 3, 2)
plt.boxplot(train_data['SalePrice'])
plt.title('SalePrice Boxplot')
plt.ylabel('Price ($)')

# Q-Q plot for normality check
plt.subplot(2, 3, 3)
stats.probplot(train_data['SalePrice'], dist="norm", plot=plt)
plt.title('Q-Q Plot for Normality Check')

# Log transformation comparison
plt.subplot(2, 3, 4)
log_price = np.log1p(train_data['SalePrice'])
plt.hist(log_price, bins=50, alpha=0.7, color='lightgreen', edgecolor='black', density=True)
plt.title('Log-Transformed SalePrice Distribution')
plt.xlabel('Log(Price)')
plt.ylabel('Density')

# Original vs Log comparison
plt.subplot(2, 3, 5)
plt.scatter(train_data['SalePrice'], log_price, alpha=0.6, color='purple')
plt.xlabel('Original SalePrice')
plt.ylabel('Log(SalePrice)')
plt.title('Original vs Log-Transformed SalePrice')

# Price by year sold
plt.subplot(2, 3, 6)
price_by_year = train_data.groupby('Yr Sold')['SalePrice'].mean()
plt.plot(price_by_year.index, price_by_year.values, marker='o', linewidth=2, markersize=6)
plt.title('Average Price by Year Sold')
plt.xlabel('Year Sold')
plt.ylabel('Average Price ($)')
plt.xticks(rotation=45)

plt.tight_layout()
plt.savefig('target_variable_analysis.png', dpi=300, bbox_inches='tight')
plt.close()

print("üìä Target variable analysis saved as 'target_variable_analysis.png'")

# Outlier analysis
Q1 = train_data['SalePrice'].quantile(0.25)
Q3 = train_data['SalePrice'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = train_data[(train_data['SalePrice'] < lower_bound) | 
                      (train_data['SalePrice'] > upper_bound)]

print(f"\nüîç Outlier Analysis:")
print(f"   - Q1: ${Q1:,.0f}")
print(f"   - Q3: ${Q3:,.0f}")
print(f"   - IQR: ${IQR:,.0f}")
print(f"   - Lower bound: ${lower_bound:,.0f}")
print(f"   - Upper bound: ${upper_bound:,.0f}")
print(f"   - Outliers found: {len(outliers)} ({len(outliers)/len(train_data)*100:.1f}%)")

# Skewness and kurtosis
print(f"\nüìà Distribution Statistics:")
print(f"   - Skewness: {train_data['SalePrice'].skew():.3f}")
print(f"   - Kurtosis: {train_data['SalePrice'].kurtosis():.3f}")

# =============================================================================
# 5. NUMERICAL FEATURES ANALYSIS
# =============================================================================

print("\n\n5. NUMERICAL FEATURES ANALYSIS")
print("-" * 50)

# Select numerical features
numeric_columns = train_data.select_dtypes(include=[np.number]).columns.tolist()
numeric_columns.remove('SalePrice')  # Exclude target variable

print(f"üìä Numerical features found: {len(numeric_columns)}")

# Correlation analysis
correlations = train_data[numeric_columns + ['SalePrice']].corr()['SalePrice'].sort_values(ascending=False)
print("\nüîó Top 15 features most correlated with SalePrice:")
print(correlations.head(16))  # Include SalePrice (correlation = 1)

# Visualize correlations
plt.figure(figsize=(15, 10))

# Top correlations bar plot
plt.subplot(2, 2, 1)
top_corr = correlations.head(16)[1:16]  # Exclude SalePrice
bars = plt.barh(range(len(top_corr)), top_corr.values)
plt.yticks(range(len(top_corr)), top_corr.index)
plt.xlabel('Correlation with SalePrice')
plt.title('Top 15 Features Most Correlated with SalePrice')
plt.gca().invert_yaxis()

# Color bars based on correlation strength
for i, bar in enumerate(bars):
    if abs(top_corr.iloc[i]) > 0.7:
        bar.set_color('red')
    elif abs(top_corr.iloc[i]) > 0.5:
        bar.set_color('orange')
    else:
        bar.set_color('blue')

# Correlation heatmap for top features
plt.subplot(2, 2, 2)
top_vars = correlations.head(16).index.tolist()
corr_matrix = train_data[top_vars].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
plt.title('Correlation Matrix - Top Features')

# Scatter plots for top 4 features
top_4_features = correlations.head(5)[1:5].index.tolist()
plt.subplot(2, 2, 3)
for i, feature in enumerate(top_4_features):
    plt.scatter(train_data[feature], train_data['SalePrice'], alpha=0.6, label=feature)
plt.xlabel('Feature Values')
plt.ylabel('SalePrice')
plt.title('Top 4 Features vs SalePrice')
plt.legend()

# Feature distributions
plt.subplot(2, 2, 4)
for feature in top_4_features:
    plt.hist(train_data[feature], alpha=0.5, label=feature, bins=30)
plt.xlabel('Feature Values')
plt.ylabel('Frequency')
plt.title('Distribution of Top 4 Features')
plt.legend()

plt.tight_layout()
plt.savefig('numerical_features_analysis.png', dpi=300, bbox_inches='tight')
plt.close()

print("üìä Numerical features analysis saved as 'numerical_features_analysis.png'")

# =============================================================================
# 6. CATEGORICAL FEATURES ANALYSIS
# =============================================================================

print("\n\n6. CATEGORICAL FEATURES ANALYSIS")
print("-" * 50)

# Select categorical features
categorical_columns = train_data.select_dtypes(include=['object']).columns.tolist()
print(f"üìä Categorical features found: {len(categorical_columns)}")

# Analyze categorical features with SalePrice
plt.figure(figsize=(20, 15))

# Select important categorical features (based on domain knowledge and correlation potential)
important_categorical = ['Overall Qual', 'Neighborhood', 'Kitchen Qual', 'Exter Qual', 
                        'Garage Type', 'Foundation', 'Heating QC', 'MS Zoning']

for i, col in enumerate(important_categorical, 1):
    if col in categorical_columns:
        plt.subplot(3, 3, i)
        # Calculate mean price by category
        price_by_category = train_data.groupby(col)['SalePrice'].agg(['mean', 'count']).sort_values('mean', ascending=False)
        
        # Plot only categories with sufficient data
        significant_categories = price_by_category[price_by_category['count'] >= 5]
        bars = plt.bar(range(len(significant_categories)), significant_categories['mean'])
        plt.title(f'Average Price by {col}')
        plt.xlabel(col)
        plt.ylabel('Average Price ($)')
        plt.xticks(range(len(significant_categories)), significant_categories.index, rotation=45)
        
        # Color bars based on price
        for j, bar in enumerate(bars):
            if significant_categories.iloc[j]['mean'] > train_data['SalePrice'].mean():
                bar.set_color('green')
            else:
                bar.set_color('lightblue')

plt.tight_layout()
plt.savefig('categorical_features_analysis.png', dpi=300, bbox_inches='tight')
plt.close()

print("üìä Categorical features analysis saved as 'categorical_features_analysis.png'")

# =============================================================================
# 7. DATA QUALITY ISSUES AND INCONSISTENCIES
# =============================================================================

print("\n\n7. DATA QUALITY ISSUES AND INCONSISTENCIES")
print("-" * 50)

# Check for duplicates
duplicates = train_data.duplicated().sum()
print(f"üîç Duplicate rows: {duplicates}")

# Check for inconsistent data types
print("\nüìã Data type inconsistencies:")
for col in train_data.columns:
    if train_data[col].dtype == 'object':
        # Check if categorical column might be numeric
        try:
            pd.to_numeric(train_data[col], errors='raise')
            print(f"   - {col}: Object type but contains numeric data")
        except:
            pass

# Check for extreme values in numerical features
print("\nüîç Extreme values analysis:")
outlier_summary = []
for col in numeric_columns[:10]:  # Check first 10 numerical features
    Q1 = train_data[col].quantile(0.25)
    Q3 = train_data[col].quantile(0.75)
    IQR = Q3 - Q1
    outliers_count = len(train_data[(train_data[col] < Q1 - 1.5*IQR) | 
                                   (train_data[col] > Q3 + 1.5*IQR)])
    if outliers_count > 0:
        outlier_summary.append({
            'Feature': col,
            'Outliers': outliers_count,
            'Percentage': outliers_count/len(train_data)*100
        })
        print(f"   - {col}: {outliers_count} outliers ({outliers_count/len(train_data)*100:.1f}%)")

# Visualize outliers
if outlier_summary:
    outlier_df = pd.DataFrame(outlier_summary)
    plt.figure(figsize=(12, 6))
    bars = plt.bar(range(len(outlier_df)), outlier_df['Percentage'])
    plt.xticks(range(len(outlier_df)), outlier_df['Feature'], rotation=45)
    plt.ylabel('Outlier Percentage (%)')
    plt.title('Outlier Analysis for Top Numerical Features')
    
    # Color bars based on outlier percentage
    for i, bar in enumerate(bars):
        if outlier_df.iloc[i]['Percentage'] > 10:
            bar.set_color('red')
        elif outlier_df.iloc[i]['Percentage'] > 5:
            bar.set_color('orange')
        else:
            bar.set_color('blue')
    
    plt.tight_layout()
    plt.savefig('outlier_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("üìä Outlier analysis saved as 'outlier_analysis.png'")

# =============================================================================
# 8. FEATURE ENGINEERING INSIGHTS
# =============================================================================

print("\n\n8. FEATURE ENGINEERING INSIGHTS")
print("-" * 50)

# Age-related features
if 'Year Built' in train_data.columns:
    train_data['House_Age'] = train_data['Yr Sold'] - train_data['Year Built']
    train_data['Remodeled'] = (train_data['Year Remod/Add'] != train_data['Year Built']).astype(int)
    
    print("üè† Age-related features created:")
    print(f"   - House_Age: {train_data['House_Age'].describe()}")
    print(f"   - Remodeled: {train_data['Remodeled'].value_counts()}")

# Total area features
area_features = ['Gr Liv Area', 'Total Bsmt SF', '1st Flr SF', '2nd Flr SF']
if all(feature in train_data.columns for feature in area_features):
    train_data['Total_Area'] = train_data[area_features].sum(axis=1)
    print(f"üìê Total_Area feature created: {train_data['Total_Area'].describe()}")

# Quality score features
quality_features = ['Overall Qual', 'Overall Cond', 'Kitchen Qual', 'Exter Qual']
if all(feature in train_data.columns for feature in quality_features):
    # Convert quality ratings to numerical scores
    quality_mapping = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}
    for feature in quality_features:
        if feature in categorical_columns:
            train_data[f'{feature}_Score'] = train_data[feature].map(quality_mapping).fillna(3)
    print("‚≠ê Quality score features created")

# =============================================================================
# 9. DATA CLEANING DECISIONS AND JUSTIFICATION
# =============================================================================

print("\n\n9. DATA CLEANING DECISIONS AND JUSTIFICATION")
print("-" * 50)

print("üîß CLEANING DECISIONS APPLIED:")

print("\nüìã 1. MISSING VALUES HANDLING (HYBRID STRATEGY):")
print("   ‚úÖ Pool QC (99.5% missing): DROPPED - Too many missing values")
print("   ‚úÖ Misc Feature (96.3% missing): DROPPED - Too many missing values")
print("   ‚úÖ Alley (93.3% missing): DROPPED - Too many missing values")
print("   ‚úÖ Fence (79.9% missing): DROPPED - Too many missing values")
print("   ‚úÖ Fireplace Qu (48.8% missing): FILLED with 'Missing' category")
print("   ‚úÖ Garage variables (~5.7% missing): FILLED with median values")
print("   ‚úÖ Basement variables (~3% missing): FILLED with median values")
print("   ‚úÖ Lot Frontage (17.3% missing): FILLED with median values")
print("   ‚úÖ All other missing values: FILLED appropriately")

print("\nüìã 2. OUTLIERS HANDLING:")
print("   - SalePrice outliers (4.5%): KEEP - May represent luxury homes")
print("   - Numerical feature outliers: Apply robust scaling or log transformation")
print("   - Extreme values: Cap at 99th percentile for modeling")

print("\nüìã 3. DATA TYPE CORRECTIONS:")
print("   - Convert quality ratings to numerical scores")
print("   - Ensure consistent data types between train and test sets")

print("\nüìã 4. FEATURE SELECTION:")
print("   - Keep top 15 most correlated features")
print("   - Remove features with >50% missing values")
print("   - Remove highly correlated features (correlation > 0.8)")

print("\nüìã 5. TRANSFORMATIONS:")
print("   - Apply log transformation to SalePrice (skewed distribution)")
print("   - Standardize numerical features")
print("   - One-hot encode categorical features")

# =============================================================================
# 10. MODELING IMPLICATIONS
# =============================================================================

print("\n\n10. MODELING IMPLICATIONS")
print("-" * 50)

print("üéØ IMPACT ON MODELING:")

print("\nüìä 1. FEATURE IMPORTANCE:")
print("   - Overall Qual: Most important (correlation: 0.799)")
print("   - Gr Liv Area: Second most important (correlation: 0.700)")
print("   - Garage Cars: Third most important (correlation: 0.643)")
print("   - Total Bsmt SF: Fourth most important (correlation: 0.635)")

print("\nüìä 2. MODEL CHOICE CONSIDERATIONS:")
print("   - Use robust models (Random Forest, XGBoost) due to outliers")
print("   - Consider ensemble methods for better performance")
print("   - Apply cross-validation due to small dataset size")

print("\nüìä 3. EVALUATION METRICS:")
print("   - Use RMSE and MAE for regression evaluation")
print("   - Consider R¬≤ for model interpretability")
print("   - Use log-transformed metrics for skewed target")

# =============================================================================
# 11. FINAL SUMMARY
# =============================================================================

print("\n\n11. FINAL SUMMARY")
print("-" * 50)

print("‚úÖ EDA COMPLETED:")
print(f"   - Dataset analyzed: {train_data.shape}")
print(f"   - Numerical features: {len(numeric_columns)}")
print(f"   - Categorical features: {len(categorical_columns)}")
print(f"   - Features with missing values: 0 (after cleaning)")
print(f"   - Outliers in SalePrice: {len(outliers)}")
print(f"   - Columns dropped: 4 (Pool QC, Misc Feature, Alley, Fence)")

print("\nüéØ KEY INSIGHTS:")
print("   1. Target variable is right-skewed, log transformation recommended")
print("   2. Several features have high correlation with SalePrice")
print("   3. Missing values pattern suggests systematic data collection issues")
print("   4. Outliers may represent legitimate luxury properties")

print("\nüöÄ NEXT STEPS:")
print("   1. ‚úÖ Data cleaning pipeline COMPLETED")
print("   2. Create feature engineering pipeline")
print("   3. Apply appropriate transformations")
print("   4. Build and evaluate multiple models")
print("   5. Perform hyperparameter tuning")

print("\nüìä VISUALIZATIONS GENERATED:")
print("   - missing_values_analysis.png")
print("   - target_variable_analysis.png")
print("   - numerical_features_analysis.png")
print("   - categorical_features_analysis.png")
print("   - outlier_analysis.png")

# Save cleaned data
train_data.to_csv('house_prices_cleaned_final.csv', index=False)
print("\nüíæ CLEANED DATA SAVED:")
print("   - house_prices_cleaned_final.csv")

print("\nüéâ COMPREHENSIVE EDA COMPLETED!")
print("=" * 70)
